2020-11-26 14:26:17,314 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_191
************************************************************/
2020-11-26 14:26:17,321 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-11-26 14:26:17,324 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-11-26 14:26:17,729 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-11-26 14:26:17,828 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-11-26 14:26:17,828 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-11-26 14:26:17,830 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2020-11-26 14:26:17,831 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2020-11-26 14:26:18,072 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-11-26 14:26:18,148 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-11-26 14:26:18,153 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-11-26 14:26:18,179 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-11-26 14:26:18,183 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-11-26 14:26:18,196 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-11-26 14:26:18,196 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-11-26 14:26:18,196 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-11-26 14:26:18,228 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-11-26 14:26:18,229 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-11-26 14:26:18,255 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-11-26 14:26:18,255 INFO org.mortbay.log: jetty-6.1.26
2020-11-26 14:26:18,419 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-11-26 14:26:18,453 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-11-26 14:26:18,453 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-11-26 14:26:18,483 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-11-26 14:26:18,483 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2020-11-26 14:26:18,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-11-26 14:26:18,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2020-11-26 14:26:18,540 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-11-26 14:26:18,541 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Nov 26 14:26:18
2020-11-26 14:26:18,542 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-11-26 14:26:18,542 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-11-26 14:26:18,543 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2020-11-26 14:26:18,543 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-11-26 14:26:18,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-11-26 14:26:18,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2020-11-26 14:26:18,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-11-26 14:26:18,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-11-26 14:26:18,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-11-26 14:26:18,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2020-11-26 14:26:18,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-11-26 14:26:18,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-11-26 14:26:18,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-11-26 14:26:18,574 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2020-11-26 14:26:18,574 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-11-26 14:26:18,574 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-11-26 14:26:18,574 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-11-26 14:26:18,575 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-11-26 14:26:18,654 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-11-26 14:26:18,654 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-11-26 14:26:18,655 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2020-11-26 14:26:18,655 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-11-26 14:26:18,655 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-11-26 14:26:18,655 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-11-26 14:26:18,656 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-11-26 14:26:18,656 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-11-26 14:26:18,668 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-11-26 14:26:18,668 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-11-26 14:26:18,669 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2020-11-26 14:26:18,669 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-11-26 14:26:18,670 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-11-26 14:26:18,670 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-11-26 14:26:18,670 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-11-26 14:26:18,690 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-11-26 14:26:18,690 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-11-26 14:26:18,691 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-11-26 14:26:18,693 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-11-26 14:26:18,693 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-11-26 14:26:18,694 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-11-26 14:26:18,694 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-11-26 14:26:18,694 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2020-11-26 14:26:18,694 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-11-26 14:26:18,708 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 185@master
2020-11-26 14:26:18,803 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2020-11-26 14:26:18,803 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2020-11-26 14:26:18,840 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2020-11-26 14:26:18,888 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-11-26 14:26:18,888 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2020-11-26 14:26:18,893 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-11-26 14:26:18,893 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2020-11-26 14:26:19,010 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-11-26 14:26:19,010 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 313 msecs
2020-11-26 14:26:19,234 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2020-11-26 14:26:19,238 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2020-11-26 14:26:19,275 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2020-11-26 14:26:19,313 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-11-26 14:26:19,319 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-11-26 14:26:19,319 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-11-26 14:26:19,319 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-11-26 14:26:19,320 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2020-11-26 14:26:19,320 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-11-26 14:26:19,320 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-11-26 14:26:19,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-11-26 14:26:19,342 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2020-11-26 14:26:19,342 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-11-26 14:26:19,342 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2020-11-26 14:26:19,342 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-11-26 14:26:19,342 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-11-26 14:26:19,342 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 22 msec
2020-11-26 14:26:19,366 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-11-26 14:26:19,366 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2020-11-26 14:26:19,368 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2020-11-26 14:26:19,368 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-11-26 14:26:19,371 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-11-26 14:26:23,884 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=26e5562b-24de-4c03-9aea-b92e4af86688, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-a667e8b6-c7c4-443b-90cf-9eeda9af83fd;nsid=1701568202;c=0) storage 26e5562b-24de-4c03-9aea-b92e4af86688
2020-11-26 14:26:23,884 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-11-26 14:26:23,884 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2020-11-26 14:26:23,902 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=6b7e0787-cf64-4e20-81f3-92e456d74456, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-a667e8b6-c7c4-443b-90cf-9eeda9af83fd;nsid=1701568202;c=0) storage 6b7e0787-cf64-4e20-81f3-92e456d74456
2020-11-26 14:26:23,902 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-11-26 14:26:23,902 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2020-11-26 14:26:23,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-11-26 14:26:23,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-55ad6825-29d0-4a6c-b110-e4ba27d04973 for DN 172.18.0.3:50010
2020-11-26 14:26:23,972 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-11-26 14:26:23,972 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28 for DN 172.18.0.4:50010
2020-11-26 14:26:23,989 INFO BlockStateChange: BLOCK* processReport: from storage DS-55ad6825-29d0-4a6c-b110-e4ba27d04973 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=6b7e0787-cf64-4e20-81f3-92e456d74456, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-a667e8b6-c7c4-443b-90cf-9eeda9af83fd;nsid=1701568202;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2020-11-26 14:26:23,991 INFO BlockStateChange: BLOCK* processReport: from storage DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=26e5562b-24de-4c03-9aea-b92e4af86688, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-a667e8b6-c7c4-443b-90cf-9eeda9af83fd;nsid=1701568202;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2020-11-26 14:29:07,921 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 8 
2020-11-26 14:29:10,351 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:10,703 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:10,718 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-11-26 14:29:10,939 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:10,947 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741826_1002{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 14:29:11,045 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:11,046 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:11,135 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:11,137 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:11,217 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:11,219 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:11,284 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:11,287 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:11,356 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:11,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:11,425 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:11,427 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:11,499 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:11,503 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:11,565 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:11,576 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:11,650 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:11,652 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:11,740 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:11,742 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:11,809 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:11,812 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:11,894 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:11,897 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:11,981 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:11,983 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:12,081 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:12,082 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:12,155 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:12,156 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:12,210 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:12,211 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:12,257 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:12,261 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:12,330 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:12,332 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:12,397 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:12,399 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:12,492 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:12,493 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741846_1022{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 14:29:12,581 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:12,590 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:12,639 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:12,652 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:12,734 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:12,742 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:12,798 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:12,807 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:12,891 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:12,893 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:12,957 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:12,966 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:13,020 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:13,029 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:13,092 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:13,098 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:13,162 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:29:13,163 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 14:29:13,196 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:29:13,205 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1332833478_1
2020-11-26 14:30:32,329 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 104 Total time for transactions(ms): 11 Number of transactions batched in Syncs: 0 Number of syncs: 38 SyncTimes(ms): 139 
2020-11-26 14:32:49,206 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 107 Total time for transactions(ms): 11 Number of transactions batched in Syncs: 0 Number of syncs: 39 SyncTimes(ms): 143 
2020-11-26 14:32:49,289 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000000_0/part-00000
2020-11-26 14:32:49,300 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000001_0/part-00001
2020-11-26 14:32:49,448 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:49,451 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:49,455 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:49,457 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:49,457 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000000_0/part-00000 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:49,460 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000001_0/part-00001 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:49,611 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000003_0/part-00003
2020-11-26 14:32:49,612 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000002_0/part-00002
2020-11-26 14:32:49,681 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:49,682 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:49,684 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:49,686 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:49,686 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000002_0/part-00002 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:49,688 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000003_0/part-00003 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:49,783 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000004_0/part-00004
2020-11-26 14:32:49,806 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:49,814 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:49,815 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000004_0/part-00004 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:49,823 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000005_0/part-00005
2020-11-26 14:32:49,873 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:49,873 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:49,875 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000005_0/part-00005 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:49,943 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000006_0/part-00006
2020-11-26 14:32:49,983 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000007_0/part-00007
2020-11-26 14:32:50,007 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:50,016 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:50,018 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000006_0/part-00006 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:50,034 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:50,034 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:50,038 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000007_0/part-00007 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:50,117 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000008_0/part-00008
2020-11-26 14:32:50,143 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000009_0/part-00009
2020-11-26 14:32:50,147 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:50,167 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:50,169 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000008_0/part-00008 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:50,220 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:50,221 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:50,223 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000009_0/part-00009 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:50,254 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000010_0/part-00010
2020-11-26 14:32:50,311 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:50,312 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:50,314 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000010_0/part-00010 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:50,355 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000011_0/part-00011
2020-11-26 14:32:50,410 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:50,413 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:50,416 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000011_0/part-00011 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:50,439 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000012_0/part-00012
2020-11-26 14:32:50,505 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000013_0/part-00013
2020-11-26 14:32:50,505 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:50,506 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:50,508 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000012_0/part-00012 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:50,575 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:50,575 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:50,580 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000013_0/part-00013 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:50,625 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000014_0/part-00014
2020-11-26 14:32:50,669 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000015_0/part-00015
2020-11-26 14:32:50,689 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:50,690 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:50,691 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000014_0/part-00014 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:50,747 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:50,750 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:50,752 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000015_0/part-00015 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:50,804 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000016_0/part-00016
2020-11-26 14:32:50,866 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000017_0/part-00017
2020-11-26 14:32:50,868 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:50,870 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:50,872 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000016_0/part-00016 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:50,928 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:50,930 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:50,938 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000017_0/part-00017 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:50,983 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000018_0/part-00018
2020-11-26 14:32:51,024 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000019_0/part-00019
2020-11-26 14:32:51,056 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,078 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,080 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000018_0/part-00018 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:51,083 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:51,096 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:51,098 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000019_0/part-00019 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:51,163 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000020_0/part-00020
2020-11-26 14:32:51,204 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000021_0/part-00021
2020-11-26 14:32:51,223 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,225 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,226 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000020_0/part-00020 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:51,270 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:51,271 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:51,273 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000021_0/part-00021 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:51,343 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000022_0/part-00022
2020-11-26 14:32:51,402 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000023_0/part-00023
2020-11-26 14:32:51,423 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,425 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,426 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000022_0/part-00022 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:51,478 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:51,481 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:51,483 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000023_0/part-00023 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:51,555 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000024_0/part-00024
2020-11-26 14:32:51,568 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000025_0/part-00025
2020-11-26 14:32:51,587 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,588 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,590 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000024_0/part-00024 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:51,602 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:51,606 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:51,608 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000025_0/part-00025 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:51,689 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000026_0/part-00026
2020-11-26 14:32:51,691 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000027_0/part-00027
2020-11-26 14:32:51,705 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,706 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,710 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000026_0/part-00026 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:51,722 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:51,724 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:51,725 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000027_0/part-00027 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:51,793 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000029_0/part-00029
2020-11-26 14:32:51,808 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,812 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,814 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000029_0/part-00029 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:51,832 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000030_0/part-00030
2020-11-26 14:32:51,888 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:51,889 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 14:32:51,891 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000030_0/part-00030 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:51,919 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000031_0/part-00031
2020-11-26 14:32:51,972 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000028_0/part-00028 is closed by DFSClient_attempt_20201126143031_0000_m_000001_0_-1877766486_26
2020-11-26 14:32:51,975 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,976 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c0d48516-3632-4fd5-8212-b4d6ebae7e28:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-55ad6825-29d0-4a6c-b110-e4ba27d04973:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 14:32:51,980 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126063032_0008_m_000031_0/part-00031 is closed by DFSClient_attempt_20201126143031_0000_m_000000_0_-1550450420_26
2020-11-26 14:32:52,352 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_943182496_16
2020-11-26 22:49:05,986 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_191
************************************************************/
2020-11-26 22:49:06,002 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-11-26 22:49:06,006 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-11-26 22:49:06,562 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-11-26 22:49:06,698 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-11-26 22:49:06,699 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-11-26 22:49:06,701 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2020-11-26 22:49:06,702 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2020-11-26 22:49:07,048 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-11-26 22:49:07,151 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-11-26 22:49:07,165 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-11-26 22:49:07,197 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-11-26 22:49:07,203 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-11-26 22:49:07,239 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-11-26 22:49:07,239 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-11-26 22:49:07,239 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-11-26 22:49:07,260 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-11-26 22:49:07,262 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-11-26 22:49:07,299 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-11-26 22:49:07,300 INFO org.mortbay.log: jetty-6.1.26
2020-11-26 22:49:07,540 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-11-26 22:49:07,593 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-11-26 22:49:07,593 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-11-26 22:49:07,646 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-11-26 22:49:07,646 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2020-11-26 22:49:07,711 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-11-26 22:49:07,711 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2020-11-26 22:49:07,713 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-11-26 22:49:07,714 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Nov 26 22:49:07
2020-11-26 22:49:07,729 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-11-26 22:49:07,730 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-11-26 22:49:07,731 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2020-11-26 22:49:07,731 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-11-26 22:49:07,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-11-26 22:49:07,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2020-11-26 22:49:07,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-11-26 22:49:07,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-11-26 22:49:07,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-11-26 22:49:07,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2020-11-26 22:49:07,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-11-26 22:49:07,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-11-26 22:49:07,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-11-26 22:49:07,758 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2020-11-26 22:49:07,759 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-11-26 22:49:07,759 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-11-26 22:49:07,759 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-11-26 22:49:07,761 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-11-26 22:49:07,873 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-11-26 22:49:07,873 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-11-26 22:49:07,874 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2020-11-26 22:49:07,874 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-11-26 22:49:07,874 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-11-26 22:49:07,874 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-11-26 22:49:07,875 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-11-26 22:49:07,875 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-11-26 22:49:07,882 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-11-26 22:49:07,882 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-11-26 22:49:07,882 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2020-11-26 22:49:07,882 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-11-26 22:49:07,930 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-11-26 22:49:07,930 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-11-26 22:49:07,930 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-11-26 22:49:07,933 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-11-26 22:49:07,933 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-11-26 22:49:07,935 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-11-26 22:49:07,937 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-11-26 22:49:07,937 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-11-26 22:49:07,938 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-11-26 22:49:07,938 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-11-26 22:49:07,939 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2020-11-26 22:49:07,939 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-11-26 22:49:07,952 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 200@master
2020-11-26 22:49:08,070 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2020-11-26 22:49:08,071 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2020-11-26 22:49:08,138 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2020-11-26 22:49:08,187 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-11-26 22:49:08,187 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2020-11-26 22:49:08,199 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-11-26 22:49:08,199 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2020-11-26 22:49:08,328 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-11-26 22:49:08,328 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 385 msecs
2020-11-26 22:49:08,631 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2020-11-26 22:49:08,636 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2020-11-26 22:49:08,649 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2020-11-26 22:49:08,695 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-11-26 22:49:08,706 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-11-26 22:49:08,706 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-11-26 22:49:08,706 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-11-26 22:49:08,707 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2020-11-26 22:49:08,707 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-11-26 22:49:08,707 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-11-26 22:49:08,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-11-26 22:49:08,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2020-11-26 22:49:08,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-11-26 22:49:08,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2020-11-26 22:49:08,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-11-26 22:49:08,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-11-26 22:49:08,737 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 29 msec
2020-11-26 22:49:08,803 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-11-26 22:49:08,804 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2020-11-26 22:49:08,806 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2020-11-26 22:49:08,806 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-11-26 22:49:08,810 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-11-26 22:49:13,785 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=b8561d8b-4f6c-4087-8949-09626010b949, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-fb04172a-742b-4aef-b727-76c3b62004ab;nsid=644343777;c=0) storage b8561d8b-4f6c-4087-8949-09626010b949
2020-11-26 22:49:13,785 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-11-26 22:49:13,786 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2020-11-26 22:49:13,835 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=50792693-13e8-43d2-9e61-c8917afef742, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-fb04172a-742b-4aef-b727-76c3b62004ab;nsid=644343777;c=0) storage 50792693-13e8-43d2-9e61-c8917afef742
2020-11-26 22:49:13,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-11-26 22:49:13,835 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2020-11-26 22:49:13,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-11-26 22:49:13,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9 for DN 172.18.0.4:50010
2020-11-26 22:49:13,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-11-26 22:49:13,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-43906cdd-11ad-4523-ad3c-86c374db507d for DN 172.18.0.3:50010
2020-11-26 22:49:13,993 INFO BlockStateChange: BLOCK* processReport: from storage DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=50792693-13e8-43d2-9e61-c8917afef742, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-fb04172a-742b-4aef-b727-76c3b62004ab;nsid=644343777;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2020-11-26 22:49:14,001 INFO BlockStateChange: BLOCK* processReport: from storage DS-43906cdd-11ad-4523-ad3c-86c374db507d node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=b8561d8b-4f6c-4087-8949-09626010b949, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-fb04172a-742b-4aef-b727-76c3b62004ab;nsid=644343777;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2020-11-26 22:50:18,309 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2020-11-26 22:50:18,309 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-11-26 22:50:18,309 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2020-11-26 22:50:18,309 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 8 
2020-11-26 22:50:18,313 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 11 
2020-11-26 22:50:18,314 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2020-11-26 22:50:18,316 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2020-11-26 22:50:19,929 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2020-11-26 22:50:19,930 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2020-11-26 22:50:19,946 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2020-11-26 22:52:18,631 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 5 
2020-11-26 22:52:22,347 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:22,955 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:22,993 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:23,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:23,383 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741826_1002{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-11-26 22:52:23,577 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:23,579 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:23,714 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 22:52:23,716 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:23,820 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:23,831 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:23,928 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:23,938 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:24,037 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 22:52:24,048 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:24,148 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 22:52:24,157 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:24,273 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:24,283 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:24,393 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:24,407 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:24,520 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:24,521 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741835_1011{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:24,623 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 22:52:24,624 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:24,712 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:24,713 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741837_1013{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-11-26 22:52:24,829 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:24,832 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:24,942 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:24,943 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741839_1015{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:25,040 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:25,040 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741840_1016{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:25,139 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:25,139 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741841_1017{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:25,265 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 22:52:25,268 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:25,387 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:25,388 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:25,498 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:25,500 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741844_1020{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:25,597 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:25,600 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:25,689 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:25,690 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:25,773 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:25,774 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741847_1023{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:25,883 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:25,883 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:25,978 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:25,979 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741849_1025{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:26,072 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 22:52:26,074 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:26,175 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:26,176 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741851_1027{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-11-26 22:52:26,275 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 22:52:26,276 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:26,353 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:26,355 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741853_1029{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:26,441 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:26,443 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:26,501 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 22:52:26,503 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:26,572 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:26,573 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:26,659 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:26,662 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741857_1033{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-11-26 22:52:26,742 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:26,743 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741858_1034{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:26,798 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:26,800 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:26,854 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:26,855 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:26,933 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:26,934 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741861_1037{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:27,000 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:27,000 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741862_1038{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-11-26 22:52:27,081 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:27,082 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741863_1039{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-11-26 22:52:27,176 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:27,178 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:27,273 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:27,274 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741865_1041{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:27,367 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:27,373 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741866_1042{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:27,472 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 22:52:27,477 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:27,537 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:27,538 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741868_1044{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:27,655 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:27,655 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741869_1045{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-11-26 22:52:27,724 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:27,725 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741870_1046{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:27,800 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 22:52:27,801 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:27,896 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:27,898 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741872_1048{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:27,941 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 22:52:27,952 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:28,009 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:28,011 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741874_1050{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:28,083 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:28,086 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741875_1051{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-11-26 22:52:28,152 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:28,157 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:28,266 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:28,276 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:28,387 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:28,389 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741878_1054{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:28,474 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 22:52:28,476 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:28,538 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:28,540 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741880_1056{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-11-26 22:52:28,610 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:28,612 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741881_1057{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-11-26 22:52:28,668 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 22:52:28,671 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:28,721 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:28,722 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:28,796 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:28,800 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741884_1060{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:28,917 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:28,919 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741885_1061{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-11-26 22:52:29,015 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:29,016 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741886_1062{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:29,098 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-11-26 22:52:29,099 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741887_1063{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-11-26 22:52:29,148 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 22:52:29,158 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1519637404_1
2020-11-26 22:53:56,931 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 200 Total time for transactions(ms): 24 Number of transactions batched in Syncs: 0 Number of syncs: 70 SyncTimes(ms): 228 
2020-11-26 23:00:10,699 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 203 Total time for transactions(ms): 24 Number of transactions batched in Syncs: 0 Number of syncs: 71 SyncTimes(ms): 231 
2020-11-26 23:00:10,808 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000001_0/part-00001
2020-11-26 23:00:10,838 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000000_0/part-00000
2020-11-26 23:00:11,034 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:11,036 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:11,045 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000001_0/part-00001 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:11,097 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:11,098 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:11,117 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000000_0/part-00000 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:11,270 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000002_0/part-00002
2020-11-26 23:00:11,348 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000003_0/part-00003
2020-11-26 23:00:11,351 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:11,357 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:11,370 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000002_0/part-00002 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:11,458 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:11,468 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:11,471 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000003_0/part-00003 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:11,545 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000004_0/part-00004
2020-11-26 23:00:11,626 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000006_0/part-00006
2020-11-26 23:00:11,655 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:11,657 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:11,658 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000004_0/part-00004 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:11,683 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:11,772 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:11,776 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000006_0/part-00006 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:11,861 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000007_0/part-00007
2020-11-26 23:00:11,921 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:11,924 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:11,929 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000007_0/part-00007 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:11,936 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000009_0/part-00009
2020-11-26 23:00:12,021 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:12,022 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:12,026 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000009_0/part-00009 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:12,068 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000010_0/part-00010
2020-11-26 23:00:12,161 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000011_0/part-00011
2020-11-26 23:00:12,177 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:12,178 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:12,192 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000010_0/part-00010 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:12,206 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:12,207 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:12,214 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000011_0/part-00011 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:12,354 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000012_0/part-00012
2020-11-26 23:00:12,369 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000013_0/part-00013
2020-11-26 23:00:12,408 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:12,410 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:12,411 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000012_0/part-00012 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:12,451 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:12,456 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:12,459 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000013_0/part-00013 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:12,565 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000014_0/part-00014
2020-11-26 23:00:12,635 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000015_0/part-00015
2020-11-26 23:00:12,650 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:12,651 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741901_1077 size 54
2020-11-26 23:00:12,652 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000014_0/part-00014 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:12,727 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:12,744 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:12,748 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000015_0/part-00015 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:12,819 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000016_0/part-00016
2020-11-26 23:00:12,895 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000017_0/part-00017
2020-11-26 23:00:12,926 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:12,929 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:12,930 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000016_0/part-00016 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:12,964 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:12,967 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:12,969 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000017_0/part-00017 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:13,110 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000018_0/part-00018
2020-11-26 23:00:13,144 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000019_0/part-00019
2020-11-26 23:00:13,175 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:13,179 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:13,182 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000018_0/part-00018 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:13,281 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:13,281 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:13,284 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000019_0/part-00019 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:13,366 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000020_0/part-00020
2020-11-26 23:00:13,419 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:13,421 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:13,423 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000020_0/part-00020 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:13,427 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000021_0/part-00021
2020-11-26 23:00:13,528 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:13,528 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:13,536 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000021_0/part-00021 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:13,569 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000022_0/part-00022
2020-11-26 23:00:13,646 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:13,647 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:13,650 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000022_0/part-00022 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:13,662 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000026_0/part-00026
2020-11-26 23:00:13,766 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:13,788 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000027_0/part-00027
2020-11-26 23:00:13,789 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:13,792 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000026_0/part-00026 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:13,926 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:13,927 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000030_0/part-00030
2020-11-26 23:00:13,927 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:13,936 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000027_0/part-00027 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:14,040 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:14,043 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:14,047 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000030_0/part-00030 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:14,077 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000031_0/part-00031
2020-11-26 23:00:14,143 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:14,144 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:14,146 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000031_0/part-00031 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:14,187 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000032_0/part-00032
2020-11-26 23:00:14,275 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000033_0/part-00033
2020-11-26 23:00:14,307 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:14,308 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:14,310 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000032_0/part-00032 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:14,382 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:14,385 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:14,386 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000033_0/part-00033 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:14,467 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000034_0/part-00034
2020-11-26 23:00:14,501 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000035_0/part-00035
2020-11-26 23:00:14,566 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:14,568 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:14,570 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:14,572 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741917_1093 size 61
2020-11-26 23:00:14,573 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000034_0/part-00034 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:14,574 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000035_0/part-00035 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:14,713 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000037_0/part-00037
2020-11-26 23:00:14,716 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000036_0/part-00036
2020-11-26 23:00:14,793 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:14,796 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741918_1094{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000037_0/part-00037
2020-11-26 23:00:14,797 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741918_1094{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 36
2020-11-26 23:00:14,797 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741918_1094 size 36
2020-11-26 23:00:14,798 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741919_1095 size 38
2020-11-26 23:00:14,800 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000036_0/part-00036 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:14,932 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000038_0/part-00038
2020-11-26 23:00:14,987 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:14,989 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:14,994 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000038_0/part-00038 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:15,117 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000040_0/part-00040
2020-11-26 23:00:15,169 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:15,170 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:15,173 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000040_0/part-00040 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:15,199 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000037_0/part-00037 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:15,295 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000041_0/part-00041
2020-11-26 23:00:15,318 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000043_0/part-00043
2020-11-26 23:00:15,364 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:15,366 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:15,367 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000041_0/part-00041 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:15,377 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:15,397 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:15,420 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000043_0/part-00043 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:15,490 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000044_0/part-00044
2020-11-26 23:00:15,531 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000045_0/part-00045
2020-11-26 23:00:15,585 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:15,586 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:15,588 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000044_0/part-00044 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:15,632 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:15,635 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:15,644 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000045_0/part-00045 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:15,700 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000046_0/part-00046
2020-11-26 23:00:15,758 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000047_0/part-00047
2020-11-26 23:00:15,763 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:15,765 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:15,767 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000046_0/part-00046 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:15,821 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:15,823 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:15,825 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000047_0/part-00047 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:15,870 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000048_0/part-00048
2020-11-26 23:00:15,954 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741929_1105{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000049_0/part-00049
2020-11-26 23:00:15,975 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:15,975 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:15,977 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000048_0/part-00048 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:16,057 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741929_1105{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:16,061 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741929_1105{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:16,063 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000049_0/part-00049 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:16,095 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741930_1106{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000050_0/part-00050
2020-11-26 23:00:16,159 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000051_0/part-00051
2020-11-26 23:00:16,181 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741930_1106{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:16,183 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741930_1106{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:16,193 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000050_0/part-00050 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:16,236 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:16,237 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:16,246 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000051_0/part-00051 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:16,295 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000053_0/part-00053
2020-11-26 23:00:16,335 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:16,336 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:16,338 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000053_0/part-00053 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:16,350 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000055_0/part-00055
2020-11-26 23:00:16,429 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:16,430 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:16,432 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000055_0/part-00055 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:16,446 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741934_1110{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000056_0/part-00056
2020-11-26 23:00:16,523 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741934_1110{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:16,525 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741934_1110{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:16,526 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000056_0/part-00056 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:16,559 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000057_0/part-00057
2020-11-26 23:00:16,602 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:16,603 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:16,606 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000057_0/part-00057 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:16,641 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000058_0/part-00058
2020-11-26 23:00:16,698 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000059_0/part-00059
2020-11-26 23:00:16,704 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:16,704 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:16,706 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000058_0/part-00058 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:16,755 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:16,756 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:16,759 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000059_0/part-00059 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:16,820 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000061_0/part-00061
2020-11-26 23:00:16,856 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:16,857 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:16,861 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000061_0/part-00061 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:16,879 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000062_0/part-00062
2020-11-26 23:00:16,937 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:16,938 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-11-26 23:00:16,942 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000062_0/part-00062 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:16,957 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741940_1116{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000063_0/part-00063
2020-11-26 23:00:17,009 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000005_0/part-00005 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:17,013 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741940_1116{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:17,017 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741940_1116{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-43906cdd-11ad-4523-ad3c-86c374db507d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-0da1e86d-64b2-4598-833a-10c27a2d3ad9:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-11-26 23:00:17,019 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000063_0/part-00063 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:17,117 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000008_0/part-00008 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:17,137 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000023_0/part-00023 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:17,223 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000024_0/part-00024 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:17,224 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000025_0/part-00025 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:17,288 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000028_0/part-00028 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:17,325 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000029_0/part-00029 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:17,374 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000039_0/part-00039 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:17,423 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000042_0/part-00042 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:17,462 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000052_0/part-00052 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:17,511 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000054_0/part-00054 is closed by DFSClient_attempt_20201126225355_0000_m_000000_0_-1233334146_26
2020-11-26 23:00:17,550 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201126145356_0008_m_000060_0/part-00060 is closed by DFSClient_attempt_20201126225355_0000_m_000001_0_1890012201_26
2020-11-26 23:00:18,307 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1924570774_16
2020-11-26 23:02:46,260 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2020-11-26 23:02:46,261 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at master/172.18.0.2
************************************************************/
2020-12-01 21:47:39,203 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_191
************************************************************/
2020-12-01 21:47:39,213 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-12-01 21:47:39,218 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-12-01 21:47:39,808 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-12-01 21:47:39,964 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-12-01 21:47:39,964 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-12-01 21:47:39,967 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2020-12-01 21:47:39,969 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2020-12-01 21:47:40,344 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-12-01 21:47:40,448 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-12-01 21:47:40,457 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-01 21:47:40,501 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-12-01 21:47:40,507 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-01 21:47:40,530 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-01 21:47:40,531 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-01 21:47:40,531 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-01 21:47:40,559 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-01 21:47:40,561 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-01 21:47:40,597 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-12-01 21:47:40,598 INFO org.mortbay.log: jetty-6.1.26
2020-12-01 21:47:40,847 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-12-01 21:47:40,909 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-12-01 21:47:40,909 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-12-01 21:47:40,969 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-12-01 21:47:40,970 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2020-12-01 21:47:41,047 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-12-01 21:47:41,047 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-01 21:47:41,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-01 21:47:41,065 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Dec 01 21:47:41
2020-12-01 21:47:41,068 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-12-01 21:47:41,068 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-12-01 21:47:41,070 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2020-12-01 21:47:41,070 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-12-01 21:47:41,077 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-12-01 21:47:41,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2020-12-01 21:47:41,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-12-01 21:47:41,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-12-01 21:47:41,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-12-01 21:47:41,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2020-12-01 21:47:41,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-12-01 21:47:41,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-12-01 21:47:41,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-12-01 21:47:41,099 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2020-12-01 21:47:41,100 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-12-01 21:47:41,100 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-12-01 21:47:41,100 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-12-01 21:47:41,101 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-12-01 21:47:41,236 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-12-01 21:47:41,236 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-12-01 21:47:41,250 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2020-12-01 21:47:41,251 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-12-01 21:47:41,252 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-12-01 21:47:41,252 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-12-01 21:47:41,252 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-12-01 21:47:41,252 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-12-01 21:47:41,261 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-12-01 21:47:41,261 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-12-01 21:47:41,261 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2020-12-01 21:47:41,261 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-12-01 21:47:41,334 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-01 21:47:41,334 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-12-01 21:47:41,334 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-12-01 21:47:41,337 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-01 21:47:41,337 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-12-01 21:47:41,339 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-01 21:47:41,340 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-12-01 21:47:41,340 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-01 21:47:41,342 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-12-01 21:47:41,342 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-12-01 21:47:41,343 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2020-12-01 21:47:41,343 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-12-01 21:47:41,361 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 203@master
2020-12-01 21:47:41,463 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2020-12-01 21:47:41,479 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2020-12-01 21:47:41,521 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2020-12-01 21:47:41,585 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-12-01 21:47:41,586 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2020-12-01 21:47:41,594 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-01 21:47:41,595 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2020-12-01 21:47:41,733 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-12-01 21:47:41,733 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 387 msecs
2020-12-01 21:47:42,108 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2020-12-01 21:47:42,114 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2020-12-01 21:47:42,132 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2020-12-01 21:47:42,208 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-12-01 21:47:42,226 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-12-01 21:47:42,226 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-12-01 21:47:42,226 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-12-01 21:47:42,241 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2020-12-01 21:47:42,241 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-12-01 21:47:42,241 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-12-01 21:47:42,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-12-01 21:47:42,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2020-12-01 21:47:42,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-12-01 21:47:42,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2020-12-01 21:47:42,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-12-01 21:47:42,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-12-01 21:47:42,300 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 58 msec
2020-12-01 21:47:42,337 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-12-01 21:47:42,338 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2020-12-01 21:47:42,340 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2020-12-01 21:47:42,340 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-12-01 21:47:42,344 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-01 21:47:47,105 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=fe1d07dd-cd9c-47fb-b36f-3eeb662f0a65, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-36f672ce-16be-4137-95d3-b261fc7fbade;nsid=299825135;c=0) storage fe1d07dd-cd9c-47fb-b36f-3eeb662f0a65
2020-12-01 21:47:47,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-12-01 21:47:47,106 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2020-12-01 21:47:47,108 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=5d39ab94-1530-4574-b897-635a3fee0a4f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-36f672ce-16be-4137-95d3-b261fc7fbade;nsid=299825135;c=0) storage 5d39ab94-1530-4574-b897-635a3fee0a4f
2020-12-01 21:47:47,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-12-01 21:47:47,109 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2020-12-01 21:47:47,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-12-01 21:47:47,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-fb7c657c-f8f3-451b-b844-ad1a619f702b for DN 172.18.0.4:50010
2020-12-01 21:47:47,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-12-01 21:47:47,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8f497452-c458-446f-86ab-60d052cb01b8 for DN 172.18.0.3:50010
2020-12-01 21:47:47,322 INFO BlockStateChange: BLOCK* processReport: from storage DS-8f497452-c458-446f-86ab-60d052cb01b8 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=fe1d07dd-cd9c-47fb-b36f-3eeb662f0a65, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-36f672ce-16be-4137-95d3-b261fc7fbade;nsid=299825135;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2020-12-01 21:47:47,324 INFO BlockStateChange: BLOCK* processReport: from storage DS-fb7c657c-f8f3-451b-b844-ad1a619f702b node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=5d39ab94-1530-4574-b897-635a3fee0a4f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-36f672ce-16be-4137-95d3-b261fc7fbade;nsid=299825135;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2020-12-01 21:48:52,091 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2020-12-01 21:48:52,091 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-12-01 21:48:52,091 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2020-12-01 21:48:52,091 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 7 
2020-12-01 21:48:52,095 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 10 
2020-12-01 21:48:52,097 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2020-12-01 21:48:52,101 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2020-12-01 21:48:53,901 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2020-12-01 21:48:53,901 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2020-12-01 21:48:53,911 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2020-12-01 21:49:13,826 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:14,512 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:14,535 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:14,703 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:49:14,705 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:15,079 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:15,090 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741827_1003{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:15,254 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:15,264 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:15,355 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:49:15,365 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:15,474 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:49:15,480 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:15,565 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:15,575 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:15,681 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:15,691 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:15,787 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:15,788 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741833_1009{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:15,903 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:49:15,912 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:16,025 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:16,026 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741835_1011{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:16,138 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:16,142 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741836_1012{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:16,269 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:49:16,270 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:16,352 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:16,353 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741838_1014{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:16,437 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:16,438 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741839_1015{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:16,565 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:16,566 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741840_1016{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:16,652 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:16,653 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:16,735 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:16,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741842_1018{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:16,840 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:16,841 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:16,946 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:16,947 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741844_1020{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:17,049 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:17,051 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741845_1021{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:17,155 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:17,155 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741846_1022{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:17,253 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:17,254 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741847_1023{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:17,345 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:49:17,346 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:17,433 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:17,434 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741849_1025{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:17,511 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:17,512 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:17,572 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:17,573 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741851_1027{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:17,682 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:49:17,683 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:17,763 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:49:17,764 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:17,848 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:17,849 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:17,935 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:17,937 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:18,021 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:18,023 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:18,104 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:18,105 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741857_1033{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:18,185 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:18,187 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741858_1034{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:18,269 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:18,269 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741859_1035{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:18,377 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:18,379 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741860_1036{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:18,459 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:18,460 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:18,527 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:18,529 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:18,662 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:18,663 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741863_1039{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:18,787 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:18,788 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741864_1040{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:18,889 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:18,889 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741865_1041{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:18,998 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:18,998 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741866_1042{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:19,116 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:19,120 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:19,190 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:19,192 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:19,267 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:19,267 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741869_1045{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:19,402 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:19,403 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741870_1046{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:19,532 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:19,541 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2020-12-01 21:49:19,545 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:19,576 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command mkdirs is: 1
2020-12-01 21:49:19,576 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command listStatus is: 1
2020-12-01 21:49:19,576 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command create is: 1
2020-12-01 21:49:19,576 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2020-12-01 21:49:19,576 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2020-12-01 21:49:19,577 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command mkdirs is: 1
2020-12-01 21:49:19,577 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command listStatus is: 1
2020-12-01 21:49:19,577 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command create is: 1
2020-12-01 21:49:19,577 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2020-12-01 21:49:19,579 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2020-12-01 21:49:19,579 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command mkdirs is: 1
2020-12-01 21:49:19,579 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command listStatus is: 1
2020-12-01 21:49:19,579 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command create is: 1
2020-12-01 21:49:19,580 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2020-12-01 21:49:19,641 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:19,642 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741872_1048{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:19,757 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:49:19,759 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:19,819 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:19,819 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741874_1050{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:19,896 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:19,898 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741875_1051{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:19,989 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:19,990 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741876_1052{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:20,099 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:49:20,101 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:20,174 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:20,175 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741878_1054{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:20,262 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:20,264 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741879_1055{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:20,338 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:20,341 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741880_1056{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:20,406 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:20,409 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741881_1057{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:20,485 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:49:20,488 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:20,544 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:20,545 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741883_1059{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-01 21:49:20,625 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:20,628 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741884_1060{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:20,699 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:20,700 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741885_1061{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:20,776 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:20,778 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741886_1062{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:20,866 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-01 21:49:20,867 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741887_1063{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-01 21:49:20,939 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:49:20,948 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_483950169_1
2020-12-01 21:50:27,289 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 200 Total time for transactions(ms): 36 Number of transactions batched in Syncs: 0 Number of syncs: 70 SyncTimes(ms): 220 
2020-12-01 21:56:18,183 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 203 Total time for transactions(ms): 36 Number of transactions batched in Syncs: 0 Number of syncs: 71 SyncTimes(ms): 225 
2020-12-01 21:56:18,306 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000000_0/part-00000
2020-12-01 21:56:18,334 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000001_0/part-00001
2020-12-01 21:56:18,532 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:18,533 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:18,540 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000000_0/part-00000 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:18,610 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:18,613 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:18,645 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000001_0/part-00001 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:18,757 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000002_0/part-00002
2020-12-01 21:56:18,835 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:18,837 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:18,840 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000002_0/part-00002 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:18,856 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000003_0/part-00003
2020-12-01 21:56:18,955 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:18,957 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:18,959 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000003_0/part-00003 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:18,980 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000004_0/part-00004
2020-12-01 21:56:19,070 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:19,072 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:19,075 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000004_0/part-00004 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:19,111 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000006_0/part-00006
2020-12-01 21:56:19,229 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000007_0/part-00007
2020-12-01 21:56:19,253 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:19,255 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:19,257 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000006_0/part-00006 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:19,353 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:19,354 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:19,358 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000007_0/part-00007 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:19,437 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000009_0/part-00009
2020-12-01 21:56:19,505 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000010_0/part-00010
2020-12-01 21:56:19,514 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:19,515 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:19,518 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000009_0/part-00009 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:19,617 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:19,618 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:19,623 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000010_0/part-00010 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:19,666 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000011_0/part-00011
2020-12-01 21:56:19,748 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:19,750 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:19,764 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000011_0/part-00011 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:19,767 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000012_0/part-00012
2020-12-01 21:56:19,863 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:19,865 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:19,867 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000012_0/part-00012 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:19,896 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000013_0/part-00013
2020-12-01 21:56:19,978 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:19,979 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:19,982 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000013_0/part-00013 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:20,007 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000014_0/part-00014
2020-12-01 21:56:20,085 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:20,086 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:20,090 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000014_0/part-00014 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:20,132 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000015_0/part-00015
2020-12-01 21:56:20,206 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:20,209 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:20,212 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000015_0/part-00015 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:20,242 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000016_0/part-00016
2020-12-01 21:56:20,329 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:20,331 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:20,333 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000016_0/part-00016 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:20,349 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000017_0/part-00017
2020-12-01 21:56:20,420 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:20,421 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:20,424 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000017_0/part-00017 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:20,474 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000018_0/part-00018
2020-12-01 21:56:20,537 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000019_0/part-00019
2020-12-01 21:56:20,539 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:20,541 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:20,558 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000018_0/part-00018 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:20,575 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:20,577 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:20,580 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000019_0/part-00019 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:20,721 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000020_0/part-00020
2020-12-01 21:56:20,737 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000021_0/part-00021
2020-12-01 21:56:20,787 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:20,787 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:20,789 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:20,789 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:20,793 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000021_0/part-00021 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:20,794 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000020_0/part-00020 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:20,928 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000022_0/part-00022
2020-12-01 21:56:20,969 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000026_0/part-00026
2020-12-01 21:56:20,995 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:20,999 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:21,002 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000022_0/part-00022 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:21,096 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:21,098 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:21,102 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000026_0/part-00026 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:21,135 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000027_0/part-00027
2020-12-01 21:56:21,215 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:21,216 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:21,219 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000027_0/part-00027 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:21,235 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000030_0/part-00030
2020-12-01 21:56:21,327 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:21,329 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:21,333 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000030_0/part-00030 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:21,344 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000031_0/part-00031
2020-12-01 21:56:21,435 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:21,438 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:21,441 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000031_0/part-00031 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:21,470 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000032_0/part-00032
2020-12-01 21:56:21,573 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:21,574 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000033_0/part-00033
2020-12-01 21:56:21,576 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:21,577 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000032_0/part-00032 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:21,673 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:21,674 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:21,676 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000033_0/part-00033 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:21,729 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000034_0/part-00034
2020-12-01 21:56:21,785 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000035_0/part-00035
2020-12-01 21:56:21,787 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:21,795 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741916_1092 size 19
2020-12-01 21:56:21,796 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000034_0/part-00034 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:21,888 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:21,890 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:21,894 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000035_0/part-00035 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:21,923 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000036_0/part-00036
2020-12-01 21:56:22,010 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:22,013 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:22,015 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000036_0/part-00036 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:22,037 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000037_0/part-00037
2020-12-01 21:56:22,150 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000038_0/part-00038
2020-12-01 21:56:22,153 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:22,154 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:22,155 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000037_0/part-00037 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:22,247 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:22,249 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:22,250 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000038_0/part-00038 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:22,275 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000040_0/part-00040
2020-12-01 21:56:22,380 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000041_0/part-00041
2020-12-01 21:56:22,404 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:22,404 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:22,405 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000040_0/part-00040 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:22,485 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:22,489 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741922_1098 size 19
2020-12-01 21:56:22,490 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000041_0/part-00041 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:22,524 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000043_0/part-00043
2020-12-01 21:56:22,606 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000044_0/part-00044
2020-12-01 21:56:22,611 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:22,611 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:22,614 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000043_0/part-00043 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:22,710 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:22,712 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:22,714 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000044_0/part-00044 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:22,756 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000045_0/part-00045
2020-12-01 21:56:22,837 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000046_0/part-00046
2020-12-01 21:56:22,838 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:22,838 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:22,840 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000045_0/part-00045 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:22,936 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:22,939 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:22,942 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000046_0/part-00046 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:22,956 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000047_0/part-00047
2020-12-01 21:56:23,053 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000048_0/part-00048
2020-12-01 21:56:23,085 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:23,087 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741927_1103 size 38
2020-12-01 21:56:23,087 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000047_0/part-00047 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:23,156 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:23,157 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:23,161 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000048_0/part-00048 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:23,213 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741929_1105{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000049_0/part-00049
2020-12-01 21:56:23,265 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741930_1106{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000050_0/part-00050
2020-12-01 21:56:23,273 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741929_1105{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:23,274 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741929_1105{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:23,275 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000049_0/part-00049 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:23,354 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741930_1106{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:23,357 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741930_1106{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:23,359 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000050_0/part-00050 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:23,409 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000051_0/part-00051
2020-12-01 21:56:23,498 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:23,500 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:23,504 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000051_0/part-00051 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:23,511 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000053_0/part-00053
2020-12-01 21:56:23,601 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:23,611 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:23,614 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000053_0/part-00053 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:23,628 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000055_0/part-00055
2020-12-01 21:56:23,686 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:23,689 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:23,693 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000055_0/part-00055 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:23,746 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741934_1110{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000056_0/part-00056
2020-12-01 21:56:23,822 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000057_0/part-00057
2020-12-01 21:56:23,826 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741934_1110{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:23,829 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741934_1110{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:23,834 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000056_0/part-00056 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:23,901 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:23,912 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:23,916 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000057_0/part-00057 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:23,959 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000058_0/part-00058
2020-12-01 21:56:24,016 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000059_0/part-00059
2020-12-01 21:56:24,020 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:24,029 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:24,031 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000058_0/part-00058 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:24,112 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:24,115 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:24,116 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000059_0/part-00059 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:24,155 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000061_0/part-00061
2020-12-01 21:56:24,220 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000062_0/part-00062
2020-12-01 21:56:24,251 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:24,258 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:24,260 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000061_0/part-00061 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:24,319 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:24,321 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-01 21:56:24,322 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000062_0/part-00062 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:24,368 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741940_1116{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000063_0/part-00063
2020-12-01 21:56:24,396 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000005_0/part-00005 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:24,431 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741940_1116{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:24,432 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741940_1116{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8f497452-c458-446f-86ab-60d052cb01b8:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-fb7c657c-f8f3-451b-b844-ad1a619f702b:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-01 21:56:24,443 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000063_0/part-00063 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:24,499 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000008_0/part-00008 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:24,550 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000023_0/part-00023 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:24,568 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000024_0/part-00024 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:24,655 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000028_0/part-00028 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:24,688 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000025_0/part-00025 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:24,747 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000029_0/part-00029 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:24,797 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000039_0/part-00039 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:24,824 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000042_0/part-00042 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:24,896 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000054_0/part-00054 is closed by DFSClient_attempt_20201201215025_0000_m_000000_0_-1962780208_26
2020-12-01 21:56:24,899 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000052_0/part-00052 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:25,003 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201201135027_0008_m_000060_0/part-00060 is closed by DFSClient_attempt_20201201215025_0000_m_000001_0_1731366401_26
2020-12-01 21:56:25,775 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1462793398_16
2020-12-02 00:10:47,947 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2020-12-02 00:10:47,948 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-12-02 00:10:47,948 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3
2020-12-02 00:10:47,948 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 681 Total time for transactions(ms): 57 Number of transactions batched in Syncs: 1 Number of syncs: 382 SyncTimes(ms): 701 
2020-12-02 00:10:47,952 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 681 Total time for transactions(ms): 57 Number of transactions batched in Syncs: 1 Number of syncs: 383 SyncTimes(ms): 705 
2020-12-02 00:10:47,953 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000003 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000003-0000000000000000683
2020-12-02 00:10:47,953 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 684
2020-12-02 00:10:48,224 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 1250.00 KB/s
2020-12-02 00:10:48,224 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000683 size 5899 bytes.
2020-12-02 00:10:48,229 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2020-12-02 00:10:48,229 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-02 01:10:49,259 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2020-12-02 01:10:49,259 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-12-02 01:10:49,259 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 684
2020-12-02 01:10:49,260 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 7 
2020-12-02 01:10:49,263 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 11 
2020-12-02 01:10:49,264 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000684 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000684-0000000000000000685
2020-12-02 01:10:49,264 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 686
2020-12-02 01:10:49,323 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 1000.00 KB/s
2020-12-02 01:10:49,323 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000685 size 5899 bytes.
2020-12-02 01:10:49,337 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 683
2020-12-02 01:10:49,338 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop/dfs/name/current/fsimage_0000000000000000002, cpktTxId=0000000000000000002)
2020-12-07 21:30:37,503 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_191
************************************************************/
2020-12-07 21:30:37,526 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-12-07 21:30:37,538 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-12-07 21:30:38,108 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-12-07 21:30:38,261 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-12-07 21:30:38,261 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-12-07 21:30:38,263 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2020-12-07 21:30:38,264 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2020-12-07 21:30:38,628 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-12-07 21:30:38,725 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-12-07 21:30:38,746 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-07 21:30:38,787 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-12-07 21:30:38,792 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-07 21:30:38,796 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-07 21:30:38,797 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-07 21:30:38,797 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-07 21:30:38,822 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-07 21:30:38,823 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-07 21:30:38,863 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-12-07 21:30:38,863 INFO org.mortbay.log: jetty-6.1.26
2020-12-07 21:30:39,107 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-12-07 21:30:39,151 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-12-07 21:30:39,151 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-12-07 21:30:39,220 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-12-07 21:30:39,220 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2020-12-07 21:30:39,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-12-07 21:30:39,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-07 21:30:39,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-07 21:30:39,302 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Dec 07 21:30:39
2020-12-07 21:30:39,303 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-12-07 21:30:39,303 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-12-07 21:30:39,305 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2020-12-07 21:30:39,306 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-12-07 21:30:39,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-12-07 21:30:39,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2020-12-07 21:30:39,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-12-07 21:30:39,318 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-12-07 21:30:39,318 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-12-07 21:30:39,318 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2020-12-07 21:30:39,318 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-12-07 21:30:39,318 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-12-07 21:30:39,318 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-12-07 21:30:39,325 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2020-12-07 21:30:39,325 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-12-07 21:30:39,325 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-12-07 21:30:39,325 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-12-07 21:30:39,328 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-12-07 21:30:39,449 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-12-07 21:30:39,449 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-12-07 21:30:39,450 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2020-12-07 21:30:39,450 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-12-07 21:30:39,451 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-12-07 21:30:39,451 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-12-07 21:30:39,451 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-12-07 21:30:39,451 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-12-07 21:30:39,458 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-12-07 21:30:39,458 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-12-07 21:30:39,459 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2020-12-07 21:30:39,459 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-12-07 21:30:39,484 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-07 21:30:39,484 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-12-07 21:30:39,484 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-12-07 21:30:39,487 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-07 21:30:39,487 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-12-07 21:30:39,489 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-07 21:30:39,492 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-12-07 21:30:39,492 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-07 21:30:39,494 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-12-07 21:30:39,494 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-12-07 21:30:39,494 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2020-12-07 21:30:39,494 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-12-07 21:30:39,512 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 294@master
2020-12-07 21:30:39,639 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2020-12-07 21:30:39,640 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2020-12-07 21:30:39,711 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2020-12-07 21:30:39,775 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-12-07 21:30:39,775 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2020-12-07 21:30:39,792 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-07 21:30:39,792 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2020-12-07 21:30:39,941 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-12-07 21:30:39,941 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 442 msecs
2020-12-07 21:30:40,336 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2020-12-07 21:30:40,347 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2020-12-07 21:30:40,360 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2020-12-07 21:30:40,403 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-12-07 21:30:40,421 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-12-07 21:30:40,421 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-12-07 21:30:40,421 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-12-07 21:30:40,422 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2020-12-07 21:30:40,422 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-12-07 21:30:40,422 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-12-07 21:30:40,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-12-07 21:30:40,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2020-12-07 21:30:40,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-12-07 21:30:40,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2020-12-07 21:30:40,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-12-07 21:30:40,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-12-07 21:30:40,447 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 24 msec
2020-12-07 21:30:40,516 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-12-07 21:30:40,519 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2020-12-07 21:30:40,520 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2020-12-07 21:30:40,520 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-12-07 21:30:40,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-07 21:30:45,103 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=9fba6524-b8c9-4ee9-a0db-f3f21e06a9e8, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cc84c8e7-2264-425c-ba9f-37caf69f3ba8;nsid=828757604;c=0) storage 9fba6524-b8c9-4ee9-a0db-f3f21e06a9e8
2020-12-07 21:30:45,103 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-12-07 21:30:45,104 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2020-12-07 21:30:45,112 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=44394f8f-9265-4f0e-bace-d3b4573d2e26, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cc84c8e7-2264-425c-ba9f-37caf69f3ba8;nsid=828757604;c=0) storage 44394f8f-9265-4f0e-bace-d3b4573d2e26
2020-12-07 21:30:45,112 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-12-07 21:30:45,113 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2020-12-07 21:30:45,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-12-07 21:30:45,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2 for DN 172.18.0.4:50010
2020-12-07 21:30:45,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-12-07 21:30:45,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b699900e-cd40-477b-9d61-dba43b104704 for DN 172.18.0.3:50010
2020-12-07 21:30:45,323 INFO BlockStateChange: BLOCK* processReport: from storage DS-b699900e-cd40-477b-9d61-dba43b104704 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=44394f8f-9265-4f0e-bace-d3b4573d2e26, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cc84c8e7-2264-425c-ba9f-37caf69f3ba8;nsid=828757604;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2020-12-07 21:30:45,326 INFO BlockStateChange: BLOCK* processReport: from storage DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=9fba6524-b8c9-4ee9-a0db-f3f21e06a9e8, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cc84c8e7-2264-425c-ba9f-37caf69f3ba8;nsid=828757604;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2020-12-07 21:31:49,883 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2020-12-07 21:31:49,883 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-12-07 21:31:49,883 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2020-12-07 21:31:49,883 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 7 
2020-12-07 21:31:49,887 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 10 
2020-12-07 21:31:49,888 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2020-12-07 21:31:49,892 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2020-12-07 21:31:51,876 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2020-12-07 21:31:51,876 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2020-12-07 21:31:51,882 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2020-12-07 21:34:04,835 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 20 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 7 
2020-12-07 21:34:08,222 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:08,783 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:08,801 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-07 21:34:08,939 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:34:08,940 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:09,272 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:09,288 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741827_1003{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-07 21:34:09,380 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:09,381 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741828_1004{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-07 21:34:09,504 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:34:09,516 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:09,626 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:09,627 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741830_1006{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-07 21:34:09,711 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:34:09,716 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:09,809 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:34:09,819 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:09,910 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:34:09,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:10,029 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:10,030 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741834_1010{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2020-12-07 21:34:10,102 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:34:10,110 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:10,214 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:34:10,216 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:10,289 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:10,290 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741837_1013{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-07 21:34:10,367 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:10,368 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741838_1014{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2020-12-07 21:34:10,443 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:34:10,445 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2020-12-07 21:34:10,545 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:34:10,555 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1434275153_1
2020-12-07 21:36:30,736 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 56 Total time for transactions(ms): 25 Number of transactions batched in Syncs: 0 Number of syncs: 22 SyncTimes(ms): 67 
2020-12-07 21:38:07,753 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 59 Total time for transactions(ms): 26 Number of transactions batched in Syncs: 0 Number of syncs: 23 SyncTimes(ms): 77 
2020-12-07 21:38:07,840 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000001_0/part-00001
2020-12-07 21:38:07,875 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000000_0/part-00000
2020-12-07 21:38:08,105 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:08,105 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:08,108 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000001_0/part-00001 is closed by DFSClient_attempt_20201207213628_0000_m_000001_0_-380636597_26
2020-12-07 21:38:08,143 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:08,144 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:08,147 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000000_0/part-00000 is closed by DFSClient_attempt_20201207213628_0000_m_000000_0_2085206987_26
2020-12-07 21:38:08,279 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000002_0/part-00002
2020-12-07 21:38:08,318 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000003_0/part-00003
2020-12-07 21:38:08,360 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:08,361 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:08,365 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000002_0/part-00002 is closed by DFSClient_attempt_20201207213628_0000_m_000001_0_-380636597_26
2020-12-07 21:38:08,439 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:08,446 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:08,453 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000003_0/part-00003 is closed by DFSClient_attempt_20201207213628_0000_m_000000_0_2085206987_26
2020-12-07 21:38:08,525 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000004_0/part-00004
2020-12-07 21:38:08,573 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000005_0/part-00005
2020-12-07 21:38:08,599 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:08,603 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:08,607 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000004_0/part-00004 is closed by DFSClient_attempt_20201207213628_0000_m_000001_0_-380636597_26
2020-12-07 21:38:08,683 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:08,685 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:08,689 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000005_0/part-00005 is closed by DFSClient_attempt_20201207213628_0000_m_000000_0_2085206987_26
2020-12-07 21:38:08,726 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000006_0/part-00006
2020-12-07 21:38:08,782 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:08,784 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:08,786 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000006_0/part-00006 is closed by DFSClient_attempt_20201207213628_0000_m_000001_0_-380636597_26
2020-12-07 21:38:08,831 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000007_0/part-00007
2020-12-07 21:38:08,902 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:08,905 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:08,909 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000007_0/part-00007 is closed by DFSClient_attempt_20201207213628_0000_m_000000_0_2085206987_26
2020-12-07 21:38:08,932 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000008_0/part-00008
2020-12-07 21:38:08,999 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:09,001 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:09,008 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000008_0/part-00008 is closed by DFSClient_attempt_20201207213628_0000_m_000001_0_-380636597_26
2020-12-07 21:38:09,063 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000009_0/part-00009
2020-12-07 21:38:09,136 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000010_0/part-00010
2020-12-07 21:38:09,154 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:09,156 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:09,160 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000009_0/part-00009 is closed by DFSClient_attempt_20201207213628_0000_m_000000_0_2085206987_26
2020-12-07 21:38:09,255 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:09,257 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:09,265 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000010_0/part-00010 is closed by DFSClient_attempt_20201207213628_0000_m_000001_0_-380636597_26
2020-12-07 21:38:09,289 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000011_0/part-00011
2020-12-07 21:38:09,360 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:09,361 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:09,368 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000011_0/part-00011 is closed by DFSClient_attempt_20201207213628_0000_m_000000_0_2085206987_26
2020-12-07 21:38:09,396 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000012_0/part-00012
2020-12-07 21:38:09,476 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:09,478 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:09,493 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000012_0/part-00012 is closed by DFSClient_attempt_20201207213628_0000_m_000001_0_-380636597_26
2020-12-07 21:38:09,516 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000013_0/part-00013
2020-12-07 21:38:09,602 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:09,603 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:09,609 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000013_0/part-00013 is closed by DFSClient_attempt_20201207213628_0000_m_000000_0_2085206987_26
2020-12-07 21:38:09,640 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000014_0/part-00014
2020-12-07 21:38:09,704 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:09,706 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW]]} size 0
2020-12-07 21:38:09,710 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000014_0/part-00014 is closed by DFSClient_attempt_20201207213628_0000_m_000001_0_-380636597_26
2020-12-07 21:38:09,755 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000015_0/part-00015
2020-12-07 21:38:09,794 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:09,796 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b699900e-cd40-477b-9d61-dba43b104704:NORMAL:172.18.0.3:50010|RBW]]} size 0
2020-12-07 21:38:09,799 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20201207133630_0008_m_000015_0/part-00015 is closed by DFSClient_attempt_20201207213628_0000_m_000000_0_2085206987_26
2020-12-07 21:38:10,172 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1711405096_16
2020-12-07 22:31:52,771 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2020-12-07 22:31:52,771 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-12-07 22:31:52,771 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3
2020-12-07 22:31:52,771 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 189 Total time for transactions(ms): 32 Number of transactions batched in Syncs: 1 Number of syncs: 106 SyncTimes(ms): 239 
2020-12-07 22:31:52,775 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 189 Total time for transactions(ms): 32 Number of transactions batched in Syncs: 1 Number of syncs: 107 SyncTimes(ms): 242 
2020-12-07 22:31:52,776 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000003 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000003-0000000000000000191
2020-12-07 22:31:52,776 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 192
2020-12-07 22:31:53,051 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 125.00 KB/s
2020-12-07 22:31:53,052 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000191 size 1979 bytes.
2020-12-07 22:31:53,062 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2020-12-07 22:31:53,062 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-07 22:48:37,979 INFO BlockStateChange: BLOCK* processReport: from storage DS-9c1a3ab8-c0a4-4250-95e8-e0e32ee2edd2 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=9fba6524-b8c9-4ee9-a0db-f3f21e06a9e8, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cc84c8e7-2264-425c-ba9f-37caf69f3ba8;nsid=828757604;c=0), blocks: 24, hasStaleStorage: false, processing time: 0 msecs
2020-12-07 23:31:54,110 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2020-12-07 23:31:54,111 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-12-07 23:31:54,111 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 192
2020-12-07 23:31:54,112 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 7 
2020-12-07 23:31:54,117 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 12 
2020-12-07 23:31:54,118 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000192 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000192-0000000000000000193
2020-12-07 23:31:54,118 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 194
2020-12-07 23:31:54,172 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 333.33 KB/s
2020-12-07 23:31:54,172 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000193 size 1979 bytes.
2020-12-07 23:31:54,176 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 191
2020-12-07 23:31:54,176 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop/dfs/name/current/fsimage_0000000000000000002, cpktTxId=0000000000000000002)
2020-12-08 00:31:55,354 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2020-12-08 00:31:55,354 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-12-08 00:31:55,354 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 194
2020-12-08 00:31:55,355 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 7 
2020-12-08 00:31:55,359 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 11 
2020-12-08 00:31:55,362 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000194 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000194-0000000000000000195
2020-12-08 00:31:55,362 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 196
2020-12-08 00:31:55,438 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 125.00 KB/s
2020-12-08 00:31:55,439 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000195 size 1979 bytes.
2020-12-08 00:31:55,444 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 193
2020-12-08 00:31:55,444 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop/dfs/name/current/fsimage_0000000000000000191, cpktTxId=0000000000000000191)
2021-01-23 18:13:12,588 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2021-01-23 18:13:12,611 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2021-01-23 18:13:12,614 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2021-01-23 18:13:13,052 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2021-01-23 18:13:13,174 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2021-01-23 18:13:13,174 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2021-01-23 18:13:13,176 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2021-01-23 18:13:13,177 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2021-01-23 18:13:13,474 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2021-01-23 18:13:13,559 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2021-01-23 18:13:13,567 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2021-01-23 18:13:13,572 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2021-01-23 18:13:13,577 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2021-01-23 18:13:13,592 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2021-01-23 18:13:13,592 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2021-01-23 18:13:13,592 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2021-01-23 18:13:13,647 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2021-01-23 18:13:13,649 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2021-01-23 18:13:13,680 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2021-01-23 18:13:13,681 INFO org.mortbay.log: jetty-6.1.26
2021-01-23 18:13:13,864 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2021-01-23 18:13:13,920 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2021-01-23 18:13:13,925 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2021-01-23 18:13:13,967 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2021-01-23 18:13:13,967 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2021-01-23 18:13:14,038 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2021-01-23 18:13:14,038 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2021-01-23 18:13:14,039 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2021-01-23 18:13:14,040 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2021 Jan 23 18:13:14
2021-01-23 18:13:14,042 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2021-01-23 18:13:14,042 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:13:14,043 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2021-01-23 18:13:14,043 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2021-01-23 18:13:14,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2021-01-23 18:13:14,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2021-01-23 18:13:14,050 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2021-01-23 18:13:14,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2021-01-23 18:13:14,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2021-01-23 18:13:14,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2021-01-23 18:13:14,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2021-01-23 18:13:14,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2021-01-23 18:13:14,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2021-01-23 18:13:14,070 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2021-01-23 18:13:14,070 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2021-01-23 18:13:14,070 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2021-01-23 18:13:14,070 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2021-01-23 18:13:14,072 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2021-01-23 18:13:14,570 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2021-01-23 18:13:14,570 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:13:14,570 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2021-01-23 18:13:14,570 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2021-01-23 18:13:14,570 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2021-01-23 18:13:14,570 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2021-01-23 18:13:14,570 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2021-01-23 18:13:14,571 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2021-01-23 18:13:14,576 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2021-01-23 18:13:14,576 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:13:14,576 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2021-01-23 18:13:14,576 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2021-01-23 18:13:14,577 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2021-01-23 18:13:14,578 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2021-01-23 18:13:14,578 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2021-01-23 18:13:14,600 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2021-01-23 18:13:14,600 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2021-01-23 18:13:14,601 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2021-01-23 18:13:14,602 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2021-01-23 18:13:14,602 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2021-01-23 18:13:14,603 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2021-01-23 18:13:14,603 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:13:14,603 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2021-01-23 18:13:14,604 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2021-01-23 18:13:14,616 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 261@master
2021-01-23 18:13:14,709 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2021-01-23 18:13:14,710 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2021-01-23 18:13:14,754 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2021-01-23 18:13:14,786 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2021-01-23 18:13:14,786 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2021-01-23 18:13:14,791 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2021-01-23 18:13:14,792 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2021-01-23 18:13:14,898 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2021-01-23 18:13:14,899 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 293 msecs
2021-01-23 18:13:15,160 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2021-01-23 18:13:15,164 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2021-01-23 18:13:15,172 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2021-01-23 18:13:15,204 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2021-01-23 18:13:15,213 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2021-01-23 18:13:15,213 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2021-01-23 18:13:15,213 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2021-01-23 18:13:15,214 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2021-01-23 18:13:15,214 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2021-01-23 18:13:15,214 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2021-01-23 18:13:15,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:13:15,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2021-01-23 18:13:15,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2021-01-23 18:13:15,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2021-01-23 18:13:15,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2021-01-23 18:13:15,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2021-01-23 18:13:15,239 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 25 msec
2021-01-23 18:13:15,282 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2021-01-23 18:13:15,293 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2021-01-23 18:13:15,294 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2021-01-23 18:13:15,294 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2021-01-23 18:13:15,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2021-01-23 18:13:19,968 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=3b65bbab-e49a-4017-aece-5b34c34350bb, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-fe118d50-7a06-4140-920f-c73a086893b7;nsid=941365349;c=0) storage 3b65bbab-e49a-4017-aece-5b34c34350bb
2021-01-23 18:13:19,968 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:13:19,969 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2021-01-23 18:13:19,981 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=6c18fb42-c287-4048-a167-a2dd4f97e431, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-fe118d50-7a06-4140-920f-c73a086893b7;nsid=941365349;c=0) storage 6c18fb42-c287-4048-a167-a2dd4f97e431
2021-01-23 18:13:19,981 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:13:19,981 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2021-01-23 18:13:20,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:13:20,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-7ee1ab27-3dfe-43a2-8f9b-7ee3a6c72d37 for DN 172.18.0.4:50010
2021-01-23 18:13:20,036 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:13:20,036 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-45f34548-a5e0-4de1-9e6c-dbcc77e9ca4e for DN 172.18.0.3:50010
2021-01-23 18:13:20,064 INFO BlockStateChange: BLOCK* processReport: from storage DS-45f34548-a5e0-4de1-9e6c-dbcc77e9ca4e node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=3b65bbab-e49a-4017-aece-5b34c34350bb, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-fe118d50-7a06-4140-920f-c73a086893b7;nsid=941365349;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2021-01-23 18:13:20,065 INFO BlockStateChange: BLOCK* processReport: from storage DS-7ee1ab27-3dfe-43a2-8f9b-7ee3a6c72d37 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=6c18fb42-c287-4048-a167-a2dd4f97e431, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-fe118d50-7a06-4140-920f-c73a086893b7;nsid=941365349;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2021-01-23 18:14:11,771 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2021-01-23 18:14:11,773 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at master/172.18.0.2
************************************************************/
2021-01-23 18:15:01,880 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2021-01-23 18:15:01,904 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2021-01-23 18:15:01,910 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2021-01-23 18:15:02,398 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2021-01-23 18:15:02,529 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2021-01-23 18:15:02,529 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2021-01-23 18:15:02,531 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2021-01-23 18:15:02,532 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2021-01-23 18:15:02,824 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2021-01-23 18:15:02,898 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2021-01-23 18:15:02,903 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2021-01-23 18:15:02,909 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2021-01-23 18:15:02,912 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2021-01-23 18:15:02,915 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2021-01-23 18:15:02,915 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2021-01-23 18:15:02,927 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2021-01-23 18:15:02,974 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2021-01-23 18:15:02,977 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2021-01-23 18:15:03,003 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2021-01-23 18:15:03,003 INFO org.mortbay.log: jetty-6.1.26
2021-01-23 18:15:03,178 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2021-01-23 18:15:03,215 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2021-01-23 18:15:03,216 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2021-01-23 18:15:03,255 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2021-01-23 18:15:03,255 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2021-01-23 18:15:03,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2021-01-23 18:15:03,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2021-01-23 18:15:03,313 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2021-01-23 18:15:03,314 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2021 Jan 23 18:15:03
2021-01-23 18:15:03,316 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2021-01-23 18:15:03,316 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:15:03,317 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2021-01-23 18:15:03,317 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2021-01-23 18:15:03,333 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2021-01-23 18:15:03,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2021-01-23 18:15:03,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2021-01-23 18:15:03,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2021-01-23 18:15:03,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2021-01-23 18:15:03,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2021-01-23 18:15:03,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2021-01-23 18:15:03,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2021-01-23 18:15:03,336 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2021-01-23 18:15:03,340 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2021-01-23 18:15:03,341 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2021-01-23 18:15:03,341 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2021-01-23 18:15:03,341 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2021-01-23 18:15:03,342 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2021-01-23 18:15:03,747 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2021-01-23 18:15:03,747 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:15:03,748 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2021-01-23 18:15:03,748 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2021-01-23 18:15:03,748 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2021-01-23 18:15:03,748 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2021-01-23 18:15:03,748 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2021-01-23 18:15:03,748 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2021-01-23 18:15:03,753 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2021-01-23 18:15:03,753 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:15:03,753 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2021-01-23 18:15:03,753 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2021-01-23 18:15:03,754 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2021-01-23 18:15:03,756 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2021-01-23 18:15:03,756 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2021-01-23 18:15:03,770 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2021-01-23 18:15:03,771 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2021-01-23 18:15:03,771 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2021-01-23 18:15:03,772 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2021-01-23 18:15:03,772 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2021-01-23 18:15:03,774 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2021-01-23 18:15:03,774 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:15:03,774 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2021-01-23 18:15:03,774 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2021-01-23 18:15:03,789 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 969@master
2021-01-23 18:15:03,881 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2021-01-23 18:15:03,882 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2021-01-23 18:15:03,932 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2021-01-23 18:15:03,965 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2021-01-23 18:15:03,965 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2021-01-23 18:15:03,970 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2021-01-23 18:15:03,971 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2021-01-23 18:15:04,102 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2021-01-23 18:15:04,102 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 326 msecs
2021-01-23 18:15:04,332 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2021-01-23 18:15:04,336 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2021-01-23 18:15:04,355 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2021-01-23 18:15:04,381 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2021-01-23 18:15:04,395 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2021-01-23 18:15:04,395 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2021-01-23 18:15:04,395 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2021-01-23 18:15:04,396 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2021-01-23 18:15:04,396 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2021-01-23 18:15:04,396 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2021-01-23 18:15:04,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:15:04,419 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2021-01-23 18:15:04,419 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2021-01-23 18:15:04,419 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2021-01-23 18:15:04,419 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2021-01-23 18:15:04,419 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2021-01-23 18:15:04,419 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 23 msec
2021-01-23 18:15:04,457 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2021-01-23 18:15:04,458 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2021-01-23 18:15:04,460 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2021-01-23 18:15:04,460 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2021-01-23 18:15:04,463 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2021-01-23 18:15:09,066 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=605c1f7c-76c7-4402-917d-3d946fa601c1, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-07353671-4518-42c4-b437-17e5e4081e03;nsid=767045300;c=0) storage 605c1f7c-76c7-4402-917d-3d946fa601c1
2021-01-23 18:15:09,069 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:15:09,070 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2021-01-23 18:15:09,118 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=49bc166e-c32a-47f2-834e-02b9a920af15, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-07353671-4518-42c4-b437-17e5e4081e03;nsid=767045300;c=0) storage 49bc166e-c32a-47f2-834e-02b9a920af15
2021-01-23 18:15:09,118 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:15:09,119 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2021-01-23 18:15:09,162 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:15:09,162 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-eb597a00-1cea-4c90-84ae-9517edae9ac2 for DN 172.18.0.3:50010
2021-01-23 18:15:09,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:15:09,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072 for DN 172.18.0.4:50010
2021-01-23 18:15:09,207 INFO BlockStateChange: BLOCK* processReport: from storage DS-eb597a00-1cea-4c90-84ae-9517edae9ac2 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=605c1f7c-76c7-4402-917d-3d946fa601c1, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-07353671-4518-42c4-b437-17e5e4081e03;nsid=767045300;c=0), blocks: 0, hasStaleStorage: false, processing time: 7 msecs
2021-01-23 18:15:09,225 INFO BlockStateChange: BLOCK* processReport: from storage DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=49bc166e-c32a-47f2-834e-02b9a920af15, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-07353671-4518-42c4-b437-17e5e4081e03;nsid=767045300;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2021-01-23 18:16:13,785 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2021-01-23 18:16:13,785 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2021-01-23 18:16:13,785 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2021-01-23 18:16:13,786 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 8 
2021-01-23 18:16:13,789 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 12 
2021-01-23 18:16:13,791 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2021-01-23 18:16:13,793 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2021-01-23 18:16:14,724 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2021-01-23 18:16:14,724 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2021-01-23 18:16:14,732 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2021-01-23 18:16:33,092 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:33,726 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-eb597a00-1cea-4c90-84ae-9517edae9ac2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:33,776 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2021-01-23 18:16:34,162 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:34,189 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741826_1002{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-eb597a00-1cea-4c90-84ae-9517edae9ac2:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2021-01-23 18:16:34,324 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:34,325 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741827_1003{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2021-01-23 18:16:34,430 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:34,430 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741828_1004{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2021-01-23 18:16:34,520 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:16:34,522 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-eb597a00-1cea-4c90-84ae-9517edae9ac2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:34,648 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:34,649 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741830_1006{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-eb597a00-1cea-4c90-84ae-9517edae9ac2:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2021-01-23 18:16:34,779 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:34,785 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741831_1007{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2021-01-23 18:16:34,881 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:34,882 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741832_1008{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2021-01-23 18:16:34,982 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-eb597a00-1cea-4c90-84ae-9517edae9ac2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:34,983 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741833_1009{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2021-01-23 18:16:35,060 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-eb597a00-1cea-4c90-84ae-9517edae9ac2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:16:35,062 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:35,135 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:16:35,137 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-eb597a00-1cea-4c90-84ae-9517edae9ac2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:35,217 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-eb597a00-1cea-4c90-84ae-9517edae9ac2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:16:35,218 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-eb597a00-1cea-4c90-84ae-9517edae9ac2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:35,298 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-eb597a00-1cea-4c90-84ae-9517edae9ac2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:16:35,302 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-eb597a00-1cea-4c90-84ae-9517edae9ac2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:35,365 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-eb597a00-1cea-4c90-84ae-9517edae9ac2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:16:35,368 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:35,457 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:16:35,459 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:16:35,531 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-fc88d270-c1a3-4b32-8b5d-eb168fc31072:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:16:35,536 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_36361330_1
2021-01-23 18:19:00,847 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2021-01-23 18:19:00,867 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2021-01-23 18:19:00,871 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2021-01-23 18:19:01,321 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2021-01-23 18:19:01,449 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2021-01-23 18:19:01,449 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2021-01-23 18:19:01,450 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2021-01-23 18:19:01,451 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2021-01-23 18:19:01,748 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2021-01-23 18:19:01,822 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2021-01-23 18:19:01,844 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2021-01-23 18:19:01,850 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2021-01-23 18:19:01,854 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2021-01-23 18:19:01,857 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2021-01-23 18:19:01,857 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2021-01-23 18:19:01,857 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2021-01-23 18:19:01,924 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2021-01-23 18:19:01,926 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2021-01-23 18:19:01,960 INFO org.apache.hadoop.http.HttpServer2: HttpServer.start() threw a non Bind IOException
java.net.BindException: Port in use: 0.0.0.0:50070
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:919)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:856)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:142)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:752)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:638)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:811)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:795)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1488)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1554)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:914)
	... 8 more
2021-01-23 18:19:01,963 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2021-01-23 18:19:01,964 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2021-01-23 18:19:01,964 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2021-01-23 18:19:01,964 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
java.net.BindException: Port in use: 0.0.0.0:50070
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:919)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:856)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:142)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:752)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:638)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:811)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:795)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1488)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1554)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:914)
	... 8 more
2021-01-23 18:19:01,966 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2021-01-23 18:19:01,966 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at master/172.18.0.2
************************************************************/
2021-01-23 18:20:29,605 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2021-01-23 18:20:29,612 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2021-01-23 18:20:29,632 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2021-01-23 18:20:30,071 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2021-01-23 18:20:30,197 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2021-01-23 18:20:30,198 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2021-01-23 18:20:30,199 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2021-01-23 18:20:30,200 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2021-01-23 18:20:30,500 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2021-01-23 18:20:30,575 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2021-01-23 18:20:30,586 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2021-01-23 18:20:30,590 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2021-01-23 18:20:30,595 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2021-01-23 18:20:30,598 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2021-01-23 18:20:30,598 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2021-01-23 18:20:30,598 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2021-01-23 18:20:30,664 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2021-01-23 18:20:30,665 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2021-01-23 18:20:30,697 INFO org.apache.hadoop.http.HttpServer2: HttpServer.start() threw a non Bind IOException
java.net.BindException: Port in use: 0.0.0.0:50070
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:919)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:856)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:142)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:752)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:638)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:811)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:795)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1488)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1554)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:914)
	... 8 more
2021-01-23 18:20:30,701 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2021-01-23 18:20:30,701 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2021-01-23 18:20:30,702 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2021-01-23 18:20:30,702 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
java.net.BindException: Port in use: 0.0.0.0:50070
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:919)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:856)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:142)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:752)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:638)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:811)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:795)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1488)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1554)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:914)
	... 8 more
2021-01-23 18:20:30,703 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2021-01-23 18:20:30,704 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at master/172.18.0.2
************************************************************/
2021-01-23 18:22:49,638 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2021-01-23 18:22:49,644 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2021-01-23 18:22:49,648 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2021-01-23 18:22:50,088 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2021-01-23 18:22:50,217 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2021-01-23 18:22:50,217 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2021-01-23 18:22:50,219 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2021-01-23 18:22:50,220 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2021-01-23 18:22:50,519 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2021-01-23 18:22:50,601 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2021-01-23 18:22:50,607 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2021-01-23 18:22:50,612 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2021-01-23 18:22:50,618 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2021-01-23 18:22:50,638 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2021-01-23 18:22:50,638 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2021-01-23 18:22:50,638 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2021-01-23 18:22:50,688 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2021-01-23 18:22:50,689 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2021-01-23 18:22:50,721 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2021-01-23 18:22:50,722 INFO org.mortbay.log: jetty-6.1.26
2021-01-23 18:22:50,921 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2021-01-23 18:22:50,957 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2021-01-23 18:22:50,957 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2021-01-23 18:22:51,009 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2021-01-23 18:22:51,009 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2021-01-23 18:22:51,060 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2021-01-23 18:22:51,061 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2021-01-23 18:22:51,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2021-01-23 18:22:51,067 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2021 Jan 23 18:22:51
2021-01-23 18:22:51,069 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2021-01-23 18:22:51,069 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:22:51,070 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2021-01-23 18:22:51,070 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2021-01-23 18:22:51,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2021-01-23 18:22:51,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2021-01-23 18:22:51,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2021-01-23 18:22:51,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2021-01-23 18:22:51,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2021-01-23 18:22:51,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2021-01-23 18:22:51,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2021-01-23 18:22:51,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2021-01-23 18:22:51,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2021-01-23 18:22:51,078 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2021-01-23 18:22:51,078 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2021-01-23 18:22:51,078 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2021-01-23 18:22:51,078 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2021-01-23 18:22:51,079 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2021-01-23 18:22:51,462 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2021-01-23 18:22:51,462 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:22:51,462 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2021-01-23 18:22:51,462 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2021-01-23 18:22:51,463 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2021-01-23 18:22:51,463 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2021-01-23 18:22:51,463 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2021-01-23 18:22:51,463 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2021-01-23 18:22:51,481 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2021-01-23 18:22:51,481 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:22:51,482 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2021-01-23 18:22:51,482 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2021-01-23 18:22:51,482 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2021-01-23 18:22:51,484 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2021-01-23 18:22:51,484 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2021-01-23 18:22:51,486 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2021-01-23 18:22:51,486 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2021-01-23 18:22:51,486 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2021-01-23 18:22:51,487 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2021-01-23 18:22:51,487 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2021-01-23 18:22:51,488 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2021-01-23 18:22:51,488 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:22:51,489 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2021-01-23 18:22:51,489 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2021-01-23 18:22:51,505 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 2807@master
2021-01-23 18:22:51,575 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2021-01-23 18:22:51,591 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2021-01-23 18:22:51,632 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2021-01-23 18:22:51,677 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2021-01-23 18:22:51,678 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2021-01-23 18:22:51,683 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2021-01-23 18:22:51,683 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2021-01-23 18:22:51,810 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2021-01-23 18:22:51,810 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 319 msecs
2021-01-23 18:22:52,044 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2021-01-23 18:22:52,048 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2021-01-23 18:22:52,059 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2021-01-23 18:22:52,083 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2021-01-23 18:22:52,089 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2021-01-23 18:22:52,089 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2021-01-23 18:22:52,089 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2021-01-23 18:22:52,090 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2021-01-23 18:22:52,090 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2021-01-23 18:22:52,090 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2021-01-23 18:22:52,104 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:22:52,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2021-01-23 18:22:52,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2021-01-23 18:22:52,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2021-01-23 18:22:52,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2021-01-23 18:22:52,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2021-01-23 18:22:52,108 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 17 msec
2021-01-23 18:22:52,139 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2021-01-23 18:22:52,140 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2021-01-23 18:22:52,141 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2021-01-23 18:22:52,141 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2021-01-23 18:22:52,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2021-01-23 18:22:56,884 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=02c7cbc0-07a5-4c67-bff3-bec318438237, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-83b1748a-0185-426e-8406-b2e797cd3ba0;nsid=460839116;c=0) storage 02c7cbc0-07a5-4c67-bff3-bec318438237
2021-01-23 18:22:56,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:22:56,887 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2021-01-23 18:22:56,903 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=74907d78-11c8-417d-af02-d3ca0173a0d4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-83b1748a-0185-426e-8406-b2e797cd3ba0;nsid=460839116;c=0) storage 74907d78-11c8-417d-af02-d3ca0173a0d4
2021-01-23 18:22:56,903 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:22:56,903 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2021-01-23 18:22:56,957 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:22:56,957 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-da105129-fbb2-4565-a78d-d01b4dc863fb for DN 172.18.0.3:50010
2021-01-23 18:22:56,965 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:22:56,965 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-fc628a7a-36c3-4185-a972-dd4e323584f3 for DN 172.18.0.4:50010
2021-01-23 18:22:56,989 INFO BlockStateChange: BLOCK* processReport: from storage DS-fc628a7a-36c3-4185-a972-dd4e323584f3 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=02c7cbc0-07a5-4c67-bff3-bec318438237, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-83b1748a-0185-426e-8406-b2e797cd3ba0;nsid=460839116;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2021-01-23 18:22:57,006 INFO BlockStateChange: BLOCK* processReport: from storage DS-da105129-fbb2-4565-a78d-d01b4dc863fb node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=74907d78-11c8-417d-af02-d3ca0173a0d4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-83b1748a-0185-426e-8406-b2e797cd3ba0;nsid=460839116;c=0), blocks: 0, hasStaleStorage: false, processing time: 3 msecs
2021-01-23 18:24:01,361 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2021-01-23 18:24:01,361 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2021-01-23 18:24:01,362 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2021-01-23 18:24:01,362 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 8 
2021-01-23 18:24:01,366 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 12 
2021-01-23 18:24:01,368 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2021-01-23 18:24:01,374 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2021-01-23 18:24:02,452 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2021-01-23 18:24:02,452 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2021-01-23 18:24:02,460 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2021-01-23 18:32:52,016 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2021-01-23 18:32:52,023 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2021-01-23 18:32:52,026 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2021-01-23 18:32:52,423 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2021-01-23 18:32:52,528 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2021-01-23 18:32:52,528 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2021-01-23 18:32:52,529 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2021-01-23 18:32:52,530 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2021-01-23 18:32:52,788 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2021-01-23 18:32:52,851 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2021-01-23 18:32:52,856 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2021-01-23 18:32:52,860 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2021-01-23 18:32:52,864 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2021-01-23 18:32:52,866 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2021-01-23 18:32:52,866 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2021-01-23 18:32:52,866 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2021-01-23 18:32:52,921 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2021-01-23 18:32:52,922 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2021-01-23 18:32:52,947 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2021-01-23 18:32:52,948 INFO org.mortbay.log: jetty-6.1.26
2021-01-23 18:32:53,094 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2021-01-23 18:32:53,141 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2021-01-23 18:32:53,142 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2021-01-23 18:32:53,176 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2021-01-23 18:32:53,176 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2021-01-23 18:32:53,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2021-01-23 18:32:53,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2021-01-23 18:32:53,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2021-01-23 18:32:53,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2021 Jan 23 18:32:53
2021-01-23 18:32:53,241 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2021-01-23 18:32:53,241 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:32:53,242 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2021-01-23 18:32:53,242 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2021-01-23 18:32:53,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2021-01-23 18:32:53,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2021-01-23 18:32:53,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2021-01-23 18:32:53,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2021-01-23 18:32:53,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2021-01-23 18:32:53,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2021-01-23 18:32:53,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2021-01-23 18:32:53,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2021-01-23 18:32:53,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2021-01-23 18:32:53,265 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2021-01-23 18:32:53,265 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2021-01-23 18:32:53,265 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2021-01-23 18:32:53,265 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2021-01-23 18:32:53,267 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2021-01-23 18:32:53,733 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2021-01-23 18:32:53,733 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:32:53,734 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2021-01-23 18:32:53,734 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2021-01-23 18:32:53,734 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2021-01-23 18:32:53,734 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2021-01-23 18:32:53,734 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2021-01-23 18:32:53,734 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2021-01-23 18:32:53,752 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2021-01-23 18:32:53,752 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:32:53,752 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2021-01-23 18:32:53,752 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2021-01-23 18:32:53,753 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2021-01-23 18:32:53,754 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2021-01-23 18:32:53,754 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2021-01-23 18:32:53,758 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2021-01-23 18:32:53,758 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2021-01-23 18:32:53,759 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2021-01-23 18:32:53,761 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2021-01-23 18:32:53,761 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2021-01-23 18:32:53,762 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2021-01-23 18:32:53,763 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:32:53,763 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2021-01-23 18:32:53,763 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2021-01-23 18:32:53,781 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 156@master
2021-01-23 18:32:53,881 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2021-01-23 18:32:53,882 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2021-01-23 18:32:53,926 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2021-01-23 18:32:53,963 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2021-01-23 18:32:53,963 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2021-01-23 18:32:53,969 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2021-01-23 18:32:53,969 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2021-01-23 18:32:54,088 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2021-01-23 18:32:54,089 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 324 msecs
2021-01-23 18:32:54,382 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2021-01-23 18:32:54,386 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2021-01-23 18:32:54,401 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2021-01-23 18:32:54,445 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2021-01-23 18:32:54,452 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2021-01-23 18:32:54,452 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2021-01-23 18:32:54,452 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2021-01-23 18:32:54,452 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2021-01-23 18:32:54,453 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2021-01-23 18:32:54,453 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2021-01-23 18:32:54,476 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:32:54,487 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2021-01-23 18:32:54,487 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2021-01-23 18:32:54,487 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2021-01-23 18:32:54,487 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2021-01-23 18:32:54,488 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2021-01-23 18:32:54,488 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 34 msec
2021-01-23 18:32:54,539 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2021-01-23 18:32:54,541 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2021-01-23 18:32:54,542 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2021-01-23 18:32:54,543 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2021-01-23 18:32:54,547 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2021-01-23 18:32:59,374 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=00e544a8-ec68-4c16-8406-8ca804a688ca, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-f5bdc4db-1e67-4bfa-b5be-12664398e087;nsid=629590770;c=0) storage 00e544a8-ec68-4c16-8406-8ca804a688ca
2021-01-23 18:32:59,377 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:32:59,377 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2021-01-23 18:32:59,404 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=255eec5f-6ad0-45d0-bc3d-f0dcf406478f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-f5bdc4db-1e67-4bfa-b5be-12664398e087;nsid=629590770;c=0) storage 255eec5f-6ad0-45d0-bc3d-f0dcf406478f
2021-01-23 18:32:59,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:32:59,405 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2021-01-23 18:32:59,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:32:59,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-f50185f4-129a-410a-be1e-92852ebcebd3 for DN 172.18.0.4:50010
2021-01-23 18:32:59,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:32:59,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b5da0522-a7f0-4cc3-b669-8919a14503e6 for DN 172.18.0.3:50010
2021-01-23 18:32:59,473 INFO BlockStateChange: BLOCK* processReport: from storage DS-f50185f4-129a-410a-be1e-92852ebcebd3 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=00e544a8-ec68-4c16-8406-8ca804a688ca, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-f5bdc4db-1e67-4bfa-b5be-12664398e087;nsid=629590770;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2021-01-23 18:32:59,485 INFO BlockStateChange: BLOCK* processReport: from storage DS-b5da0522-a7f0-4cc3-b669-8919a14503e6 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=255eec5f-6ad0-45d0-bc3d-f0dcf406478f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-f5bdc4db-1e67-4bfa-b5be-12664398e087;nsid=629590770;c=0), blocks: 0, hasStaleStorage: false, processing time: 8 msecs
2021-01-23 18:34:04,006 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2021-01-23 18:34:04,006 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2021-01-23 18:34:04,006 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2021-01-23 18:34:04,006 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 10 
2021-01-23 18:34:04,011 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 14 
2021-01-23 18:34:04,012 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2021-01-23 18:34:04,013 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2021-01-23 18:34:04,897 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2021-01-23 18:34:04,897 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2021-01-23 18:34:04,905 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2021-01-23 18:35:16,473 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 6 
2021-01-23 18:35:19,215 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:19,666 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:35:19,668 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:19,910 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:19,921 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741826_1002{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2021-01-23 18:35:20,017 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:35:20,020 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:20,113 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:35:20,116 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:20,203 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:35:20,204 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:20,287 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:35:20,290 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:20,392 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:35:20,394 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:20,468 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:35:20,470 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:20,537 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:35:20,539 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:20,650 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:20,651 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741834_1010{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2021-01-23 18:35:20,735 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:35:20,735 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:20,821 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:35:20,823 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:20,911 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:35:20,912 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:20,975 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:35:20,978 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:21,069 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:35:21,072 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:35:21,146 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:35:21,154 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-105051857_1
2021-01-23 18:36:35,538 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 59 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 0 Number of syncs: 23 SyncTimes(ms): 109 
2021-01-23 18:36:35,626 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000001_0/part-00001
2021-01-23 18:36:35,644 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000000_0/part-00000
2021-01-23 18:36:35,825 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:35,826 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:35,832 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000001_0/part-00001 is closed by DFSClient_attempt_20210123183538_0000_m_000000_0_-1063595375_26
2021-01-23 18:36:35,839 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:35,840 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:35,844 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000000_0/part-00000 is closed by DFSClient_attempt_20210123183538_0000_m_000001_0_907132721_26
2021-01-23 18:36:35,967 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000003_0/part-00003
2021-01-23 18:36:35,971 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000002_0/part-00002
2021-01-23 18:36:36,084 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:36,084 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:36,089 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:36,089 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:36,090 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000003_0/part-00003 is closed by DFSClient_attempt_20210123183538_0000_m_000001_0_907132721_26
2021-01-23 18:36:36,092 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000002_0/part-00002 is closed by DFSClient_attempt_20210123183538_0000_m_000000_0_-1063595375_26
2021-01-23 18:36:36,189 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000004_0/part-00004
2021-01-23 18:36:36,206 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000005_0/part-00005
2021-01-23 18:36:36,260 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:36,260 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:36,264 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000004_0/part-00004 is closed by DFSClient_attempt_20210123183538_0000_m_000001_0_907132721_26
2021-01-23 18:36:36,322 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:36,323 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:36,327 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000005_0/part-00005 is closed by DFSClient_attempt_20210123183538_0000_m_000000_0_-1063595375_26
2021-01-23 18:36:36,396 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000006_0/part-00006
2021-01-23 18:36:36,430 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:36,432 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:36,434 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000006_0/part-00006 is closed by DFSClient_attempt_20210123183538_0000_m_000001_0_907132721_26
2021-01-23 18:36:36,455 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000007_0/part-00007
2021-01-23 18:36:36,528 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:36,530 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:36,532 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000007_0/part-00007 is closed by DFSClient_attempt_20210123183538_0000_m_000000_0_-1063595375_26
2021-01-23 18:36:36,542 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000008_0/part-00008
2021-01-23 18:36:36,622 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:36,635 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:36,638 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000008_0/part-00008 is closed by DFSClient_attempt_20210123183538_0000_m_000001_0_907132721_26
2021-01-23 18:36:36,649 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000009_0/part-00009
2021-01-23 18:36:36,717 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:36,725 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:36,728 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000009_0/part-00009 is closed by DFSClient_attempt_20210123183538_0000_m_000000_0_-1063595375_26
2021-01-23 18:36:36,744 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000010_0/part-00010
2021-01-23 18:36:36,819 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:36,820 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:36,822 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000010_0/part-00010 is closed by DFSClient_attempt_20210123183538_0000_m_000001_0_907132721_26
2021-01-23 18:36:36,828 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000011_0/part-00011
2021-01-23 18:36:36,899 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:36,901 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:36,903 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000011_0/part-00011 is closed by DFSClient_attempt_20210123183538_0000_m_000000_0_-1063595375_26
2021-01-23 18:36:36,937 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000012_0/part-00012
2021-01-23 18:36:36,974 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:36,977 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:36,984 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000012_0/part-00012 is closed by DFSClient_attempt_20210123183538_0000_m_000001_0_907132721_26
2021-01-23 18:36:37,025 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000013_0/part-00013
2021-01-23 18:36:37,079 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000014_0/part-00014
2021-01-23 18:36:37,080 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:37,080 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:37,088 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000013_0/part-00013 is closed by DFSClient_attempt_20210123183538_0000_m_000000_0_-1063595375_26
2021-01-23 18:36:37,118 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:37,118 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:36:37,122 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000014_0/part-00014 is closed by DFSClient_attempt_20210123183538_0000_m_000001_0_907132721_26
2021-01-23 18:36:37,190 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000015_0/part-00015
2021-01-23 18:36:37,216 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:37,219 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f50185f4-129a-410a-be1e-92852ebcebd3:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b5da0522-a7f0-4cc3-b669-8919a14503e6:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:36:37,221 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20210123183539_0008_m_000015_0/part-00015 is closed by DFSClient_attempt_20210123183538_0000_m_000000_0_-1063595375_26
2021-01-23 18:36:37,564 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_728534441_16
2021-01-23 18:43:19,159 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2021-01-23 18:43:19,183 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2021-01-23 18:43:19,188 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2021-01-23 18:43:19,604 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2021-01-23 18:43:19,707 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2021-01-23 18:43:19,707 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2021-01-23 18:43:19,709 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2021-01-23 18:43:19,709 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2021-01-23 18:43:19,971 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2021-01-23 18:43:20,038 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2021-01-23 18:43:20,043 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2021-01-23 18:43:20,051 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2021-01-23 18:43:20,056 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2021-01-23 18:43:20,072 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2021-01-23 18:43:20,072 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2021-01-23 18:43:20,072 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2021-01-23 18:43:20,115 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2021-01-23 18:43:20,116 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2021-01-23 18:43:20,141 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2021-01-23 18:43:20,142 INFO org.mortbay.log: jetty-6.1.26
2021-01-23 18:43:20,318 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2021-01-23 18:43:20,352 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2021-01-23 18:43:20,352 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2021-01-23 18:43:20,398 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2021-01-23 18:43:20,399 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2021-01-23 18:43:20,455 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2021-01-23 18:43:20,455 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2021-01-23 18:43:20,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2021-01-23 18:43:20,457 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2021 Jan 23 18:43:20
2021-01-23 18:43:20,462 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2021-01-23 18:43:20,462 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:43:20,463 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2021-01-23 18:43:20,463 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2021-01-23 18:43:20,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2021-01-23 18:43:20,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2021-01-23 18:43:20,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2021-01-23 18:43:20,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2021-01-23 18:43:20,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2021-01-23 18:43:20,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2021-01-23 18:43:20,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2021-01-23 18:43:20,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2021-01-23 18:43:20,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2021-01-23 18:43:20,476 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2021-01-23 18:43:20,476 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2021-01-23 18:43:20,476 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2021-01-23 18:43:20,477 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2021-01-23 18:43:20,478 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2021-01-23 18:43:20,839 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2021-01-23 18:43:20,839 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:43:20,839 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2021-01-23 18:43:20,839 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2021-01-23 18:43:20,840 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2021-01-23 18:43:20,840 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2021-01-23 18:43:20,840 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2021-01-23 18:43:20,840 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2021-01-23 18:43:20,845 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2021-01-23 18:43:20,845 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:43:20,846 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2021-01-23 18:43:20,846 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2021-01-23 18:43:20,847 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2021-01-23 18:43:20,847 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2021-01-23 18:43:20,847 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2021-01-23 18:43:20,848 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2021-01-23 18:43:20,848 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2021-01-23 18:43:20,849 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2021-01-23 18:43:20,849 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2021-01-23 18:43:20,849 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2021-01-23 18:43:20,851 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2021-01-23 18:43:20,851 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-23 18:43:20,851 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2021-01-23 18:43:20,851 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2021-01-23 18:43:20,879 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 157@master
2021-01-23 18:43:20,955 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2021-01-23 18:43:20,955 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2021-01-23 18:43:21,006 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2021-01-23 18:43:21,039 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2021-01-23 18:43:21,040 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2021-01-23 18:43:21,045 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2021-01-23 18:43:21,045 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2021-01-23 18:43:21,198 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2021-01-23 18:43:21,199 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 346 msecs
2021-01-23 18:43:21,427 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2021-01-23 18:43:21,430 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2021-01-23 18:43:21,456 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2021-01-23 18:43:21,495 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2021-01-23 18:43:21,514 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2021-01-23 18:43:21,514 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2021-01-23 18:43:21,514 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2021-01-23 18:43:21,520 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2021-01-23 18:43:21,520 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2021-01-23 18:43:21,520 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2021-01-23 18:43:21,532 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:43:21,560 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2021-01-23 18:43:21,560 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2021-01-23 18:43:21,560 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2021-01-23 18:43:21,560 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2021-01-23 18:43:21,561 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2021-01-23 18:43:21,561 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 40 msec
2021-01-23 18:43:21,607 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2021-01-23 18:43:21,620 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2021-01-23 18:43:21,629 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2021-01-23 18:43:21,629 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2021-01-23 18:43:21,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2021-01-23 18:43:26,344 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=16806596-a856-45a5-b831-2d88df5b61ae, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-1b155ced-ce84-468a-b3c8-042c1a2a88cf;nsid=1020823805;c=0) storage 16806596-a856-45a5-b831-2d88df5b61ae
2021-01-23 18:43:26,344 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:43:26,345 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2021-01-23 18:43:26,358 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=24d6c4b1-3ada-42dc-bf92-9e436eb6d372, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-1b155ced-ce84-468a-b3c8-042c1a2a88cf;nsid=1020823805;c=0) storage 24d6c4b1-3ada-42dc-bf92-9e436eb6d372
2021-01-23 18:43:26,358 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:43:26,358 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2021-01-23 18:43:26,445 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:43:26,445 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-7aa6d585-f370-4f22-95be-ead35b244012 for DN 172.18.0.4:50010
2021-01-23 18:43:26,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-23 18:43:26,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb for DN 172.18.0.3:50010
2021-01-23 18:43:26,486 INFO BlockStateChange: BLOCK* processReport: from storage DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=24d6c4b1-3ada-42dc-bf92-9e436eb6d372, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-1b155ced-ce84-468a-b3c8-042c1a2a88cf;nsid=1020823805;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2021-01-23 18:43:26,496 INFO BlockStateChange: BLOCK* processReport: from storage DS-7aa6d585-f370-4f22-95be-ead35b244012 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=16806596-a856-45a5-b831-2d88df5b61ae, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-1b155ced-ce84-468a-b3c8-042c1a2a88cf;nsid=1020823805;c=0), blocks: 0, hasStaleStorage: false, processing time: 3 msecs
2021-01-23 18:44:07,750 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:08,141 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:08,155 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2021-01-23 18:44:08,261 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:44:08,262 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:08,476 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:08,481 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741827_1003{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2021-01-23 18:44:08,544 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:44:08,546 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:08,640 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:08,641 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741829_1005{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2021-01-23 18:44:08,708 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:44:08,711 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:08,795 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:08,796 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741831_1007{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2021-01-23 18:44:08,858 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:44:08,860 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:08,941 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:44:08,943 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:09,008 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:44:09,010 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:09,091 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:44:09,093 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:09,180 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:44:09,182 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:09,261 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:09,262 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741837_1013{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2021-01-23 18:44:09,346 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:44:09,349 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:09,409 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb:NORMAL:172.18.0.3:50010|RBW]]} size 0
2021-01-23 18:44:09,412 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2021-01-23 18:44:09,470 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7aa6d585-f370-4f22-95be-ead35b244012:NORMAL:172.18.0.4:50010|RBW]]} size 0
2021-01-23 18:44:09,476 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1698404395_1
2021-01-23 18:44:30,666 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 56 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 0 Number of syncs: 22 SyncTimes(ms): 81 
2021-01-23 18:44:31,208 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2021-01-23 18:44:31,208 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2021-01-23 18:44:31,208 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2021-01-23 18:44:31,210 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 57 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 0 Number of syncs: 24 SyncTimes(ms): 93 
2021-01-23 18:44:31,212 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000057
2021-01-23 18:44:31,214 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 58
2021-01-23 18:44:32,734 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2021-01-23 18:44:32,735 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000057 size 935 bytes.
2021-01-23 18:44:32,739 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2021-01-23 19:44:33,814 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2021-01-23 19:44:33,814 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2021-01-23 19:44:33,814 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 58
2021-01-23 19:44:33,816 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 1 Number of syncs: 3 SyncTimes(ms): 15 
2021-01-23 19:44:33,818 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 1 Number of syncs: 4 SyncTimes(ms): 18 
2021-01-23 19:44:33,820 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000058 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000058-0000000000000000060
2021-01-23 19:44:33,820 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 61
2021-01-23 19:44:33,881 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2021-01-23 19:44:33,882 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000060 size 806 bytes.
2021-01-23 19:44:33,888 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 57
2021-01-23 19:44:33,889 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2021-01-23 20:02:16,455 INFO BlockStateChange: BLOCK* processReport: from storage DS-be9fc9bd-ae4d-4165-afcf-693f4f3a0cfb node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=24d6c4b1-3ada-42dc-bf92-9e436eb6d372, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-1b155ced-ce84-468a-b3c8-042c1a2a88cf;nsid=1020823805;c=0), blocks: 8, hasStaleStorage: false, processing time: 2 msecs
2021-01-27 08:53:29,335 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2021-01-27 08:53:29,346 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2021-01-27 08:53:29,349 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2021-01-27 08:53:29,705 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2021-01-27 08:53:29,806 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2021-01-27 08:53:29,806 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2021-01-27 08:53:29,808 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2021-01-27 08:53:29,808 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2021-01-27 08:53:30,032 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2021-01-27 08:53:30,094 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2021-01-27 08:53:30,099 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2021-01-27 08:53:30,104 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2021-01-27 08:53:30,107 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2021-01-27 08:53:30,108 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2021-01-27 08:53:30,109 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2021-01-27 08:53:30,109 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2021-01-27 08:53:30,157 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2021-01-27 08:53:30,158 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2021-01-27 08:53:30,181 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2021-01-27 08:53:30,181 INFO org.mortbay.log: jetty-6.1.26
2021-01-27 08:53:30,342 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2021-01-27 08:53:30,370 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2021-01-27 08:53:30,370 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2021-01-27 08:53:30,411 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2021-01-27 08:53:30,411 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2021-01-27 08:53:30,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2021-01-27 08:53:30,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2021-01-27 08:53:30,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2021-01-27 08:53:30,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2021 Jan 27 08:53:30
2021-01-27 08:53:30,472 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2021-01-27 08:53:30,472 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-27 08:53:30,473 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2021-01-27 08:53:30,473 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2021-01-27 08:53:30,476 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2021-01-27 08:53:30,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2021-01-27 08:53:30,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2021-01-27 08:53:30,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2021-01-27 08:53:30,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2021-01-27 08:53:30,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2021-01-27 08:53:30,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2021-01-27 08:53:30,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2021-01-27 08:53:30,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2021-01-27 08:53:30,496 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2021-01-27 08:53:30,496 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2021-01-27 08:53:30,496 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2021-01-27 08:53:30,496 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2021-01-27 08:53:30,498 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2021-01-27 08:53:30,818 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2021-01-27 08:53:30,818 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-27 08:53:30,818 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2021-01-27 08:53:30,818 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2021-01-27 08:53:30,818 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2021-01-27 08:53:30,819 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2021-01-27 08:53:30,819 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2021-01-27 08:53:30,819 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2021-01-27 08:53:30,834 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2021-01-27 08:53:30,834 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-27 08:53:30,834 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2021-01-27 08:53:30,834 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2021-01-27 08:53:30,835 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2021-01-27 08:53:30,835 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2021-01-27 08:53:30,835 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2021-01-27 08:53:30,838 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2021-01-27 08:53:30,838 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2021-01-27 08:53:30,839 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2021-01-27 08:53:30,840 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2021-01-27 08:53:30,840 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2021-01-27 08:53:30,841 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2021-01-27 08:53:30,841 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2021-01-27 08:53:30,841 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2021-01-27 08:53:30,841 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2021-01-27 08:53:30,851 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 147@master
2021-01-27 08:53:30,924 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2021-01-27 08:53:30,924 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2021-01-27 08:53:30,951 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2021-01-27 08:53:30,975 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2021-01-27 08:53:30,975 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2021-01-27 08:53:30,980 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2021-01-27 08:53:30,980 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2021-01-27 08:53:31,064 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2021-01-27 08:53:31,064 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 221 msecs
2021-01-27 08:53:31,306 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2021-01-27 08:53:31,310 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2021-01-27 08:53:31,329 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2021-01-27 08:53:31,367 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2021-01-27 08:53:31,373 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2021-01-27 08:53:31,373 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2021-01-27 08:53:31,373 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2021-01-27 08:53:31,374 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2021-01-27 08:53:31,374 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2021-01-27 08:53:31,374 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2021-01-27 08:53:31,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-27 08:53:31,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2021-01-27 08:53:31,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2021-01-27 08:53:31,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2021-01-27 08:53:31,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2021-01-27 08:53:31,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2021-01-27 08:53:31,388 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 14 msec
2021-01-27 08:53:31,414 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2021-01-27 08:53:31,415 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2021-01-27 08:53:31,416 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2021-01-27 08:53:31,416 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2021-01-27 08:53:31,420 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2021-01-27 08:53:36,285 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=578a59e8-6a62-4cc7-a963-6cc838dbf538, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-ae6077e4-015e-44b3-8712-a06ddf744854;nsid=1826582023;c=0) storage 578a59e8-6a62-4cc7-a963-6cc838dbf538
2021-01-27 08:53:36,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-27 08:53:36,287 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2021-01-27 08:53:36,360 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=ee7f03f5-730f-401b-9fda-1996325e676f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-ae6077e4-015e-44b3-8712-a06ddf744854;nsid=1826582023;c=0) storage ee7f03f5-730f-401b-9fda-1996325e676f
2021-01-27 08:53:36,360 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-27 08:53:36,360 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2021-01-27 08:53:36,379 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-27 08:53:36,379 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8f8bd674-b64f-4110-b61b-27eb02eea00a for DN 172.18.0.4:50010
2021-01-27 08:53:36,415 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2021-01-27 08:53:36,415 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-2eb468b7-4105-46da-803c-4121f2f4da49 for DN 172.18.0.3:50010
2021-01-27 08:53:36,416 INFO BlockStateChange: BLOCK* processReport: from storage DS-8f8bd674-b64f-4110-b61b-27eb02eea00a node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=578a59e8-6a62-4cc7-a963-6cc838dbf538, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-ae6077e4-015e-44b3-8712-a06ddf744854;nsid=1826582023;c=0), blocks: 0, hasStaleStorage: false, processing time: 3 msecs
2021-01-27 08:53:36,431 INFO BlockStateChange: BLOCK* processReport: from storage DS-2eb468b7-4105-46da-803c-4121f2f4da49 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=ee7f03f5-730f-401b-9fda-1996325e676f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-ae6077e4-015e-44b3-8712-a06ddf744854;nsid=1826582023;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2021-01-27 08:54:40,742 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2021-01-27 08:54:40,742 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2021-01-27 08:54:40,742 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2021-01-27 08:54:40,742 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 5 
2021-01-27 08:54:40,745 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 7 
2021-01-27 08:54:40,745 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2021-01-27 08:54:40,747 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2021-01-27 08:54:41,595 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2021-01-27 08:54:41,595 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2021-01-27 08:54:41,599 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2022-12-18 08:03:48,861 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2022-12-18 08:03:48,884 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2022-12-18 08:03:48,888 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2022-12-18 08:03:49,228 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2022-12-18 08:03:49,338 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2022-12-18 08:03:49,338 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2022-12-18 08:03:49,339 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2022-12-18 08:03:49,340 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2022-12-18 08:03:49,556 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2022-12-18 08:03:49,608 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2022-12-18 08:03:49,615 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2022-12-18 08:03:49,621 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2022-12-18 08:03:49,624 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2022-12-18 08:03:49,627 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2022-12-18 08:03:49,627 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2022-12-18 08:03:49,627 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2022-12-18 08:03:49,679 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2022-12-18 08:03:49,680 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2022-12-18 08:03:49,705 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2022-12-18 08:03:49,706 INFO org.mortbay.log: jetty-6.1.26
2022-12-18 08:03:49,872 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2022-12-18 08:03:49,906 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2022-12-18 08:03:49,906 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2022-12-18 08:03:49,950 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2022-12-18 08:03:49,950 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2022-12-18 08:03:50,004 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2022-12-18 08:03:50,004 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2022-12-18 08:03:50,005 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2022-12-18 08:03:50,006 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2022 Dec 18 08:03:50
2022-12-18 08:03:50,008 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2022-12-18 08:03:50,008 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2022-12-18 08:03:50,009 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2022-12-18 08:03:50,009 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2022-12-18 08:03:50,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2022-12-18 08:03:50,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2022-12-18 08:03:50,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2022-12-18 08:03:50,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2022-12-18 08:03:50,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2022-12-18 08:03:50,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2022-12-18 08:03:50,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2022-12-18 08:03:50,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2022-12-18 08:03:50,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2022-12-18 08:03:50,020 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2022-12-18 08:03:50,020 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2022-12-18 08:03:50,020 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2022-12-18 08:03:50,020 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2022-12-18 08:03:50,028 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2022-12-18 08:03:50,321 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2022-12-18 08:03:50,321 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2022-12-18 08:03:50,321 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2022-12-18 08:03:50,321 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2022-12-18 08:03:50,321 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2022-12-18 08:03:50,321 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2022-12-18 08:03:50,321 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2022-12-18 08:03:50,321 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2022-12-18 08:03:50,338 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2022-12-18 08:03:50,338 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2022-12-18 08:03:50,338 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2022-12-18 08:03:50,338 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2022-12-18 08:03:50,339 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2022-12-18 08:03:50,340 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2022-12-18 08:03:50,340 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2022-12-18 08:03:50,355 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2022-12-18 08:03:50,355 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2022-12-18 08:03:50,355 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2022-12-18 08:03:50,356 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2022-12-18 08:03:50,356 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2022-12-18 08:03:50,357 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2022-12-18 08:03:50,357 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2022-12-18 08:03:50,357 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2022-12-18 08:03:50,358 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2022-12-18 08:03:50,368 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 156@master
2022-12-18 08:03:50,436 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2022-12-18 08:03:50,437 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2022-12-18 08:03:50,466 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2022-12-18 08:03:50,481 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2022-12-18 08:03:50,481 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2022-12-18 08:03:50,486 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2022-12-18 08:03:50,486 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2022-12-18 08:03:50,575 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2022-12-18 08:03:50,575 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 216 msecs
2022-12-18 08:03:50,807 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2022-12-18 08:03:50,811 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2022-12-18 08:03:50,820 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2022-12-18 08:03:50,853 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2022-12-18 08:03:50,860 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2022-12-18 08:03:50,860 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2022-12-18 08:03:50,860 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2022-12-18 08:03:50,862 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2022-12-18 08:03:50,862 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2022-12-18 08:03:50,862 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2022-12-18 08:03:50,882 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 08:03:50,890 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2022-12-18 08:03:50,890 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2022-12-18 08:03:50,890 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2022-12-18 08:03:50,890 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2022-12-18 08:03:50,890 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2022-12-18 08:03:50,890 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 20 msec
2022-12-18 08:03:50,926 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2022-12-18 08:03:50,927 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2022-12-18 08:03:50,928 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2022-12-18 08:03:50,929 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2022-12-18 08:03:50,931 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2022-12-18 08:03:55,666 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=8cffac09-1eba-45bf-b915-43df748e02f4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-52e7924c-17d4-4d95-8d84-341aece6ea1a;nsid=986042096;c=0) storage 8cffac09-1eba-45bf-b915-43df748e02f4
2022-12-18 08:03:55,666 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 08:03:55,667 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2022-12-18 08:03:55,669 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=cfed8242-3ad1-42ca-9d23-40dc018f7953, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-52e7924c-17d4-4d95-8d84-341aece6ea1a;nsid=986042096;c=0) storage cfed8242-3ad1-42ca-9d23-40dc018f7953
2022-12-18 08:03:55,669 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 08:03:55,670 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2022-12-18 08:03:55,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 08:03:55,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313 for DN 172.18.0.4:50010
2022-12-18 08:03:55,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 08:03:55,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d for DN 172.18.0.3:50010
2022-12-18 08:03:55,760 INFO BlockStateChange: BLOCK* processReport: from storage DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=8cffac09-1eba-45bf-b915-43df748e02f4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-52e7924c-17d4-4d95-8d84-341aece6ea1a;nsid=986042096;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2022-12-18 08:03:55,763 INFO BlockStateChange: BLOCK* processReport: from storage DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=cfed8242-3ad1-42ca-9d23-40dc018f7953, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-52e7924c-17d4-4d95-8d84-341aece6ea1a;nsid=986042096;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2022-12-18 08:04:59,957 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2022-12-18 08:04:59,957 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2022-12-18 08:04:59,957 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2022-12-18 08:04:59,958 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 9 
2022-12-18 08:04:59,976 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 27 
2022-12-18 08:04:59,978 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2022-12-18 08:04:59,982 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2022-12-18 08:05:00,700 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2022-12-18 08:05:00,700 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2022-12-18 08:05:00,704 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2022-12-18 08:06:02,726 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 7 
2022-12-18 08:06:04,866 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:06:05,212 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:06:05,225 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2022-12-18 08:06:05,436 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:06:05,437 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741826_1002{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2022-12-18 08:06:05,527 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:06:05,528 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:06:05,591 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:06:05,594 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:06:05,676 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:06:05,678 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:06:05,742 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:06:05,744 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:06:05,813 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:06:05,814 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:06:05,887 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:06:05,898 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1420242843_1
2022-12-18 08:11:44,024 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 32 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 0 Number of syncs: 14 SyncTimes(ms): 41 
2022-12-18 08:14:13,834 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 37 Total time for transactions(ms): 16 Number of transactions batched in Syncs: 1 Number of syncs: 17 SyncTimes(ms): 52 
2022-12-18 08:14:33,930 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000001_0/part-00001
2022-12-18 08:14:34,061 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000000_0/part-00000
2022-12-18 08:14:34,098 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:14:34,099 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:14:34,102 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000001_0/part-00001 is closed by DFSClient_attempt_20221218081412_0000_m_000001_0_1091487769_26
2022-12-18 08:14:34,222 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:14:34,223 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:14:34,226 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000000_0/part-00000 is closed by DFSClient_attempt_20221218081412_0000_m_000000_0_1320954571_26
2022-12-18 08:14:34,262 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000002_0/part-00002
2022-12-18 08:14:34,314 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:14:34,316 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:14:34,319 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000002_0/part-00002 is closed by DFSClient_attempt_20221218081412_0000_m_000001_0_1091487769_26
2022-12-18 08:14:34,424 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000003_0/part-00003
2022-12-18 08:14:34,492 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000004_0/part-00004
2022-12-18 08:14:34,495 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:14:34,495 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:14:34,498 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000003_0/part-00003 is closed by DFSClient_attempt_20221218081412_0000_m_000000_0_1320954571_26
2022-12-18 08:14:34,585 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:14:34,587 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:14:34,589 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000004_0/part-00004 is closed by DFSClient_attempt_20221218081412_0000_m_000001_0_1091487769_26
2022-12-18 08:14:34,653 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000005_0/part-00005
2022-12-18 08:14:34,705 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:14:34,707 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:14:34,709 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000005_0/part-00005 is closed by DFSClient_attempt_20221218081412_0000_m_000000_0_1320954571_26
2022-12-18 08:14:34,727 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000006_0/part-00006
2022-12-18 08:14:34,791 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:14:34,792 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:14:34,796 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000006_0/part-00006 is closed by DFSClient_attempt_20221218081412_0000_m_000001_0_1091487769_26
2022-12-18 08:14:34,847 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000007_0/part-00007
2022-12-18 08:14:34,868 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:14:34,871 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c7e50c05-81ff-488d-b0cb-e01d9668db0d:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-5e501a2d-e149-4fb1-91c6-a1c3ff973313:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:14:34,873 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218081413_0008_m_000007_0/part-00007 is closed by DFSClient_attempt_20221218081412_0000_m_000000_0_1320954571_26
2022-12-18 08:14:34,992 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-139506455_16
2022-12-18 08:20:12,666 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2022-12-18 08:20:12,681 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2022-12-18 08:20:12,683 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2022-12-18 08:20:12,994 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2022-12-18 08:20:13,082 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2022-12-18 08:20:13,083 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2022-12-18 08:20:13,084 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2022-12-18 08:20:13,085 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2022-12-18 08:20:13,312 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2022-12-18 08:20:13,369 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2022-12-18 08:20:13,373 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2022-12-18 08:20:13,379 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2022-12-18 08:20:13,382 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2022-12-18 08:20:13,399 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2022-12-18 08:20:13,399 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2022-12-18 08:20:13,399 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2022-12-18 08:20:13,434 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2022-12-18 08:20:13,435 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2022-12-18 08:20:13,461 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2022-12-18 08:20:13,462 INFO org.mortbay.log: jetty-6.1.26
2022-12-18 08:20:13,606 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2022-12-18 08:20:13,643 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2022-12-18 08:20:13,643 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2022-12-18 08:20:13,676 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2022-12-18 08:20:13,677 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2022-12-18 08:20:13,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2022-12-18 08:20:13,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2022-12-18 08:20:13,721 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2022-12-18 08:20:13,722 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2022 Dec 18 08:20:13
2022-12-18 08:20:13,733 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2022-12-18 08:20:13,733 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2022-12-18 08:20:13,735 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2022-12-18 08:20:13,735 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2022-12-18 08:20:13,739 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2022-12-18 08:20:13,739 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2022-12-18 08:20:13,739 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2022-12-18 08:20:13,754 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2022-12-18 08:20:13,754 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2022-12-18 08:20:13,754 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2022-12-18 08:20:13,754 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2022-12-18 08:20:13,754 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2022-12-18 08:20:13,754 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2022-12-18 08:20:13,758 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2022-12-18 08:20:13,758 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2022-12-18 08:20:13,758 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2022-12-18 08:20:13,758 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2022-12-18 08:20:13,760 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2022-12-18 08:20:14,105 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2022-12-18 08:20:14,105 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2022-12-18 08:20:14,105 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2022-12-18 08:20:14,105 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2022-12-18 08:20:14,106 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2022-12-18 08:20:14,106 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2022-12-18 08:20:14,106 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2022-12-18 08:20:14,106 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2022-12-18 08:20:14,110 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2022-12-18 08:20:14,111 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2022-12-18 08:20:14,111 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2022-12-18 08:20:14,111 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2022-12-18 08:20:14,112 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2022-12-18 08:20:14,112 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2022-12-18 08:20:14,112 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2022-12-18 08:20:14,113 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2022-12-18 08:20:14,113 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2022-12-18 08:20:14,113 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2022-12-18 08:20:14,114 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2022-12-18 08:20:14,114 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2022-12-18 08:20:14,115 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2022-12-18 08:20:14,115 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2022-12-18 08:20:14,115 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2022-12-18 08:20:14,115 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2022-12-18 08:20:14,137 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 143@master
2022-12-18 08:20:14,194 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2022-12-18 08:20:14,195 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2022-12-18 08:20:14,216 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2022-12-18 08:20:14,241 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2022-12-18 08:20:14,241 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2022-12-18 08:20:14,246 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2022-12-18 08:20:14,246 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2022-12-18 08:20:14,336 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2022-12-18 08:20:14,336 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 219 msecs
2022-12-18 08:20:14,550 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2022-12-18 08:20:14,554 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2022-12-18 08:20:14,575 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2022-12-18 08:20:14,609 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2022-12-18 08:20:14,615 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2022-12-18 08:20:14,616 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2022-12-18 08:20:14,616 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2022-12-18 08:20:14,616 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2022-12-18 08:20:14,616 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2022-12-18 08:20:14,616 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2022-12-18 08:20:14,634 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 08:20:14,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2022-12-18 08:20:14,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2022-12-18 08:20:14,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2022-12-18 08:20:14,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2022-12-18 08:20:14,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2022-12-18 08:20:14,642 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 25 msec
2022-12-18 08:20:14,680 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2022-12-18 08:20:14,681 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2022-12-18 08:20:14,682 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2022-12-18 08:20:14,682 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2022-12-18 08:20:14,685 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2022-12-18 08:20:19,483 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=59480029-17b4-485b-acda-f3df4f249fa5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cc203620-c2ce-413b-96a0-d04a3dcbf0af;nsid=1512424922;c=0) storage 59480029-17b4-485b-acda-f3df4f249fa5
2022-12-18 08:20:19,485 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 08:20:19,485 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2022-12-18 08:20:19,557 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 08:20:19,557 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-387ee28d-ec17-4821-87cc-48946e9b8984 for DN 172.18.0.4:50010
2022-12-18 08:20:19,567 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=2b56237b-a108-4734-ae13-db686bf336d3, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cc203620-c2ce-413b-96a0-d04a3dcbf0af;nsid=1512424922;c=0) storage 2b56237b-a108-4734-ae13-db686bf336d3
2022-12-18 08:20:19,567 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 08:20:19,567 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2022-12-18 08:20:19,586 INFO BlockStateChange: BLOCK* processReport: from storage DS-387ee28d-ec17-4821-87cc-48946e9b8984 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=59480029-17b4-485b-acda-f3df4f249fa5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cc203620-c2ce-413b-96a0-d04a3dcbf0af;nsid=1512424922;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2022-12-18 08:20:19,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 08:20:19,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228 for DN 172.18.0.3:50010
2022-12-18 08:20:19,623 INFO BlockStateChange: BLOCK* processReport: from storage DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=2b56237b-a108-4734-ae13-db686bf336d3, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cc203620-c2ce-413b-96a0-d04a3dcbf0af;nsid=1512424922;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2022-12-18 08:21:11,557 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:21:12,005 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:21:12,011 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2022-12-18 08:21:12,276 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:21:12,284 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741826_1002{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2022-12-18 08:21:12,428 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:21:12,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:21:12,516 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:21:12,518 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:21:12,588 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:21:12,590 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:21:12,651 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:21:12,653 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:21:12,732 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:21:12,735 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 08:21:12,798 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:21:12,806 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_805203440_1
2022-12-18 08:21:19,772 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2022-12-18 08:21:19,776 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command rename is: 1
2022-12-18 08:21:19,776 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command mkdirs is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command listStatus is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command create is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command rename is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command mkdirs is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command listStatus is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command create is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command rename is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command mkdirs is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command listStatus is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command create is: 1
2022-12-18 08:21:19,777 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2022-12-18 08:21:23,834 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2022-12-18 08:21:23,834 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2022-12-18 08:21:23,834 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2022-12-18 08:21:23,834 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 30 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 0 Number of syncs: 14 SyncTimes(ms): 51 
2022-12-18 08:21:23,853 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 30 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 0 Number of syncs: 15 SyncTimes(ms): 69 
2022-12-18 08:21:23,853 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000030
2022-12-18 08:21:23,856 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 31
2022-12-18 08:21:24,634 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2022-12-18 08:21:24,634 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000030 size 620 bytes.
2022-12-18 08:21:24,639 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2022-12-18 08:22:01,202 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000000_0/part-00000
2022-12-18 08:22:01,225 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000001_0/part-00001
2022-12-18 08:22:01,372 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:22:01,382 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:22:01,383 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:22:01,384 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:22:01,388 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000001_0/part-00001 is closed by DFSClient_attempt_20221218082140_0000_m_000001_0_-1125157306_26
2022-12-18 08:22:01,397 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000000_0/part-00000 is closed by DFSClient_attempt_20221218082140_0000_m_000000_0_1281960914_26
2022-12-18 08:22:01,541 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000002_0/part-00002
2022-12-18 08:22:01,569 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000003_0/part-00003
2022-12-18 08:22:01,578 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:22:01,580 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:22:01,582 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000002_0/part-00002 is closed by DFSClient_attempt_20221218082140_0000_m_000001_0_-1125157306_26
2022-12-18 08:22:01,594 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:22:01,628 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:22:01,632 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000003_0/part-00003 is closed by DFSClient_attempt_20221218082140_0000_m_000000_0_1281960914_26
2022-12-18 08:22:01,733 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000004_0/part-00004
2022-12-18 08:22:01,766 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:22:01,767 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:22:01,775 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000004_0/part-00004 is closed by DFSClient_attempt_20221218082140_0000_m_000001_0_-1125157306_26
2022-12-18 08:22:01,785 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000005_0/part-00005
2022-12-18 08:22:01,874 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:22:01,876 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:22:01,880 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000005_0/part-00005 is closed by DFSClient_attempt_20221218082140_0000_m_000000_0_1281960914_26
2022-12-18 08:22:01,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000006_0/part-00006
2022-12-18 08:22:01,970 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:22:01,972 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 08:22:01,975 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000006_0/part-00006 is closed by DFSClient_attempt_20221218082140_0000_m_000001_0_-1125157306_26
2022-12-18 08:22:02,004 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000007_0/part-00007
2022-12-18 08:22:02,025 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:22:02,027 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b2a1d8ea-06d6-4e60-8b00-840d4532b228:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-387ee28d-ec17-4821-87cc-48946e9b8984:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 08:22:02,030 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218082141_0008_m_000007_0/part-00007 is closed by DFSClient_attempt_20221218082140_0000_m_000000_0_1281960914_26
2022-12-18 08:22:02,174 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_321412718_16
2022-12-18 09:46:31,988 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2022-12-18 09:46:31,993 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2022-12-18 09:46:31,996 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2022-12-18 09:46:32,351 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2022-12-18 09:46:32,444 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2022-12-18 09:46:32,444 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2022-12-18 09:46:32,446 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2022-12-18 09:46:32,446 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2022-12-18 09:46:32,671 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2022-12-18 09:46:32,722 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2022-12-18 09:46:32,741 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2022-12-18 09:46:32,746 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2022-12-18 09:46:32,749 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2022-12-18 09:46:32,750 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2022-12-18 09:46:32,750 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2022-12-18 09:46:32,750 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2022-12-18 09:46:32,797 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2022-12-18 09:46:32,798 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2022-12-18 09:46:32,806 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2022-12-18 09:46:32,806 INFO org.mortbay.log: jetty-6.1.26
2022-12-18 09:46:32,943 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2022-12-18 09:46:32,981 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2022-12-18 09:46:32,981 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2022-12-18 09:46:33,013 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2022-12-18 09:46:33,013 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2022-12-18 09:46:33,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2022-12-18 09:46:33,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2022-12-18 09:46:33,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2022-12-18 09:46:33,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2022 Dec 18 09:46:33
2022-12-18 09:46:33,082 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2022-12-18 09:46:33,082 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2022-12-18 09:46:33,083 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2022-12-18 09:46:33,083 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2022-12-18 09:46:33,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2022-12-18 09:46:33,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2022-12-18 09:46:33,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2022-12-18 09:46:33,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2022-12-18 09:46:33,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2022-12-18 09:46:33,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2022-12-18 09:46:33,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2022-12-18 09:46:33,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2022-12-18 09:46:33,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2022-12-18 09:46:33,090 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2022-12-18 09:46:33,090 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2022-12-18 09:46:33,090 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2022-12-18 09:46:33,091 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2022-12-18 09:46:33,091 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2022-12-18 09:46:33,389 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2022-12-18 09:46:33,389 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2022-12-18 09:46:33,395 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2022-12-18 09:46:33,395 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2022-12-18 09:46:33,395 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2022-12-18 09:46:33,395 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2022-12-18 09:46:33,395 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2022-12-18 09:46:33,395 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2022-12-18 09:46:33,400 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2022-12-18 09:46:33,400 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2022-12-18 09:46:33,400 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2022-12-18 09:46:33,400 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2022-12-18 09:46:33,401 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2022-12-18 09:46:33,402 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2022-12-18 09:46:33,402 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2022-12-18 09:46:33,415 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2022-12-18 09:46:33,415 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2022-12-18 09:46:33,416 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2022-12-18 09:46:33,417 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2022-12-18 09:46:33,417 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2022-12-18 09:46:33,418 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2022-12-18 09:46:33,419 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2022-12-18 09:46:33,420 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2022-12-18 09:46:33,420 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2022-12-18 09:46:33,430 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 148@master
2022-12-18 09:46:33,507 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2022-12-18 09:46:33,507 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2022-12-18 09:46:33,532 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2022-12-18 09:46:33,551 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2022-12-18 09:46:33,551 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2022-12-18 09:46:33,556 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2022-12-18 09:46:33,556 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2022-12-18 09:46:33,646 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2022-12-18 09:46:33,646 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 224 msecs
2022-12-18 09:46:33,854 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2022-12-18 09:46:33,857 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2022-12-18 09:46:33,894 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2022-12-18 09:46:33,928 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2022-12-18 09:46:33,934 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2022-12-18 09:46:33,934 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2022-12-18 09:46:33,934 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2022-12-18 09:46:33,944 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2022-12-18 09:46:33,944 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2022-12-18 09:46:33,944 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2022-12-18 09:46:33,957 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 09:46:33,988 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2022-12-18 09:46:33,988 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2022-12-18 09:46:33,988 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2022-12-18 09:46:33,988 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2022-12-18 09:46:33,988 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2022-12-18 09:46:33,988 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 43 msec
2022-12-18 09:46:34,001 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2022-12-18 09:46:34,001 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2022-12-18 09:46:34,003 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2022-12-18 09:46:34,003 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2022-12-18 09:46:34,006 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2022-12-18 09:46:38,642 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=73983929-9749-47a1-a77e-929f67e590ca, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-fd2d1757-d3ec-4c41-bc3b-9a90a68f1dcf;nsid=755013202;c=0) storage 73983929-9749-47a1-a77e-929f67e590ca
2022-12-18 09:46:38,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 09:46:38,643 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2022-12-18 09:46:38,651 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=bd2ec7a4-21e7-42ae-b248-030a497a66a5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-fd2d1757-d3ec-4c41-bc3b-9a90a68f1dcf;nsid=755013202;c=0) storage bd2ec7a4-21e7-42ae-b248-030a497a66a5
2022-12-18 09:46:38,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 09:46:38,651 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2022-12-18 09:46:38,702 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 09:46:38,702 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-7f1ff6ba-f227-4541-81cb-e47615aada9f for DN 172.18.0.3:50010
2022-12-18 09:46:38,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2022-12-18 09:46:38,709 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8 for DN 172.18.0.4:50010
2022-12-18 09:46:38,729 INFO BlockStateChange: BLOCK* processReport: from storage DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=73983929-9749-47a1-a77e-929f67e590ca, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-fd2d1757-d3ec-4c41-bc3b-9a90a68f1dcf;nsid=755013202;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2022-12-18 09:46:38,738 INFO BlockStateChange: BLOCK* processReport: from storage DS-7f1ff6ba-f227-4541-81cb-e47615aada9f node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=bd2ec7a4-21e7-42ae-b248-030a497a66a5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-fd2d1757-d3ec-4c41-bc3b-9a90a68f1dcf;nsid=755013202;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2022-12-18 09:47:42,997 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2022-12-18 09:47:42,997 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2022-12-18 09:47:42,997 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2022-12-18 09:47:42,997 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 9 
2022-12-18 09:47:43,009 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 21 
2022-12-18 09:47:43,010 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2022-12-18 09:47:43,012 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2022-12-18 09:47:43,927 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2022-12-18 09:47:43,928 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2022-12-18 09:47:43,963 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2022-12-18 09:49:28,666 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 54 
2022-12-18 09:49:30,871 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 09:49:31,217 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 09:49:31,230 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2022-12-18 09:49:31,452 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 09:49:31,453 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 09:49:31,563 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 09:49:31,564 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 09:49:31,640 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 09:49:31,642 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 09:49:31,706 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 09:49:31,709 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 09:49:31,785 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 09:49:31,786 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741830_1006{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2022-12-18 09:49:31,872 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 09:49:31,873 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2022-12-18 09:49:31,936 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 09:49:31,942 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1313163031_1
2022-12-18 09:50:37,450 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 35 Total time for transactions(ms): 16 Number of transactions batched in Syncs: 0 Number of syncs: 15 SyncTimes(ms): 127 
2022-12-18 09:50:37,553 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000000_0/part-00000
2022-12-18 09:50:37,581 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000001_0/part-00001
2022-12-18 09:50:37,716 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 09:50:37,716 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 09:50:37,722 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000000_0/part-00000 is closed by DFSClient_attempt_20221218095016_0000_m_000000_0_-559007600_26
2022-12-18 09:50:37,770 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 09:50:37,774 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 09:50:37,783 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000001_0/part-00001 is closed by DFSClient_attempt_20221218095016_0000_m_000001_0_230538026_26
2022-12-18 09:50:37,874 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000002_0/part-00002
2022-12-18 09:50:37,951 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 09:50:37,953 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 09:50:37,958 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000002_0/part-00002 is closed by DFSClient_attempt_20221218095016_0000_m_000000_0_-559007600_26
2022-12-18 09:50:37,976 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000003_0/part-00003
2022-12-18 09:50:38,069 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 09:50:38,072 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 09:50:38,075 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000003_0/part-00003 is closed by DFSClient_attempt_20221218095016_0000_m_000001_0_230538026_26
2022-12-18 09:50:38,122 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000004_0/part-00004
2022-12-18 09:50:38,194 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 09:50:38,196 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 09:50:38,198 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000004_0/part-00004 is closed by DFSClient_attempt_20221218095016_0000_m_000000_0_-559007600_26
2022-12-18 09:50:38,247 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000005_0/part-00005
2022-12-18 09:50:38,316 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 09:50:38,319 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 09:50:38,321 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000005_0/part-00005 is closed by DFSClient_attempt_20221218095016_0000_m_000001_0_230538026_26
2022-12-18 09:50:38,355 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000006_0/part-00006
2022-12-18 09:50:38,426 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 09:50:38,428 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW]]} size 0
2022-12-18 09:50:38,432 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000006_0/part-00006 is closed by DFSClient_attempt_20221218095016_0000_m_000000_0_-559007600_26
2022-12-18 09:50:38,489 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000007_0/part-00007
2022-12-18 09:50:38,512 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 09:50:38,514 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7f1ff6ba-f227-4541-81cb-e47615aada9f:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-9cfb2444-ddb2-4fc5-be61-b911530de8e8:NORMAL:172.18.0.4:50010|RBW]]} size 0
2022-12-18 09:50:38,517 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20221218095017_0008_m_000007_0/part-00007 is closed by DFSClient_attempt_20221218095016_0000_m_000001_0_230538026_26
2022-12-18 09:50:38,698 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_2145623293_16
2023-12-10 22:12:06,428 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2023-12-10 22:12:06,441 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-12-10 22:12:06,443 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2023-12-10 22:12:06,709 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-12-10 22:12:06,788 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2023-12-10 22:12:06,788 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2023-12-10 22:12:06,789 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2023-12-10 22:12:06,790 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2023-12-10 22:12:06,953 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2023-12-10 22:12:06,999 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-12-10 22:12:07,015 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-12-10 22:12:07,019 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2023-12-10 22:12:07,022 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-12-10 22:12:07,033 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2023-12-10 22:12:07,033 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-12-10 22:12:07,033 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-12-10 22:12:07,048 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2023-12-10 22:12:07,048 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2023-12-10 22:12:07,058 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2023-12-10 22:12:07,059 INFO org.mortbay.log: jetty-6.1.26
2023-12-10 22:12:07,165 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2023-12-10 22:12:07,195 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2023-12-10 22:12:07,195 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2023-12-10 22:12:07,219 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2023-12-10 22:12:07,219 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2023-12-10 22:12:07,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-12-10 22:12:07,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-12-10 22:12:07,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-12-10 22:12:07,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Dec 10 22:12:07
2023-12-10 22:12:07,281 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-12-10 22:12:07,281 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-12-10 22:12:07,282 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2023-12-10 22:12:07,282 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-12-10 22:12:07,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-12-10 22:12:07,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-12-10 22:12:07,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-12-10 22:12:07,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-12-10 22:12:07,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-12-10 22:12:07,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2023-12-10 22:12:07,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-12-10 22:12:07,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-12-10 22:12:07,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-12-10 22:12:07,288 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2023-12-10 22:12:07,288 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-12-10 22:12:07,288 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-12-10 22:12:07,301 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-12-10 22:12:07,303 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-12-10 22:12:07,538 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-12-10 22:12:07,538 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-12-10 22:12:07,538 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2023-12-10 22:12:07,538 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-12-10 22:12:07,539 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-12-10 22:12:07,539 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-12-10 22:12:07,539 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2023-12-10 22:12:07,539 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2023-12-10 22:12:07,551 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-12-10 22:12:07,551 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-12-10 22:12:07,551 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2023-12-10 22:12:07,551 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-12-10 22:12:07,552 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-12-10 22:12:07,552 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-12-10 22:12:07,552 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-12-10 22:12:07,554 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-12-10 22:12:07,555 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-12-10 22:12:07,556 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-12-10 22:12:07,556 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2023-12-10 22:12:07,556 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2023-12-10 22:12:07,557 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2023-12-10 22:12:07,557 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-12-10 22:12:07,557 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2023-12-10 22:12:07,557 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2023-12-10 22:12:07,568 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 144@master
2023-12-10 22:12:07,626 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2023-12-10 22:12:07,626 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2023-12-10 22:12:07,669 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2023-12-10 22:12:07,688 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2023-12-10 22:12:07,688 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2023-12-10 22:12:07,692 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2023-12-10 22:12:07,692 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2023-12-10 22:12:07,746 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2023-12-10 22:12:07,746 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 185 msecs
2023-12-10 22:12:07,936 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2023-12-10 22:12:07,940 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2023-12-10 22:12:07,946 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2023-12-10 22:12:07,971 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2023-12-10 22:12:07,978 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2023-12-10 22:12:07,978 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2023-12-10 22:12:07,978 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2023-12-10 22:12:07,979 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2023-12-10 22:12:07,979 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2023-12-10 22:12:07,979 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2023-12-10 22:12:07,987 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-10 22:12:07,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2023-12-10 22:12:07,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2023-12-10 22:12:07,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2023-12-10 22:12:07,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2023-12-10 22:12:07,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2023-12-10 22:12:07,989 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 10 msec
2023-12-10 22:12:08,009 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2023-12-10 22:12:08,010 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2023-12-10 22:12:08,011 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2023-12-10 22:12:08,011 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2023-12-10 22:12:08,015 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2023-12-10 22:12:14,290 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=056fafb3-97ac-4a52-a99e-7c26613ba8dc, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-dfb45879-218e-4e95-b6d1-4970483b32da;nsid=1807287671;c=0) storage 056fafb3-97ac-4a52-a99e-7c26613ba8dc
2023-12-10 22:12:14,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-10 22:12:14,291 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2023-12-10 22:12:14,324 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=480f1eab-05ad-45cd-8d97-8cc7aa8bb0b2, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-dfb45879-218e-4e95-b6d1-4970483b32da;nsid=1807287671;c=0) storage 480f1eab-05ad-45cd-8d97-8cc7aa8bb0b2
2023-12-10 22:12:14,324 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-10 22:12:14,324 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2023-12-10 22:12:14,382 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-10 22:12:14,382 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-90625483-fa96-46ce-869c-0cfaa55714f0 for DN 172.18.0.4:50010
2023-12-10 22:12:14,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-10 22:12:14,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-48e2783b-9293-49f5-8844-914ac63b02a7 for DN 172.18.0.3:50010
2023-12-10 22:12:14,422 INFO BlockStateChange: BLOCK* processReport: from storage DS-90625483-fa96-46ce-869c-0cfaa55714f0 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=056fafb3-97ac-4a52-a99e-7c26613ba8dc, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-dfb45879-218e-4e95-b6d1-4970483b32da;nsid=1807287671;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2023-12-10 22:12:14,428 INFO BlockStateChange: BLOCK* processReport: from storage DS-48e2783b-9293-49f5-8844-914ac63b02a7 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=480f1eab-05ad-45cd-8d97-8cc7aa8bb0b2, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-dfb45879-218e-4e95-b6d1-4970483b32da;nsid=1807287671;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2023-12-10 22:23:29,873 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 3 
2023-12-10 22:23:32,009 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:32,421 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:32,434 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2023-12-10 22:23:32,704 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:32,713 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741826_1002{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2023-12-10 22:23:32,844 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:32,844 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741827_1003{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2023-12-10 22:23:32,935 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:23:32,936 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:33,055 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:33,056 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741829_1005{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2023-12-10 22:23:33,155 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:33,156 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741830_1006{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2023-12-10 22:23:33,279 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:33,280 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741831_1007{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2023-12-10 22:23:33,404 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:33,404 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741832_1008{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2023-12-10 22:23:33,505 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:33,506 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741833_1009{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2023-12-10 22:23:33,612 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:33,613 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741834_1010{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2023-12-10 22:23:33,707 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:23:33,711 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:33,815 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:33,816 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741836_1012{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2023-12-10 22:23:33,931 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:33,932 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741837_1013{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2023-12-10 22:23:34,037 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:34,038 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741838_1014{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2023-12-10 22:23:34,154 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:34,155 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741839_1015{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2023-12-10 22:23:34,231 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:34,232 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741840_1016{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2023-12-10 22:23:34,304 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:34,304 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741841_1017{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2023-12-10 22:23:34,396 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:34,398 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741842_1018{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2023-12-10 22:23:34,499 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:23:34,507 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:34,617 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:23:34,626 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:34,720 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:23:34,725 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:34,791 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:23:34,796 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:34,863 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:23:34,870 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:34,924 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:23:34,932 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:35,002 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:23:35,009 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:35,083 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:23:35,090 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:35,172 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:35,173 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741851_1027{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2023-12-10 22:23:35,221 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:35,221 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741852_1028{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2023-12-10 22:23:35,266 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:35,266 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741853_1029{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2023-12-10 22:23:35,343 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:35,343 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741854_1030{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2023-12-10 22:23:35,401 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-10 22:23:35,401 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741855_1031{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2023-12-10 22:23:35,447 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:23:35,452 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1234285672_1
2023-12-10 22:25:01,018 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 107 Total time for transactions(ms): 16 Number of transactions batched in Syncs: 0 Number of syncs: 39 SyncTimes(ms): 100 
2023-12-10 22:25:01,160 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000000_0/part-00000
2023-12-10 22:25:01,184 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000001_0/part-00001
2023-12-10 22:25:01,384 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:01,384 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:01,389 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000000_0/part-00000 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:01,401 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:01,406 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:01,407 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000001_0/part-00001 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:01,619 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000002_0/part-00002
2023-12-10 22:25:01,625 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000003_0/part-00003
2023-12-10 22:25:01,689 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:01,691 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:01,694 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:01,694 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000002_0/part-00002 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:01,696 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:01,699 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000003_0/part-00003 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:01,870 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000005_0/part-00005
2023-12-10 22:25:01,872 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000004_0/part-00004
2023-12-10 22:25:01,912 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:01,914 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:01,916 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000005_0/part-00005 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:01,920 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:01,922 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:01,926 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000004_0/part-00004 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:02,120 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000006_0/part-00006
2023-12-10 22:25:02,126 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000007_0/part-00007
2023-12-10 22:25:02,204 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:02,206 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:02,206 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:02,209 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000006_0/part-00006 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:02,209 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000007_0/part-00007 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:02,213 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741864_1040 size 78
2023-12-10 22:25:02,391 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000008_0/part-00008
2023-12-10 22:25:02,393 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000009_0/part-00009
2023-12-10 22:25:02,453 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:02,455 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:02,461 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:02,461 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:02,461 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000009_0/part-00009 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:02,463 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000008_0/part-00008 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:02,615 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000011_0/part-00011
2023-12-10 22:25:02,653 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000010_0/part-00010
2023-12-10 22:25:02,663 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:02,665 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:02,667 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000011_0/part-00011 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:02,740 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:02,742 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:02,744 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000010_0/part-00010 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:02,832 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000012_0/part-00012
2023-12-10 22:25:02,875 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:02,875 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:02,877 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000012_0/part-00012 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:02,888 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000013_0/part-00013
2023-12-10 22:25:02,951 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:02,957 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:02,958 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000013_0/part-00013 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:03,061 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000014_0/part-00014
2023-12-10 22:25:03,137 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:03,138 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:03,140 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000014_0/part-00014 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:03,161 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000015_0/part-00015
2023-12-10 22:25:03,270 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:03,273 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:03,274 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000015_0/part-00015 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:03,347 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000016_0/part-00016
2023-12-10 22:25:03,431 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:03,432 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:03,435 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000016_0/part-00016 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:03,478 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000017_0/part-00017
2023-12-10 22:25:03,530 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:03,532 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:03,534 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000017_0/part-00017 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:03,618 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000018_0/part-00018
2023-12-10 22:25:03,676 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:03,677 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:03,680 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000018_0/part-00018 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:03,712 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000019_0/part-00019
2023-12-10 22:25:03,813 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:03,815 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:03,817 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000019_0/part-00019 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:03,886 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000020_0/part-00020
2023-12-10 22:25:03,956 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:03,957 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:03,960 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000020_0/part-00020 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:04,008 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000021_0/part-00021
2023-12-10 22:25:04,080 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:04,082 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:04,088 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000021_0/part-00021 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:04,100 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000022_0/part-00022
2023-12-10 22:25:04,172 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:04,173 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:04,177 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000022_0/part-00022 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:04,301 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000023_0/part-00023
2023-12-10 22:25:04,302 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000024_0/part-00024
2023-12-10 22:25:04,376 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:04,376 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:04,378 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741880_1056 size 53
2023-12-10 22:25:04,379 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000023_0/part-00023 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:04,386 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:04,388 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000024_0/part-00024 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:04,567 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000025_0/part-00025
2023-12-10 22:25:04,584 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000026_0/part-00026
2023-12-10 22:25:04,656 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:04,657 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:04,659 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000025_0/part-00025 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:04,662 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:04,665 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:04,671 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000026_0/part-00026 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:04,883 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000027_0/part-00027
2023-12-10 22:25:04,903 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000029_0/part-00029
2023-12-10 22:25:04,952 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:04,953 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:04,955 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:04,955 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000027_0/part-00027 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:04,955 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:04,957 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000029_0/part-00029 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:05,128 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000031_0/part-00031
2023-12-10 22:25:05,131 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000030_0/part-00030
2023-12-10 22:25:05,165 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:05,166 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW]]} size 0
2023-12-10 22:25:05,174 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000030_0/part-00030 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:05,178 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:05,187 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-48e2783b-9293-49f5-8844-914ac63b02a7:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-90625483-fa96-46ce-869c-0cfaa55714f0:NORMAL:172.18.0.4:50010|RBW]]} size 0
2023-12-10 22:25:05,195 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000031_0/part-00031 is closed by DFSClient_attempt_20231210222354_0000_m_000001_0_-281111758_26
2023-12-10 22:25:05,328 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231210222355_0008_m_000028_0/part-00028 is closed by DFSClient_attempt_20231210222354_0000_m_000000_0_1245083975_26
2023-12-10 22:25:05,533 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_449325479_16
2023-12-10 22:25:40,214 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2023-12-10 22:25:40,215 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at master/172.18.0.2
************************************************************/
2023-12-16 08:09:52,473 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.19.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2023-12-16 08:09:52,522 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-12-16 08:09:52,552 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2023-12-16 08:09:54,037 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-12-16 08:09:54,419 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2023-12-16 08:09:54,419 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2023-12-16 08:09:54,425 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2023-12-16 08:09:54,427 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2023-12-16 08:09:55,442 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2023-12-16 08:09:55,496 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-12-16 08:09:55,500 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-12-16 08:09:55,503 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2023-12-16 08:09:55,506 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-12-16 08:09:55,507 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2023-12-16 08:09:55,507 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-12-16 08:09:55,507 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-12-16 08:09:55,522 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2023-12-16 08:09:55,522 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2023-12-16 08:09:55,541 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2023-12-16 08:09:55,541 INFO org.mortbay.log: jetty-6.1.26
2023-12-16 08:09:55,683 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2023-12-16 08:09:55,715 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2023-12-16 08:09:55,715 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2023-12-16 08:09:55,746 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2023-12-16 08:09:55,746 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2023-12-16 08:09:55,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-12-16 08:09:55,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-12-16 08:09:55,795 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-12-16 08:09:55,795 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Dec 16 08:09:55
2023-12-16 08:09:55,811 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-12-16 08:09:55,811 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-12-16 08:09:55,812 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2023-12-16 08:09:55,812 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-12-16 08:09:55,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-12-16 08:09:55,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-12-16 08:09:55,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-12-16 08:09:55,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-12-16 08:09:55,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-12-16 08:09:55,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2023-12-16 08:09:55,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-12-16 08:09:55,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-12-16 08:09:55,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-12-16 08:09:55,837 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2023-12-16 08:09:55,837 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-12-16 08:09:55,837 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-12-16 08:09:55,837 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-12-16 08:09:55,838 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-12-16 08:09:56,101 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-12-16 08:09:56,101 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-12-16 08:09:56,101 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2023-12-16 08:09:56,101 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-12-16 08:09:56,102 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-12-16 08:09:56,102 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-12-16 08:09:56,102 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2023-12-16 08:09:56,102 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2023-12-16 08:09:56,111 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-12-16 08:09:56,111 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-12-16 08:09:56,111 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2023-12-16 08:09:56,111 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-12-16 08:09:56,112 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-12-16 08:09:56,112 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-12-16 08:09:56,112 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-12-16 08:09:56,114 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-12-16 08:09:56,115 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-12-16 08:09:56,115 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-12-16 08:09:56,116 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2023-12-16 08:09:56,116 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2023-12-16 08:09:56,117 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2023-12-16 08:09:56,117 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-12-16 08:09:56,117 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2023-12-16 08:09:56,117 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2023-12-16 08:09:56,149 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 185@master
2023-12-16 08:09:56,192 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2023-12-16 08:09:56,192 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2023-12-16 08:09:56,222 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2023-12-16 08:09:56,241 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2023-12-16 08:09:56,242 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2023-12-16 08:09:56,246 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2023-12-16 08:09:56,246 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2023-12-16 08:09:56,338 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2023-12-16 08:09:56,338 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 202 msecs
2023-12-16 08:09:56,556 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2023-12-16 08:09:56,559 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2023-12-16 08:09:56,565 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2023-12-16 08:09:56,600 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2023-12-16 08:09:56,616 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2023-12-16 08:09:56,616 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2023-12-16 08:09:56,616 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2023-12-16 08:09:56,617 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2023-12-16 08:09:56,617 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2023-12-16 08:09:56,617 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2023-12-16 08:09:56,625 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-16 08:09:56,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2023-12-16 08:09:56,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2023-12-16 08:09:56,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2023-12-16 08:09:56,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2023-12-16 08:09:56,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2023-12-16 08:09:56,631 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 14 msec
2023-12-16 08:09:56,643 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.19.0.2:54310
2023-12-16 08:09:56,641 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2023-12-16 08:09:56,642 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2023-12-16 08:09:56,651 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2023-12-16 08:09:56,663 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2023-12-16 08:09:59,476 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.19.0.3:50010, datanodeUuid=f3cff374-8623-4a46-98c7-bc5d12e7c9c3, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-2bbcc24e-5268-42cb-b63f-a0eb25f33987;nsid=1110687171;c=0) storage f3cff374-8623-4a46-98c7-bc5d12e7c9c3
2023-12-16 08:09:59,483 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-16 08:09:59,485 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.19.0.3:50010
2023-12-16 08:09:59,564 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.19.0.4:50010, datanodeUuid=2ee9eb69-157f-4706-8ce3-1936700eec6f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-2bbcc24e-5268-42cb-b63f-a0eb25f33987;nsid=1110687171;c=0) storage 2ee9eb69-157f-4706-8ce3-1936700eec6f
2023-12-16 08:09:59,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-16 08:09:59,565 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.19.0.4:50010
2023-12-16 08:09:59,873 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-16 08:09:59,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-57010e22-184f-408c-b309-4eee3be6bffc for DN 172.19.0.4:50010
2023-12-16 08:09:59,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-16 08:09:59,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753 for DN 172.19.0.3:50010
2023-12-16 08:10:00,008 INFO BlockStateChange: BLOCK* processReport: from storage DS-57010e22-184f-408c-b309-4eee3be6bffc node DatanodeRegistration(172.19.0.4:50010, datanodeUuid=2ee9eb69-157f-4706-8ce3-1936700eec6f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-2bbcc24e-5268-42cb-b63f-a0eb25f33987;nsid=1110687171;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2023-12-16 08:10:00,019 INFO BlockStateChange: BLOCK* processReport: from storage DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753 node DatanodeRegistration(172.19.0.3:50010, datanodeUuid=f3cff374-8623-4a46-98c7-bc5d12e7c9c3, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-2bbcc24e-5268-42cb-b63f-a0eb25f33987;nsid=1110687171;c=0), blocks: 0, hasStaleStorage: false, processing time: 17 msecs
2023-12-16 08:11:08,994 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 15 
2023-12-16 08:11:10,825 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-16 08:11:11,097 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-16 08:11:11,103 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW]]} size 16777216
2023-12-16 08:11:11,187 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-16 08:11:11,187 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-16 08:11:11,373 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-16 08:11:11,374 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-16 08:11:11,446 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-16 08:11:11,450 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_853522575_1
2023-12-16 08:11:34,804 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command rename is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command mkdirs is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command listStatus is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command create is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command rename is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command mkdirs is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command listStatus is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command create is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command rename is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command mkdirs is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command listStatus is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command create is: 1
2023-12-16 08:11:34,805 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2023-12-16 08:12:10,558 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 20 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 0 Number of syncs: 10 SyncTimes(ms): 49 
2023-12-16 08:12:36,330 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231216081210_0008_m_000000_0/part-00000
2023-12-16 08:12:36,367 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231216081210_0008_m_000001_0/part-00001
2023-12-16 08:12:36,525 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-16 08:12:36,526 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-16 08:12:36,530 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231216081210_0008_m_000000_0/part-00000 is closed by DFSClient_attempt_20231216081206_0000_m_000001_0_497024143_26
2023-12-16 08:12:36,577 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-16 08:12:36,580 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-16 08:12:36,586 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231216081210_0008_m_000001_0/part-00001 is closed by DFSClient_attempt_20231216081206_0000_m_000000_0_1389196456_26
2023-12-16 08:12:36,733 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231216081210_0008_m_000002_0/part-00002
2023-12-16 08:12:36,780 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231216081210_0008_m_000003_0/part-00003
2023-12-16 08:12:36,781 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-16 08:12:36,782 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-16 08:12:36,784 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231216081210_0008_m_000002_0/part-00002 is closed by DFSClient_attempt_20231216081206_0000_m_000001_0_497024143_26
2023-12-16 08:12:36,820 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-16 08:12:36,822 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-57010e22-184f-408c-b309-4eee3be6bffc:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-cd3fa1ef-eeaa-4e5b-b4ac-fe55d26e2753:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-16 08:12:36,825 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231216081210_0008_m_000003_0/part-00003 is closed by DFSClient_attempt_20231216081206_0000_m_000000_0_1389196456_26
2023-12-16 08:12:36,978 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1167685407_16
2023-12-16 08:20:04,908 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.19.0.2
2023-12-16 08:20:04,909 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2023-12-16 08:20:04,909 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2023-12-16 08:20:04,909 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 57 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 1 Number of syncs: 34 SyncTimes(ms): 78 
2023-12-16 08:20:04,917 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 57 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 1 Number of syncs: 35 SyncTimes(ms): 87 
2023-12-16 08:20:04,918 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000057
2023-12-16 08:20:04,921 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 58
2023-12-16 08:20:05,677 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-12-16 08:20:05,677 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000057 size 953 bytes.
2023-12-16 08:20:05,680 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2023-12-16 08:25:09,384 INFO BlockStateChange: BLOCK* processReport: from storage DS-57010e22-184f-408c-b309-4eee3be6bffc node DatanodeRegistration(172.19.0.4:50010, datanodeUuid=2ee9eb69-157f-4706-8ce3-1936700eec6f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-2bbcc24e-5268-42cb-b63f-a0eb25f33987;nsid=1110687171;c=0), blocks: 5, hasStaleStorage: false, processing time: 2 msecs
2023-12-16 08:49:13,620 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1649ms
No GCs detected
2023-12-19 14:14:28,852 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.19.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2023-12-19 14:14:28,866 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-12-19 14:14:28,869 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2023-12-19 14:14:29,193 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-12-19 14:14:29,285 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2023-12-19 14:14:29,285 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2023-12-19 14:14:29,287 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2023-12-19 14:14:29,288 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2023-12-19 14:14:29,490 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2023-12-19 14:14:29,549 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-12-19 14:14:29,555 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-12-19 14:14:29,559 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2023-12-19 14:14:29,562 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-12-19 14:14:29,577 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2023-12-19 14:14:29,577 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-12-19 14:14:29,577 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-12-19 14:14:29,594 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2023-12-19 14:14:29,595 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2023-12-19 14:14:29,618 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2023-12-19 14:14:29,619 INFO org.mortbay.log: jetty-6.1.26
2023-12-19 14:14:29,787 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2023-12-19 14:14:29,826 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2023-12-19 14:14:29,826 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2023-12-19 14:14:29,869 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2023-12-19 14:14:29,869 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2023-12-19 14:14:29,933 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-12-19 14:14:29,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-12-19 14:14:29,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-12-19 14:14:29,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Dec 19 14:14:29
2023-12-19 14:14:29,940 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-12-19 14:14:29,940 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-12-19 14:14:29,941 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2023-12-19 14:14:29,941 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-12-19 14:14:29,956 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-12-19 14:14:29,956 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-12-19 14:14:29,956 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-12-19 14:14:29,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-12-19 14:14:29,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-12-19 14:14:29,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2023-12-19 14:14:29,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-12-19 14:14:29,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-12-19 14:14:29,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-12-19 14:14:29,966 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2023-12-19 14:14:29,966 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-12-19 14:14:29,966 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-12-19 14:14:29,966 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-12-19 14:14:29,967 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-12-19 14:14:30,277 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-12-19 14:14:30,277 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-12-19 14:14:30,277 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2023-12-19 14:14:30,277 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-12-19 14:14:30,278 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-12-19 14:14:30,278 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-12-19 14:14:30,278 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2023-12-19 14:14:30,278 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2023-12-19 14:14:30,282 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-12-19 14:14:30,282 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-12-19 14:14:30,282 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2023-12-19 14:14:30,282 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-12-19 14:14:30,283 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-12-19 14:14:30,283 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-12-19 14:14:30,283 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-12-19 14:14:30,296 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-12-19 14:14:30,297 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-12-19 14:14:30,298 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-12-19 14:14:30,298 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2023-12-19 14:14:30,298 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2023-12-19 14:14:30,300 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2023-12-19 14:14:30,300 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-12-19 14:14:30,300 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2023-12-19 14:14:30,300 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2023-12-19 14:14:30,327 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 333@master
2023-12-19 14:14:30,378 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2023-12-19 14:14:30,379 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2023-12-19 14:14:30,407 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2023-12-19 14:14:30,426 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2023-12-19 14:14:30,426 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2023-12-19 14:14:30,430 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2023-12-19 14:14:30,430 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2023-12-19 14:14:30,519 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2023-12-19 14:14:30,520 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 210 msecs
2023-12-19 14:14:30,713 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2023-12-19 14:14:30,717 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2023-12-19 14:14:30,724 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2023-12-19 14:14:30,752 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2023-12-19 14:14:30,760 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2023-12-19 14:14:30,760 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2023-12-19 14:14:30,760 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2023-12-19 14:14:30,761 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2023-12-19 14:14:30,761 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2023-12-19 14:14:30,761 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2023-12-19 14:14:30,770 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-19 14:14:30,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2023-12-19 14:14:30,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2023-12-19 14:14:30,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2023-12-19 14:14:30,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2023-12-19 14:14:30,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2023-12-19 14:14:30,774 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 13 msec
2023-12-19 14:14:30,796 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2023-12-19 14:14:30,797 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2023-12-19 14:14:30,799 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.19.0.2:54310
2023-12-19 14:14:30,799 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2023-12-19 14:14:30,801 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2023-12-19 14:14:35,317 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.19.0.4:50010, datanodeUuid=1bd9466b-cf79-45f2-a070-d12eff280490, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0970c588-93a0-43a8-ba25-6e35d5178fa9;nsid=470937626;c=0) storage 1bd9466b-cf79-45f2-a070-d12eff280490
2023-12-19 14:14:35,317 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-19 14:14:35,317 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.19.0.4:50010
2023-12-19 14:14:35,325 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.19.0.3:50010, datanodeUuid=307c3e9e-8c08-4691-a747-c068b4803654, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0970c588-93a0-43a8-ba25-6e35d5178fa9;nsid=470937626;c=0) storage 307c3e9e-8c08-4691-a747-c068b4803654
2023-12-19 14:14:35,325 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-19 14:14:35,326 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.19.0.3:50010
2023-12-19 14:14:35,376 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-19 14:14:35,376 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-857828ab-f0dc-41cd-bfaf-530b360b2742 for DN 172.19.0.4:50010
2023-12-19 14:14:35,381 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2023-12-19 14:14:35,381 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932 for DN 172.19.0.3:50010
2023-12-19 14:14:35,399 INFO BlockStateChange: BLOCK* processReport: from storage DS-857828ab-f0dc-41cd-bfaf-530b360b2742 node DatanodeRegistration(172.19.0.4:50010, datanodeUuid=1bd9466b-cf79-45f2-a070-d12eff280490, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0970c588-93a0-43a8-ba25-6e35d5178fa9;nsid=470937626;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2023-12-19 14:14:35,406 INFO BlockStateChange: BLOCK* processReport: from storage DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932 node DatanodeRegistration(172.19.0.3:50010, datanodeUuid=307c3e9e-8c08-4691-a747-c068b4803654, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0970c588-93a0-43a8-ba25-6e35d5178fa9;nsid=470937626;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2023-12-19 14:15:41,400 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.19.0.2
2023-12-19 14:15:41,400 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2023-12-19 14:15:41,400 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2023-12-19 14:15:41,400 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 4 
2023-12-19 14:15:41,404 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 8 
2023-12-19 14:15:41,405 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2023-12-19 14:15:41,406 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2023-12-19 14:15:42,028 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-12-19 14:15:42,028 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2023-12-19 14:15:42,032 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2023-12-19 14:16:39,035 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:39,340 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:39,341 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 16777216
2023-12-19 14:16:39,559 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:39,560 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:39,668 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:16:39,668 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:39,736 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:16:39,737 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:39,801 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:39,802 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741829_1005{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 16777216
2023-12-19 14:16:39,903 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:39,904 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:39,974 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:39,975 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:40,040 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:40,043 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:40,094 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:40,096 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:40,161 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:40,161 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741834_1010{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 16777216
2023-12-19 14:16:40,255 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:40,255 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741835_1011{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 16777216
2023-12-19 14:16:40,343 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:40,345 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:40,415 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:16:40,416 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:40,480 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:40,482 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:40,560 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:16:40,561 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:40,635 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:40,636 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741840_1016{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 16777216
2023-12-19 14:16:40,690 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:40,695 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:40,789 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:40,790 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741842_1018{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 16777216
2023-12-19 14:16:40,867 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:16:40,869 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:40,953 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:40,955 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:41,025 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:16:41,025 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:41,088 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:41,089 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:41,153 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:41,154 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:41,208 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:16:41,212 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:41,264 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:41,265 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:41,337 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:16:41,339 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:41,414 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:41,414 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 87 Total time for transactions(ms): 11 Number of transactions batched in Syncs: 0 Number of syncs: 31 SyncTimes(ms): 61 
2023-12-19 14:16:41,414 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741851_1027{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 16777216
2023-12-19 14:16:41,457 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:16:41,459 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:41,501 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:16:41,502 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:41,546 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:41,557 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:41,623 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:41,631 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2023-12-19 14:16:41,670 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:16:41,675 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1723221237_1
2023-12-19 14:17:49,900 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 107 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 0 Number of syncs: 39 SyncTimes(ms): 70 
2023-12-19 14:17:50,016 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000000_0/part-00000
2023-12-19 14:17:50,041 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000001_0/part-00001
2023-12-19 14:17:50,165 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:50,169 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:50,175 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000001_0/part-00001 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:50,181 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:50,181 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:50,190 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000000_0/part-00000 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:50,323 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000002_0/part-00002
2023-12-19 14:17:50,333 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000003_0/part-00003
2023-12-19 14:17:50,400 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:50,403 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:50,404 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:50,404 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000002_0/part-00002 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:50,405 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:50,406 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000003_0/part-00003 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:50,539 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000005_0/part-00005
2023-12-19 14:17:50,544 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000004_0/part-00004
2023-12-19 14:17:50,584 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:50,585 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:50,587 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000005_0/part-00005 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:50,588 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:50,591 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:50,595 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000004_0/part-00004 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:50,728 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000006_0/part-00006
2023-12-19 14:17:50,731 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000007_0/part-00007
2023-12-19 14:17:50,769 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:50,769 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:50,771 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:50,772 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:50,772 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000006_0/part-00006 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:50,773 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000007_0/part-00007 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:50,881 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000008_0/part-00008
2023-12-19 14:17:50,921 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:50,925 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:50,929 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000008_0/part-00008 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:50,937 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000009_0/part-00009
2023-12-19 14:17:50,978 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:50,979 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:50,980 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000009_0/part-00009 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:51,045 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000010_0/part-00010
2023-12-19 14:17:51,086 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:51,089 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:51,090 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000010_0/part-00010 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:51,115 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000011_0/part-00011
2023-12-19 14:17:51,180 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:51,181 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:51,183 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000011_0/part-00011 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:51,185 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000012_0/part-00012
2023-12-19 14:17:51,236 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:51,237 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:51,238 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000012_0/part-00012 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:51,301 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000013_0/part-00013
2023-12-19 14:17:51,335 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:51,336 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:51,337 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000013_0/part-00013 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:51,344 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000014_0/part-00014
2023-12-19 14:17:51,427 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:51,428 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:51,429 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000014_0/part-00014 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:51,453 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000015_0/part-00015
2023-12-19 14:17:51,507 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:51,508 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:51,511 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000015_0/part-00015 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:51,538 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000016_0/part-00016
2023-12-19 14:17:51,608 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:51,609 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:51,611 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000016_0/part-00016 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:51,632 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000017_0/part-00017
2023-12-19 14:17:51,693 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:51,699 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741874_1050 size 130
2023-12-19 14:17:51,699 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000017_0/part-00017 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:51,717 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000018_0/part-00018
2023-12-19 14:17:51,794 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:51,795 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:51,797 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000018_0/part-00018 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:51,831 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000019_0/part-00019
2023-12-19 14:17:51,905 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:51,906 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:51,915 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000019_0/part-00019 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:51,941 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000020_0/part-00020
2023-12-19 14:17:52,007 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:52,008 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:52,014 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000020_0/part-00020 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:52,026 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000021_0/part-00021
2023-12-19 14:17:52,106 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:52,108 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:52,111 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000021_0/part-00021 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:52,153 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000022_0/part-00022
2023-12-19 14:17:52,224 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:52,225 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:52,227 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000022_0/part-00022 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:52,236 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000023_0/part-00023
2023-12-19 14:17:52,344 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:52,345 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:52,347 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000023_0/part-00023 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:52,409 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000024_0/part-00024
2023-12-19 14:17:52,441 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000025_0/part-00025
2023-12-19 14:17:52,448 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:52,448 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:52,450 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000024_0/part-00024 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:52,530 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:52,530 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:52,532 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000025_0/part-00025 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:52,604 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000026_0/part-00026
2023-12-19 14:17:52,641 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:52,645 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:52,646 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000026_0/part-00026 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:52,671 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000027_0/part-00027
2023-12-19 14:17:52,748 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:52,749 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:52,750 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000027_0/part-00027 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:52,812 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000029_0/part-00029
2023-12-19 14:17:52,847 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:52,848 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:52,851 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000029_0/part-00029 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:52,872 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000030_0/part-00030
2023-12-19 14:17:52,964 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:52,965 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW]]} size 0
2023-12-19 14:17:52,967 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000030_0/part-00030 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:52,979 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000031_0/part-00031
2023-12-19 14:17:53,056 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.4:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:53,056 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.19.0.3:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c217d529-0e7e-46a4-8f5e-3f10fefe3932:NORMAL:172.19.0.3:50010|RBW], ReplicaUC[[DISK]DS-857828ab-f0dc-41cd-bfaf-530b360b2742:NORMAL:172.19.0.4:50010|RBW]]} size 0
2023-12-19 14:17:53,058 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000031_0/part-00031 is closed by DFSClient_attempt_20231219141657_0000_m_000001_0_175125571_26
2023-12-19 14:17:53,104 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20231219141658_0008_m_000028_0/part-00028 is closed by DFSClient_attempt_20231219141657_0000_m_000000_0_-904886821_26
2023-12-19 14:17:53,336 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_649370939_16
2023-12-19 14:19:14,142 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2023-12-19 14:19:14,143 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at master/172.19.0.2
************************************************************/
2024-01-09 14:41:55,491 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-09 14:41:55,524 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-09 14:41:55,531 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-09 14:41:56,090 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-09 14:41:56,317 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-09 14:41:56,317 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-09 14:41:56,320 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-09 14:41:56,321 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-09 14:41:56,636 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-09 14:41:56,823 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-09 14:41:56,829 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-09 14:41:56,864 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-09 14:41:56,868 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-09 14:41:56,881 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-09 14:41:56,881 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-09 14:41:56,881 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-09 14:41:56,937 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-09 14:41:56,938 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-09 14:41:56,990 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-09 14:41:56,991 INFO org.mortbay.log: jetty-6.1.26
2024-01-09 14:41:57,396 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-09 14:41:57,442 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-09 14:41:57,442 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-09 14:41:57,488 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-09 14:41:57,488 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-09 14:41:57,547 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-09 14:41:57,547 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-09 14:41:57,550 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-09 14:41:57,553 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 09 14:41:57
2024-01-09 14:41:57,557 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-09 14:41:57,557 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-09 14:41:57,558 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-09 14:41:57,558 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-09 14:41:57,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-09 14:41:57,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-09 14:41:57,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-09 14:41:57,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-09 14:41:57,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-09 14:41:57,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-09 14:41:57,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-09 14:41:57,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-09 14:41:57,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-09 14:41:57,578 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-09 14:41:57,578 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-09 14:41:57,578 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-09 14:41:57,580 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-09 14:41:57,582 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-09 14:41:57,946 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-09 14:41:57,946 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-09 14:41:57,946 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-09 14:41:57,946 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-09 14:41:57,966 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-09 14:41:57,966 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-09 14:41:57,966 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-09 14:41:57,966 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-09 14:41:57,977 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-09 14:41:57,978 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-09 14:41:57,978 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-09 14:41:57,978 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-09 14:41:57,980 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-09 14:41:57,981 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-09 14:41:57,981 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-09 14:41:57,985 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-09 14:41:57,987 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-09 14:41:57,988 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-09 14:41:57,988 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-09 14:41:57,990 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-09 14:41:57,993 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-09 14:41:57,993 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-09 14:41:57,993 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-09 14:41:57,993 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-09 14:41:58,070 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 143@master
2024-01-09 14:41:58,145 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-09 14:41:58,146 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-09 14:41:58,178 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-09 14:41:58,205 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-09 14:41:58,205 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-09 14:41:58,211 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-09 14:41:58,211 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-09 14:41:58,409 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-09 14:41:58,409 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 412 msecs
2024-01-09 14:41:58,867 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-09 14:41:58,884 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-09 14:41:58,893 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-09 14:41:58,973 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-09 14:41:58,987 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-09 14:41:58,987 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-09 14:41:58,987 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-09 14:41:58,988 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-09 14:41:58,988 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-09 14:41:58,988 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-09 14:41:59,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-09 14:41:59,011 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-09 14:41:59,011 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-09 14:41:59,011 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-09 14:41:59,011 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-09 14:41:59,011 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-09 14:41:59,011 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 22 msec
2024-01-09 14:41:59,045 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-09 14:41:59,046 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-09 14:41:59,054 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2024-01-09 14:41:59,054 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-09 14:41:59,075 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-09 14:42:03,950 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=2413845c-6810-4457-b89a-db11054c2b05, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-e260f19a-81d4-4697-8a04-de9a411d3aba;nsid=1675739272;c=0) storage 2413845c-6810-4457-b89a-db11054c2b05
2024-01-09 14:42:03,953 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-09 14:42:03,954 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-09 14:42:03,964 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=848496bd-f03c-4d98-a516-4945fcf9da68, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-e260f19a-81d4-4697-8a04-de9a411d3aba;nsid=1675739272;c=0) storage 848496bd-f03c-4d98-a516-4945fcf9da68
2024-01-09 14:42:03,964 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-09 14:42:03,965 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-01-09 14:42:04,101 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-09 14:42:04,102 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-44466d3e-768f-41cc-8bf4-36800ef8bf34 for DN 172.18.0.4:50010
2024-01-09 14:42:04,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-09 14:42:04,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-86666c3a-6e11-4068-adb3-bec2903792c2 for DN 172.18.0.3:50010
2024-01-09 14:42:04,146 INFO BlockStateChange: BLOCK* processReport: from storage DS-44466d3e-768f-41cc-8bf4-36800ef8bf34 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=848496bd-f03c-4d98-a516-4945fcf9da68, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-e260f19a-81d4-4697-8a04-de9a411d3aba;nsid=1675739272;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-09 14:42:04,148 INFO BlockStateChange: BLOCK* processReport: from storage DS-86666c3a-6e11-4068-adb3-bec2903792c2 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=2413845c-6810-4457-b89a-db11054c2b05, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-e260f19a-81d4-4697-8a04-de9a411d3aba;nsid=1675739272;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2024-01-09 14:44:56,151 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 22 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 65 
2024-01-09 14:44:58,549 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:44:59,038 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:44:59,066 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2024-01-09 14:44:59,249 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:44:59,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:44:59,459 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:44:59,461 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:44:59,665 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:44:59,667 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:44:59,937 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:44:59,944 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741829_1005{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2024-01-09 14:45:00,146 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:00,146 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741830_1006{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2024-01-09 14:45:00,352 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:45:00,352 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:00,576 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:45:00,578 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:00,770 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:45:00,772 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:00,988 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:00,989 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741834_1010{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2024-01-09 14:45:01,205 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:45:01,206 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:01,417 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:01,420 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741836_1012{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2024-01-09 14:45:01,623 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:01,623 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741837_1013{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2024-01-09 14:45:01,837 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:45:01,839 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:02,068 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:45:02,071 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:02,289 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:02,290 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741840_1016{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2024-01-09 14:45:02,496 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:45:02,499 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:02,760 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:45:02,765 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:02,989 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:45:02,991 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:03,203 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:03,204 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741844_1020{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2024-01-09 14:45:03,431 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:45:03,432 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:03,661 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:03,661 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741846_1022{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2024-01-09 14:45:03,876 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:03,881 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741847_1023{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2024-01-09 14:45:04,077 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:45:04,078 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:04,293 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:45:04,294 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:04,496 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:04,497 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741850_1026{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2024-01-09 14:45:04,741 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:04,742 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741851_1027{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2024-01-09 14:45:05,260 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:45:05,261 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:05,969 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:45:05,970 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:06,179 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:06,181 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741854_1030{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 16777216
2024-01-09 14:45:06,569 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /input/data.txt._COPYING_
2024-01-09 14:45:06,570 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741855_1031{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 16777216
2024-01-09 14:45:07,268 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:45:07,286 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/data.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1657129345_1
2024-01-09 14:46:31,572 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 104 Total time for transactions(ms): 28 Number of transactions batched in Syncs: 0 Number of syncs: 38 SyncTimes(ms): 1147 
2024-01-09 14:47:47,449 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 107 Total time for transactions(ms): 31 Number of transactions batched in Syncs: 0 Number of syncs: 39 SyncTimes(ms): 1165 
2024-01-09 14:47:47,631 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000000_0/part-00000
2024-01-09 14:47:47,633 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000001_0/part-00001
2024-01-09 14:47:47,834 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:47,836 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:47,846 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:47,847 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:47,856 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000001_0/part-00001 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:47,873 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000000_0/part-00000 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:48,049 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000002_0/part-00002
2024-01-09 14:47:48,096 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:48,100 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:48,103 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000002_0/part-00002 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:48,158 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000003_0/part-00003
2024-01-09 14:47:48,200 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:48,202 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:48,210 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000003_0/part-00003 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:48,293 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000004_0/part-00004
2024-01-09 14:47:48,379 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:48,380 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:48,391 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000004_0/part-00004 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:48,411 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000005_0/part-00005
2024-01-09 14:47:48,475 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:48,477 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:48,482 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000005_0/part-00005 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:48,572 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000006_0/part-00006
2024-01-09 14:47:48,625 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:48,626 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:48,639 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000006_0/part-00006 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:48,642 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000007_0/part-00007
2024-01-09 14:47:48,697 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:48,700 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:48,713 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000007_0/part-00007 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:48,809 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000008_0/part-00008
2024-01-09 14:47:48,863 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:48,865 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:48,878 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000008_0/part-00008 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:48,881 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000009_0/part-00009
2024-01-09 14:47:48,928 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:48,931 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:48,944 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000009_0/part-00009 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:49,022 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000010_0/part-00010
2024-01-09 14:47:49,095 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:49,096 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:49,100 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000010_0/part-00010 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:49,140 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000011_0/part-00011
2024-01-09 14:47:49,207 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:49,209 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:49,216 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000011_0/part-00011 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:49,258 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000012_0/part-00012
2024-01-09 14:47:49,338 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:49,342 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:49,348 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000012_0/part-00012 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:49,454 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000013_0/part-00013
2024-01-09 14:47:49,487 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:49,488 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:49,492 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000014_0/part-00014
2024-01-09 14:47:49,496 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000013_0/part-00013 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:49,556 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:49,558 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:49,562 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000014_0/part-00014 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:49,691 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000015_0/part-00015
2024-01-09 14:47:49,702 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000016_0/part-00016
2024-01-09 14:47:49,735 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:49,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:49,739 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:49,740 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:49,744 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000015_0/part-00015 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:49,752 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000016_0/part-00016 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:49,916 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000018_0/part-00018
2024-01-09 14:47:49,959 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:49,966 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:49,975 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000017_0/part-00017
2024-01-09 14:47:49,975 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000018_0/part-00018 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:50,039 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:50,040 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:50,049 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000017_0/part-00017 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:50,155 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000019_0/part-00019
2024-01-09 14:47:50,200 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:50,203 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:50,205 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000019_0/part-00019 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:50,244 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000020_0/part-00020
2024-01-09 14:47:50,301 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:50,303 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:50,312 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000020_0/part-00020 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:50,417 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000021_0/part-00021
2024-01-09 14:47:50,472 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000022_0/part-00022
2024-01-09 14:47:50,475 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:50,475 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:50,486 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000021_0/part-00021 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:50,510 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:50,558 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:50,568 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000022_0/part-00022 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:50,639 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000023_0/part-00023
2024-01-09 14:47:50,688 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:50,692 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:50,700 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000023_0/part-00023 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:50,753 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000024_0/part-00024
2024-01-09 14:47:50,811 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:50,813 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:50,815 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000024_0/part-00024 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:50,879 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000025_0/part-00025
2024-01-09 14:47:50,930 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:50,931 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:50,939 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000025_0/part-00025 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:50,967 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000026_0/part-00026
2024-01-09 14:47:51,033 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:51,035 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:51,038 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000026_0/part-00026 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:51,118 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000027_0/part-00027
2024-01-09 14:47:51,180 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:51,182 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:51,186 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000027_0/part-00027 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:51,259 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000029_0/part-00029
2024-01-09 14:47:51,322 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:51,323 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:51,327 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000029_0/part-00029 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:51,409 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000030_0/part-00030
2024-01-09 14:47:51,459 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:51,460 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW]]} size 0
2024-01-09 14:47:51,467 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000030_0/part-00030 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:51,531 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} for /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000031_0/part-00031
2024-01-09 14:47:51,586 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:51,588 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.4:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-44466d3e-768f-41cc-8bf4-36800ef8bf34:NORMAL:172.18.0.4:50010|RBW], ReplicaUC[[DISK]DS-86666c3a-6e11-4068-adb3-bec2903792c2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2024-01-09 14:47:51,590 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000031_0/part-00031 is closed by DFSClient_attempt_20240109144629_0000_m_000000_0_307968708_26
2024-01-09 14:47:51,648 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_temporary/0/_temporary/attempt_20240109144631_0008_m_000028_0/part-00028 is closed by DFSClient_attempt_20240109144629_0000_m_000001_0_801507785_27
2024-01-09 14:47:52,110 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1146491808_16
2024-01-09 15:27:08,104 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-09 15:27:08,104 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-09 15:27:08,104 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-09 15:27:08,104 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 362 Total time for transactions(ms): 43 Number of transactions batched in Syncs: 1 Number of syncs: 201 SyncTimes(ms): 2119 
2024-01-09 15:27:08,131 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 362 Total time for transactions(ms): 43 Number of transactions batched in Syncs: 1 Number of syncs: 202 SyncTimes(ms): 2146 
2024-01-09 15:27:08,132 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000362
2024-01-09 15:27:08,134 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 363
2024-01-09 15:27:09,349 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.03s at 90.91 KB/s
2024-01-09 15:27:09,349 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000362 size 3322 bytes.
2024-01-09 15:27:09,375 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-09 16:27:10,006 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-09 16:27:10,006 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-09 16:27:10,006 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 363
2024-01-09 16:27:10,007 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 82 
2024-01-09 16:27:10,025 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 100 
2024-01-09 16:27:10,025 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000363 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000363-0000000000000000364
2024-01-09 16:27:10,026 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 365
2024-01-09 16:27:10,309 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.06s at 47.62 KB/s
2024-01-09 16:27:10,309 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000364 size 3322 bytes.
2024-01-09 16:27:10,344 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 362
2024-01-09 16:27:10,345 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2024-01-09 16:39:44,397 INFO BlockStateChange: BLOCK* processReport: from storage DS-44466d3e-768f-41cc-8bf4-36800ef8bf34 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=848496bd-f03c-4d98-a516-4945fcf9da68, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-e260f19a-81d4-4697-8a04-de9a411d3aba;nsid=1675739272;c=0), blocks: 52, hasStaleStorage: false, processing time: 1 msecs
2024-01-09 17:27:11,008 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-09 17:27:11,008 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-09 17:27:11,008 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 365
2024-01-09 17:27:11,008 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 84 
2024-01-09 17:27:11,030 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 106 
2024-01-09 17:27:11,030 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000365 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000365-0000000000000000366
2024-01-09 17:27:11,030 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 367
2024-01-09 17:27:11,260 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 81.08 KB/s
2024-01-09 17:27:11,261 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000366 size 3322 bytes.
2024-01-09 17:27:11,286 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 364
2024-01-09 17:27:11,286 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop/dfs/name/current/fsimage_0000000000000000362, cpktTxId=0000000000000000362)
2024-01-09 18:27:11,879 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-09 18:27:11,879 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-09 18:27:11,879 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 367
2024-01-09 18:27:11,879 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 68 
2024-01-09 18:27:11,902 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 90 
2024-01-09 18:27:11,902 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000367 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000367-0000000000000000368
2024-01-09 18:27:11,902 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 369
2024-01-09 18:27:12,128 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 81.08 KB/s
2024-01-09 18:27:12,128 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000368 size 3322 bytes.
2024-01-09 18:27:12,161 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 366
2024-01-09 18:27:12,161 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop/dfs/name/current/fsimage_0000000000000000364, cpktTxId=0000000000000000364)
2024-01-09 19:27:12,768 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-09 19:27:12,768 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-09 19:27:12,768 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 369
2024-01-09 19:27:12,768 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 64 
2024-01-09 19:27:12,792 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 88 
2024-01-09 19:27:12,793 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000369 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000369-0000000000000000370
2024-01-09 19:27:12,793 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 371
2024-01-09 19:27:13,075 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 56.60 KB/s
2024-01-09 19:27:13,075 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000370 size 3322 bytes.
2024-01-09 19:27:13,117 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 368
2024-01-09 19:27:13,117 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop/dfs/name/current/fsimage_0000000000000000366, cpktTxId=0000000000000000366)
2024-01-20 13:35:14,191 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-20 13:35:14,218 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-20 13:35:14,221 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-20 13:35:14,786 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-20 13:35:15,039 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-20 13:35:15,039 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-20 13:35:15,041 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-20 13:35:15,042 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-20 13:35:15,351 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-20 13:35:15,511 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-20 13:35:15,519 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-20 13:35:15,546 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-20 13:35:15,550 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-20 13:35:15,562 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-20 13:35:15,562 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-20 13:35:15,562 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-20 13:35:15,610 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-20 13:35:15,611 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-20 13:35:15,657 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-20 13:35:15,657 INFO org.mortbay.log: jetty-6.1.26
2024-01-20 13:35:16,031 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-20 13:35:16,090 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 13:35:16,090 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 13:35:16,133 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-20 13:35:16,133 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-20 13:35:16,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-20 13:35:16,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-20 13:35:16,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-20 13:35:16,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 20 13:35:16
2024-01-20 13:35:16,200 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-20 13:35:16,200 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 13:35:16,201 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-20 13:35:16,202 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-20 13:35:16,206 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-20 13:35:16,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-20 13:35:16,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-20 13:35:16,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-20 13:35:16,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-20 13:35:16,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-20 13:35:16,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-20 13:35:16,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-20 13:35:16,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-20 13:35:16,223 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-20 13:35:16,223 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-20 13:35:16,223 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-20 13:35:16,223 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-20 13:35:16,224 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-20 13:35:16,554 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-20 13:35:16,554 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 13:35:16,554 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-20 13:35:16,554 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-20 13:35:16,578 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-20 13:35:16,578 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-20 13:35:16,578 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-20 13:35:16,578 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-20 13:35:16,584 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-20 13:35:16,584 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 13:35:16,584 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-20 13:35:16,584 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-20 13:35:16,585 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-20 13:35:16,585 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-20 13:35:16,585 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-20 13:35:16,587 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-20 13:35:16,587 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-20 13:35:16,587 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-20 13:35:16,588 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-20 13:35:16,588 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-20 13:35:16,589 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-20 13:35:16,589 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 13:35:16,589 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-20 13:35:16,589 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-20 13:35:16,649 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 143@master
2024-01-20 13:35:16,723 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-20 13:35:16,723 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-20 13:35:16,782 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-20 13:35:16,812 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-20 13:35:16,812 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-20 13:35:16,830 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-20 13:35:16,830 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-20 13:35:17,020 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-20 13:35:17,021 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 430 msecs
2024-01-20 13:35:17,549 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-20 13:35:17,567 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-20 13:35:17,576 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-20 13:35:17,646 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-20 13:35:17,678 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 13:35:17,678 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 13:35:17,678 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-20 13:35:17,679 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-20 13:35:17,679 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-20 13:35:17,679 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-20 13:35:17,692 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 13:35:17,695 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-20 13:35:17,695 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-20 13:35:17,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-20 13:35:17,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-20 13:35:17,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-20 13:35:17,696 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 16 msec
2024-01-20 13:35:17,755 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-20 13:35:17,756 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-20 13:35:17,798 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2024-01-20 13:35:17,799 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-20 13:35:17,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-20 13:35:22,828 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=3e4d8b25-3327-4e50-afa6-cd99930bfa82, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-389d0ca0-b953-4d93-ae1e-d64cf269b391;nsid=1692160234;c=0) storage 3e4d8b25-3327-4e50-afa6-cd99930bfa82
2024-01-20 13:35:22,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 13:35:22,831 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-20 13:35:22,867 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=78df9b64-7d31-446c-80ed-63368df88aac, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-389d0ca0-b953-4d93-ae1e-d64cf269b391;nsid=1692160234;c=0) storage 78df9b64-7d31-446c-80ed-63368df88aac
2024-01-20 13:35:22,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 13:35:22,868 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-01-20 13:35:22,998 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 13:35:22,998 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c6944e1e-f9d7-49b2-bcce-d0510d5e168c for DN 172.18.0.4:50010
2024-01-20 13:35:23,005 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 13:35:23,005 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-5560514a-94ef-4076-a94c-27b944584263 for DN 172.18.0.3:50010
2024-01-20 13:35:23,043 INFO BlockStateChange: BLOCK* processReport: from storage DS-c6944e1e-f9d7-49b2-bcce-d0510d5e168c node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=78df9b64-7d31-446c-80ed-63368df88aac, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-389d0ca0-b953-4d93-ae1e-d64cf269b391;nsid=1692160234;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-20 13:35:23,049 INFO BlockStateChange: BLOCK* processReport: from storage DS-5560514a-94ef-4076-a94c-27b944584263 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=3e4d8b25-3327-4e50-afa6-cd99930bfa82, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-389d0ca0-b953-4d93-ae1e-d64cf269b391;nsid=1692160234;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2024-01-20 13:36:26,283 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-20 13:36:26,283 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-20 13:36:26,283 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-20 13:36:26,283 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 58 
2024-01-20 13:36:26,294 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 69 
2024-01-20 13:36:26,295 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-20 13:36:26,297 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-20 13:36:27,581 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2024-01-20 13:36:27,581 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-01-20 13:36:27,624 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-20 16:19:33,251 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-20 16:19:33,284 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-20 16:19:33,289 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-20 16:19:33,749 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-20 16:19:33,865 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-20 16:19:33,865 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-20 16:19:33,867 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-20 16:19:33,868 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-20 16:19:34,186 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-20 16:19:34,338 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-20 16:19:34,348 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-20 16:19:34,365 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-20 16:19:34,371 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-20 16:19:34,384 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-20 16:19:34,384 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-20 16:19:34,384 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-20 16:19:34,420 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-20 16:19:34,422 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-20 16:19:34,452 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-20 16:19:34,452 INFO org.mortbay.log: jetty-6.1.26
2024-01-20 16:19:34,795 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-20 16:19:34,844 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 16:19:34,844 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 16:19:34,891 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-20 16:19:34,891 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-20 16:19:34,947 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-20 16:19:34,947 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-20 16:19:34,957 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-20 16:19:34,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 20 16:19:34
2024-01-20 16:19:34,960 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-20 16:19:34,960 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 16:19:34,961 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-20 16:19:34,961 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-20 16:19:34,965 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-20 16:19:34,971 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-20 16:19:34,971 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-20 16:19:34,971 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-20 16:19:34,971 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-20 16:19:34,971 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-20 16:19:34,971 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-20 16:19:34,971 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-20 16:19:34,971 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-20 16:19:34,975 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-20 16:19:34,976 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-20 16:19:34,976 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-20 16:19:34,977 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-20 16:19:34,989 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-20 16:19:35,342 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-20 16:19:35,343 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 16:19:35,343 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-20 16:19:35,343 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-20 16:19:35,364 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-20 16:19:35,364 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-20 16:19:35,364 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-20 16:19:35,365 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-20 16:19:35,380 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-20 16:19:35,380 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 16:19:35,380 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-20 16:19:35,380 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-20 16:19:35,381 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-20 16:19:35,381 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-20 16:19:35,381 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-20 16:19:35,383 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-20 16:19:35,384 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-20 16:19:35,385 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-20 16:19:35,385 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-20 16:19:35,385 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-20 16:19:35,387 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-20 16:19:35,387 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 16:19:35,387 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-20 16:19:35,387 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-20 16:19:35,454 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 1754@master
2024-01-20 16:19:35,524 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-20 16:19:35,524 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-20 16:19:35,557 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-20 16:19:35,589 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-20 16:19:35,589 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-20 16:19:35,594 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-20 16:19:35,595 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-20 16:19:35,770 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-20 16:19:35,771 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 375 msecs
2024-01-20 16:19:36,143 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-20 16:19:36,147 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-20 16:19:36,156 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-20 16:19:36,233 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-20 16:19:36,258 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 16:19:36,258 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 16:19:36,258 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-20 16:19:36,259 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-20 16:19:36,259 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-20 16:19:36,259 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-20 16:19:36,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 16:19:36,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-20 16:19:36,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-20 16:19:36,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-20 16:19:36,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-20 16:19:36,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-20 16:19:36,275 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 15 msec
2024-01-20 16:19:36,333 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-20 16:19:36,337 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-20 16:19:36,340 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2024-01-20 16:19:36,340 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-20 16:19:36,359 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-20 16:19:41,358 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=905d38c6-3ed8-4994-b740-baed0e8781c9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a37a394-0884-486e-9b78-814b7ed43dbe;nsid=2024432647;c=0) storage 905d38c6-3ed8-4994-b740-baed0e8781c9
2024-01-20 16:19:41,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 16:19:41,364 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-20 16:19:41,406 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=8397fcea-5b6e-4f12-92e4-7d3800643d49, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a37a394-0884-486e-9b78-814b7ed43dbe;nsid=2024432647;c=0) storage 8397fcea-5b6e-4f12-92e4-7d3800643d49
2024-01-20 16:19:41,406 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 16:19:41,406 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-01-20 16:19:41,519 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 16:19:41,520 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9e00c6f7-30df-417d-9011-8a879e7fca53 for DN 172.18.0.4:50010
2024-01-20 16:19:41,525 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 16:19:41,531 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-f1c6d22a-a7db-4e29-967f-5c7ca7435419 for DN 172.18.0.3:50010
2024-01-20 16:19:41,573 INFO BlockStateChange: BLOCK* processReport: from storage DS-9e00c6f7-30df-417d-9011-8a879e7fca53 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=8397fcea-5b6e-4f12-92e4-7d3800643d49, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a37a394-0884-486e-9b78-814b7ed43dbe;nsid=2024432647;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-20 16:19:41,576 INFO BlockStateChange: BLOCK* processReport: from storage DS-f1c6d22a-a7db-4e29-967f-5c7ca7435419 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=905d38c6-3ed8-4994-b740-baed0e8781c9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a37a394-0884-486e-9b78-814b7ed43dbe;nsid=2024432647;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2024-01-20 16:23:45,241 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-20 16:23:45,241 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-20 16:23:45,241 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-20 16:23:45,241 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 44 
2024-01-20 16:23:45,261 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 63 
2024-01-20 16:23:45,262 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-20 16:23:45,263 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-20 16:23:46,437 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2024-01-20 16:23:46,437 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-01-20 16:23:46,481 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-20 17:00:23,374 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-20 17:00:23,380 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-20 17:00:23,394 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-20 17:00:23,777 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-20 17:00:23,875 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-20 17:00:23,875 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-20 17:00:23,877 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-20 17:00:23,878 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-20 17:00:24,115 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-20 17:00:24,175 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-20 17:00:24,194 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-20 17:00:24,199 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-20 17:00:24,202 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-20 17:00:24,212 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-20 17:00:24,213 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-20 17:00:24,213 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-20 17:00:24,231 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-20 17:00:24,232 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-20 17:00:24,264 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-20 17:00:24,265 INFO org.mortbay.log: jetty-6.1.26
2024-01-20 17:00:24,433 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-20 17:00:24,475 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 17:00:24,475 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 17:00:24,526 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-20 17:00:24,527 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-20 17:00:24,586 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-20 17:00:24,586 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-20 17:00:24,587 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-20 17:00:24,588 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 20 17:00:24
2024-01-20 17:00:24,590 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-20 17:00:24,590 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 17:00:24,596 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-20 17:00:24,596 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-20 17:00:24,601 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-20 17:00:24,612 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-20 17:00:24,612 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-20 17:00:24,612 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-20 17:00:24,612 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-20 17:00:24,612 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-20 17:00:24,612 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-20 17:00:24,612 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-20 17:00:24,612 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-20 17:00:24,617 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-20 17:00:24,617 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-20 17:00:24,617 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-20 17:00:24,618 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-20 17:00:24,619 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-20 17:00:24,933 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-20 17:00:24,933 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 17:00:24,934 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-20 17:00:24,934 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-20 17:00:24,958 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-20 17:00:24,958 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-20 17:00:24,958 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-20 17:00:24,958 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-20 17:00:24,963 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-20 17:00:24,963 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 17:00:24,963 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-20 17:00:24,963 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-20 17:00:24,964 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-20 17:00:24,964 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-20 17:00:24,964 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-20 17:00:24,966 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-20 17:00:24,966 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-20 17:00:24,966 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-20 17:00:24,967 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-20 17:00:24,967 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-20 17:00:24,968 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-20 17:00:24,968 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 17:00:24,979 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-20 17:00:24,979 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-20 17:00:25,020 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 143@master
2024-01-20 17:00:25,084 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-20 17:00:25,085 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-20 17:00:25,115 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-20 17:00:25,142 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-20 17:00:25,142 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-20 17:00:25,148 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-20 17:00:25,148 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-20 17:00:25,367 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-20 17:00:25,367 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 386 msecs
2024-01-20 17:00:25,620 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-20 17:00:25,625 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-20 17:00:25,646 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-20 17:00:25,690 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-20 17:00:25,708 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 17:00:25,708 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 17:00:25,708 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-20 17:00:25,708 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-20 17:00:25,709 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-20 17:00:25,709 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-20 17:00:25,724 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 17:00:25,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-20 17:00:25,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-20 17:00:25,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-20 17:00:25,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-20 17:00:25,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-20 17:00:25,749 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 39 msec
2024-01-20 17:00:25,780 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-20 17:00:25,785 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-20 17:00:25,787 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2024-01-20 17:00:25,787 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-20 17:00:25,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-20 17:00:30,742 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=d9121e60-53bd-44ab-ba32-52b548a8f483, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-770b6b9f-afc4-4233-a71d-9fdb943b02b5;nsid=757749215;c=0) storage d9121e60-53bd-44ab-ba32-52b548a8f483
2024-01-20 17:00:30,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 17:00:30,746 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-01-20 17:00:30,813 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=1f9a6ae9-ec68-496e-b824-fa3f9c707a3e, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-770b6b9f-afc4-4233-a71d-9fdb943b02b5;nsid=757749215;c=0) storage 1f9a6ae9-ec68-496e-b824-fa3f9c707a3e
2024-01-20 17:00:30,813 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 17:00:30,813 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-20 17:00:30,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 17:00:30,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9fb60ade-859c-482a-9612-d3ed13177484 for DN 172.18.0.4:50010
2024-01-20 17:00:30,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 17:00:30,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-71a4883e-6128-47b8-92af-ab914e2d7a22 for DN 172.18.0.3:50010
2024-01-20 17:00:30,892 INFO BlockStateChange: BLOCK* processReport: from storage DS-71a4883e-6128-47b8-92af-ab914e2d7a22 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=1f9a6ae9-ec68-496e-b824-fa3f9c707a3e, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-770b6b9f-afc4-4233-a71d-9fdb943b02b5;nsid=757749215;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-20 17:00:30,903 INFO BlockStateChange: BLOCK* processReport: from storage DS-9fb60ade-859c-482a-9612-d3ed13177484 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=d9121e60-53bd-44ab-ba32-52b548a8f483, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-770b6b9f-afc4-4233-a71d-9fdb943b02b5;nsid=757749215;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2024-01-20 17:01:34,824 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-20 17:01:34,824 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-20 17:01:34,824 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-20 17:01:34,825 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 106 
2024-01-20 17:01:34,849 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 131 
2024-01-20 17:01:34,850 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-20 17:01:34,852 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-20 17:01:35,968 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2024-01-20 17:01:35,968 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-01-20 17:01:35,995 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-20 17:16:12,726 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-20 17:16:12,734 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-20 17:16:12,747 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-20 17:16:13,140 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-20 17:16:13,248 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-20 17:16:13,248 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-20 17:16:13,249 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-20 17:16:13,251 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-20 17:16:13,513 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-20 17:16:13,583 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-20 17:16:13,589 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-20 17:16:13,594 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-20 17:16:13,599 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-20 17:16:13,611 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-20 17:16:13,611 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-20 17:16:13,611 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-20 17:16:13,637 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-20 17:16:13,638 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-20 17:16:13,665 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-20 17:16:13,665 INFO org.mortbay.log: jetty-6.1.26
2024-01-20 17:16:13,850 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-20 17:16:13,896 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 17:16:13,896 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 17:16:13,945 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-20 17:16:13,945 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-20 17:16:14,003 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-20 17:16:14,003 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-20 17:16:14,010 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-20 17:16:14,011 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 20 17:16:14
2024-01-20 17:16:14,013 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-20 17:16:14,013 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 17:16:14,014 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-20 17:16:14,014 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-20 17:16:14,024 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-20 17:16:14,024 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-20 17:16:14,024 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-20 17:16:14,024 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-20 17:16:14,024 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-20 17:16:14,024 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-20 17:16:14,024 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-20 17:16:14,024 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-20 17:16:14,024 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-20 17:16:14,032 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-20 17:16:14,032 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-20 17:16:14,032 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-20 17:16:14,038 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-20 17:16:14,040 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-20 17:16:14,409 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-20 17:16:14,409 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 17:16:14,409 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-20 17:16:14,410 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-20 17:16:14,432 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-20 17:16:14,432 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-20 17:16:14,433 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-20 17:16:14,433 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-20 17:16:14,439 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-20 17:16:14,439 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 17:16:14,439 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-20 17:16:14,439 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-20 17:16:14,440 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-20 17:16:14,440 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-20 17:16:14,440 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-20 17:16:14,446 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-20 17:16:14,449 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-20 17:16:14,450 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-20 17:16:14,451 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-20 17:16:14,451 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-20 17:16:14,452 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-20 17:16:14,452 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 17:16:14,456 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-20 17:16:14,456 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-20 17:16:14,513 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 143@master
2024-01-20 17:16:14,583 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-20 17:16:14,583 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-20 17:16:14,613 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-20 17:16:14,639 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-20 17:16:14,639 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-20 17:16:14,645 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-20 17:16:14,645 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-20 17:16:14,877 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-20 17:16:14,877 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 419 msecs
2024-01-20 17:16:15,105 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-20 17:16:15,113 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-20 17:16:15,129 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-20 17:16:15,194 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-20 17:16:15,212 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 17:16:15,212 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 17:16:15,212 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-20 17:16:15,215 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-20 17:16:15,215 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-20 17:16:15,215 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-20 17:16:15,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 17:16:15,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-20 17:16:15,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-20 17:16:15,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-20 17:16:15,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-20 17:16:15,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-20 17:16:15,266 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 49 msec
2024-01-20 17:16:15,292 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-20 17:16:15,297 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-20 17:16:15,351 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2024-01-20 17:16:15,351 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-20 17:16:15,357 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-20 17:16:20,408 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=2d34e84d-287e-4b73-8579-e54384fb3e2c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b8843bbc-681b-4a29-bd0f-a3d3dd4bfc1f;nsid=2096026784;c=0) storage 2d34e84d-287e-4b73-8579-e54384fb3e2c
2024-01-20 17:16:20,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 17:16:20,409 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-01-20 17:16:20,453 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=246d9baa-5d08-440d-a82d-3d837b0e5c44, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b8843bbc-681b-4a29-bd0f-a3d3dd4bfc1f;nsid=2096026784;c=0) storage 246d9baa-5d08-440d-a82d-3d837b0e5c44
2024-01-20 17:16:20,453 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 17:16:20,453 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-20 17:16:20,513 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 17:16:20,513 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-0360fe39-d77d-4387-912d-01898a41c241 for DN 172.18.0.4:50010
2024-01-20 17:16:20,519 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 17:16:20,519 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-316c1a1d-771c-45de-81a1-d427aa5efcc5 for DN 172.18.0.3:50010
2024-01-20 17:16:20,564 INFO BlockStateChange: BLOCK* processReport: from storage DS-0360fe39-d77d-4387-912d-01898a41c241 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=2d34e84d-287e-4b73-8579-e54384fb3e2c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b8843bbc-681b-4a29-bd0f-a3d3dd4bfc1f;nsid=2096026784;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-20 17:16:20,572 INFO BlockStateChange: BLOCK* processReport: from storage DS-316c1a1d-771c-45de-81a1-d427aa5efcc5 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=246d9baa-5d08-440d-a82d-3d837b0e5c44, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b8843bbc-681b-4a29-bd0f-a3d3dd4bfc1f;nsid=2096026784;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2024-01-20 17:17:24,584 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-20 17:17:24,585 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-20 17:17:24,585 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-20 17:17:24,585 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 107 
2024-01-20 17:17:24,606 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 128 
2024-01-20 17:17:24,607 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-20 17:17:24,609 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-20 17:17:25,730 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.03s at 0.00 KB/s
2024-01-20 17:17:25,730 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-01-20 17:17:25,765 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-20 21:11:43,372 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-20 21:11:43,403 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-20 21:11:43,406 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-20 21:11:44,095 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-20 21:11:44,324 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-20 21:11:44,324 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-20 21:11:44,326 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-20 21:11:44,327 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-20 21:11:44,674 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-20 21:11:44,853 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-20 21:11:44,867 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-20 21:11:44,901 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-20 21:11:44,909 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-20 21:11:44,926 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-20 21:11:44,926 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-20 21:11:44,926 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-20 21:11:44,981 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-20 21:11:44,983 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-20 21:11:45,036 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-20 21:11:45,037 INFO org.mortbay.log: jetty-6.1.26
2024-01-20 21:11:45,509 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-20 21:11:45,624 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 21:11:45,624 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 21:11:45,677 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-20 21:11:45,677 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-20 21:11:45,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-20 21:11:45,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-20 21:11:45,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-20 21:11:45,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 20 21:11:45
2024-01-20 21:11:45,748 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-20 21:11:45,748 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 21:11:45,762 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-20 21:11:45,762 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-20 21:11:45,768 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-20 21:11:45,768 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-20 21:11:45,768 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-20 21:11:45,768 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-20 21:11:45,768 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-20 21:11:45,768 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-20 21:11:45,768 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-20 21:11:45,768 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-20 21:11:45,768 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-20 21:11:45,774 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-20 21:11:45,774 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-20 21:11:45,774 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-20 21:11:45,785 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-20 21:11:45,787 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-20 21:11:46,218 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-20 21:11:46,218 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 21:11:46,219 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-20 21:11:46,219 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-20 21:11:46,240 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-20 21:11:46,240 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-20 21:11:46,240 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-20 21:11:46,241 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-20 21:11:46,256 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-20 21:11:46,256 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 21:11:46,257 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-20 21:11:46,257 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-20 21:11:46,258 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-20 21:11:46,258 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-20 21:11:46,258 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-20 21:11:46,260 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-20 21:11:46,261 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-20 21:11:46,261 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-20 21:11:46,262 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-20 21:11:46,263 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-20 21:11:46,264 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-20 21:11:46,264 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 21:11:46,264 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-20 21:11:46,264 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-20 21:11:46,318 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 143@master
2024-01-20 21:11:46,380 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-20 21:11:46,380 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-20 21:11:46,422 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-20 21:11:46,454 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-20 21:11:46,455 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-20 21:11:46,460 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-20 21:11:46,460 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-20 21:11:46,649 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-20 21:11:46,649 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 375 msecs
2024-01-20 21:11:47,093 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-20 21:11:47,114 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-20 21:11:47,138 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-20 21:11:47,228 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-20 21:11:47,262 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 21:11:47,262 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 21:11:47,263 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-20 21:11:47,264 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-20 21:11:47,264 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-20 21:11:47,264 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-20 21:11:47,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 21:11:47,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-20 21:11:47,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-20 21:11:47,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-20 21:11:47,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-20 21:11:47,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-20 21:11:47,295 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 28 msec
2024-01-20 21:11:47,367 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2024-01-20 21:11:47,367 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-20 21:11:47,363 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-20 21:11:47,365 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-20 21:11:47,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-20 21:11:51,775 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=e88a49bd-e34e-4336-a9b4-fa0c5def2621, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9e37cee3-5766-489c-898a-40c8b8b382be;nsid=2087465963;c=0) storage e88a49bd-e34e-4336-a9b4-fa0c5def2621
2024-01-20 21:11:51,778 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 21:11:51,779 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-20 21:11:51,795 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=5bd23252-91ad-4471-9444-89926efc8afa, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9e37cee3-5766-489c-898a-40c8b8b382be;nsid=2087465963;c=0) storage 5bd23252-91ad-4471-9444-89926efc8afa
2024-01-20 21:11:51,795 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 21:11:51,795 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-01-20 21:11:51,926 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 21:11:51,926 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-36d9a03e-f567-4bff-930c-7aff2574be1a for DN 172.18.0.4:50010
2024-01-20 21:11:51,939 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 21:11:51,939 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9541843e-9944-4191-9fe0-abeef55bde37 for DN 172.18.0.3:50010
2024-01-20 21:11:51,987 INFO BlockStateChange: BLOCK* processReport: from storage DS-9541843e-9944-4191-9fe0-abeef55bde37 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=e88a49bd-e34e-4336-a9b4-fa0c5def2621, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9e37cee3-5766-489c-898a-40c8b8b382be;nsid=2087465963;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2024-01-20 21:11:51,990 INFO BlockStateChange: BLOCK* processReport: from storage DS-36d9a03e-f567-4bff-930c-7aff2574be1a node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=5bd23252-91ad-4471-9444-89926efc8afa, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9e37cee3-5766-489c-898a-40c8b8b382be;nsid=2087465963;c=0), blocks: 0, hasStaleStorage: false, processing time: 5 msecs
2024-01-20 21:37:55,948 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-20 21:37:55,958 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-20 21:37:55,959 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-20 21:37:55,959 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 57 
2024-01-20 21:37:55,977 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 75 
2024-01-20 21:37:55,978 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-20 21:37:55,991 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-20 21:37:57,277 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2024-01-20 21:37:57,277 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-01-20 21:37:57,311 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-20 22:16:38,085 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-20 22:16:38,091 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-20 22:16:38,094 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-20 22:16:38,495 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-20 22:16:38,596 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-20 22:16:38,596 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-20 22:16:38,598 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-20 22:16:38,599 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-20 22:16:38,826 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-20 22:16:38,894 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-20 22:16:38,900 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-20 22:16:38,906 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-20 22:16:38,911 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-20 22:16:38,925 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-20 22:16:38,925 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-20 22:16:38,925 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-20 22:16:38,949 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-20 22:16:38,950 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-20 22:16:38,984 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-20 22:16:38,984 INFO org.mortbay.log: jetty-6.1.26
2024-01-20 22:16:39,145 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-20 22:16:39,186 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 22:16:39,186 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 22:16:39,230 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-20 22:16:39,230 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-20 22:16:39,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-20 22:16:39,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-20 22:16:39,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-20 22:16:39,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 20 22:16:39
2024-01-20 22:16:39,295 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-20 22:16:39,295 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 22:16:39,296 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-20 22:16:39,296 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-20 22:16:39,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-20 22:16:39,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-20 22:16:39,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-20 22:16:39,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-20 22:16:39,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-20 22:16:39,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-20 22:16:39,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-20 22:16:39,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-20 22:16:39,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-20 22:16:39,317 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-20 22:16:39,317 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-20 22:16:39,317 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-20 22:16:39,317 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-20 22:16:39,318 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-20 22:16:39,651 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-20 22:16:39,651 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 22:16:39,652 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-20 22:16:39,652 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-20 22:16:39,673 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-20 22:16:39,673 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-20 22:16:39,673 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-20 22:16:39,674 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-20 22:16:39,682 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-20 22:16:39,683 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 22:16:39,683 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-20 22:16:39,683 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-20 22:16:39,684 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-20 22:16:39,684 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-20 22:16:39,684 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-20 22:16:39,686 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-20 22:16:39,686 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-20 22:16:39,687 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-20 22:16:39,688 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-20 22:16:39,688 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-20 22:16:39,697 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-20 22:16:39,697 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 22:16:39,698 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-20 22:16:39,698 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-20 22:16:39,756 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 143@master
2024-01-20 22:16:39,829 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-20 22:16:39,830 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-20 22:16:39,863 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-20 22:16:39,896 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-20 22:16:39,896 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-20 22:16:39,903 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-20 22:16:39,903 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-20 22:16:40,111 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-20 22:16:40,112 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 406 msecs
2024-01-20 22:16:40,369 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-20 22:16:40,386 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-20 22:16:40,402 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-20 22:16:40,436 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-20 22:16:40,444 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 22:16:40,444 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 22:16:40,444 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-20 22:16:40,445 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-20 22:16:40,445 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-20 22:16:40,445 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-20 22:16:40,458 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 22:16:40,465 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-20 22:16:40,465 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-20 22:16:40,465 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-20 22:16:40,465 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-20 22:16:40,465 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-20 22:16:40,465 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 20 msec
2024-01-20 22:16:40,513 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-20 22:16:40,514 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-20 22:16:40,516 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2024-01-20 22:16:40,516 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-20 22:16:40,534 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-20 22:16:46,157 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=0a97d558-3504-4ae1-95d1-132c6ff3d8eb, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-089f6790-0632-4a14-b372-671e4247dc3d;nsid=842859768;c=0) storage 0a97d558-3504-4ae1-95d1-132c6ff3d8eb
2024-01-20 22:16:46,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 22:16:46,161 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-01-20 22:16:46,182 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=768cd4ba-c7dc-426b-a8c8-483fcdf2227f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-089f6790-0632-4a14-b372-671e4247dc3d;nsid=842859768;c=0) storage 768cd4ba-c7dc-426b-a8c8-483fcdf2227f
2024-01-20 22:16:46,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 22:16:46,183 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-20 22:16:46,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 22:16:46,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c9462270-80a0-4308-a184-c4ddf9d3a1da for DN 172.18.0.4:50010
2024-01-20 22:16:46,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 22:16:46,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-eeecef26-18d2-4030-afc5-4ebfcfe3ceb5 for DN 172.18.0.3:50010
2024-01-20 22:16:46,339 INFO BlockStateChange: BLOCK* processReport: from storage DS-c9462270-80a0-4308-a184-c4ddf9d3a1da node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=0a97d558-3504-4ae1-95d1-132c6ff3d8eb, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-089f6790-0632-4a14-b372-671e4247dc3d;nsid=842859768;c=0), blocks: 0, hasStaleStorage: false, processing time: 5 msecs
2024-01-20 22:16:46,347 INFO BlockStateChange: BLOCK* processReport: from storage DS-eeecef26-18d2-4030-afc5-4ebfcfe3ceb5 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=768cd4ba-c7dc-426b-a8c8-483fcdf2227f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-089f6790-0632-4a14-b372-671e4247dc3d;nsid=842859768;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2024-01-20 22:17:50,019 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-20 22:17:50,019 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-20 22:17:50,019 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-20 22:17:50,019 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 89 
2024-01-20 22:17:50,046 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 116 
2024-01-20 22:17:50,047 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-20 22:17:50,049 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-20 22:17:51,300 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2024-01-20 22:17:51,301 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-01-20 22:17:51,337 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-20 22:20:35,888 INFO BlockStateChange: BLOCK* processReport: from storage DS-eeecef26-18d2-4030-afc5-4ebfcfe3ceb5 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=768cd4ba-c7dc-426b-a8c8-483fcdf2227f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-089f6790-0632-4a14-b372-671e4247dc3d;nsid=842859768;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-20 22:38:50,862 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-20 22:38:50,883 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-20 22:38:50,886 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-20 22:38:51,500 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-20 22:38:51,743 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-20 22:38:51,743 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-20 22:38:51,746 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-20 22:38:51,746 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-20 22:38:52,103 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-20 22:38:52,263 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-20 22:38:52,281 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-20 22:38:52,315 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-20 22:38:52,318 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-20 22:38:52,330 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-20 22:38:52,330 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-20 22:38:52,330 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-20 22:38:52,380 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-20 22:38:52,381 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-20 22:38:52,425 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-20 22:38:52,426 INFO org.mortbay.log: jetty-6.1.26
2024-01-20 22:38:52,790 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-20 22:38:52,838 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 22:38:52,838 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-20 22:38:52,888 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-20 22:38:52,888 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-20 22:38:52,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-20 22:38:52,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-20 22:38:52,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-20 22:38:52,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 20 22:38:52
2024-01-20 22:38:52,964 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-20 22:38:52,964 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 22:38:52,966 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-20 22:38:52,966 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-20 22:38:52,970 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-20 22:38:52,970 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-20 22:38:52,970 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-20 22:38:52,970 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-20 22:38:52,970 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-20 22:38:52,970 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-20 22:38:52,970 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-20 22:38:52,970 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-20 22:38:52,970 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-20 22:38:52,987 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-20 22:38:52,988 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-20 22:38:52,988 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-20 22:38:52,988 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-20 22:38:52,990 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-20 22:38:53,416 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-20 22:38:53,416 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 22:38:53,416 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-20 22:38:53,416 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-20 22:38:53,437 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-20 22:38:53,437 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-20 22:38:53,437 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-20 22:38:53,438 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-20 22:38:53,449 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-20 22:38:53,449 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 22:38:53,449 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-20 22:38:53,449 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-20 22:38:53,450 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-20 22:38:53,450 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-20 22:38:53,450 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-20 22:38:53,452 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-20 22:38:53,452 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-20 22:38:53,452 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-20 22:38:53,453 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-20 22:38:53,453 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-20 22:38:53,454 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-20 22:38:53,454 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-20 22:38:53,454 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-20 22:38:53,454 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-20 22:38:53,543 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 143@master
2024-01-20 22:38:53,617 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-20 22:38:53,618 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-20 22:38:53,656 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-20 22:38:53,696 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-20 22:38:53,696 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-20 22:38:53,702 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-20 22:38:53,702 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-20 22:38:53,906 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-20 22:38:53,907 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 436 msecs
2024-01-20 22:38:54,302 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-20 22:38:54,319 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-20 22:38:54,328 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-20 22:38:54,424 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-20 22:38:54,446 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 22:38:54,446 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-20 22:38:54,446 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-20 22:38:54,452 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-20 22:38:54,452 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-20 22:38:54,452 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-20 22:38:54,466 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 22:38:54,484 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-20 22:38:54,484 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-20 22:38:54,484 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-20 22:38:54,484 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-20 22:38:54,484 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-20 22:38:54,484 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 31 msec
2024-01-20 22:38:54,542 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-20 22:38:54,543 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-20 22:38:54,546 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2024-01-20 22:38:54,546 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-20 22:38:54,576 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-20 22:38:58,835 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=93c69fca-1048-42dd-b044-d3ac19f33cd3, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-2d22698e-c393-4ab9-adbe-72631a4165c4;nsid=178358391;c=0) storage 93c69fca-1048-42dd-b044-d3ac19f33cd3
2024-01-20 22:38:58,839 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 22:38:58,840 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-01-20 22:38:58,857 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=f00c4886-b020-440c-b630-22b0e1c2b669, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-2d22698e-c393-4ab9-adbe-72631a4165c4;nsid=178358391;c=0) storage f00c4886-b020-440c-b630-22b0e1c2b669
2024-01-20 22:38:58,857 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 22:38:58,870 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-20 22:38:59,011 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 22:38:59,011 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d18a9ba5-2f9f-4bf4-a832-691f38f55921 for DN 172.18.0.4:50010
2024-01-20 22:38:59,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-20 22:38:59,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b3957d6d-9000-4d60-90af-f32289a7e40e for DN 172.18.0.3:50010
2024-01-20 22:38:59,061 INFO BlockStateChange: BLOCK* processReport: from storage DS-d18a9ba5-2f9f-4bf4-a832-691f38f55921 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=93c69fca-1048-42dd-b044-d3ac19f33cd3, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-2d22698e-c393-4ab9-adbe-72631a4165c4;nsid=178358391;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-20 22:38:59,063 INFO BlockStateChange: BLOCK* processReport: from storage DS-b3957d6d-9000-4d60-90af-f32289a7e40e node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=f00c4886-b020-440c-b630-22b0e1c2b669, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-2d22698e-c393-4ab9-adbe-72631a4165c4;nsid=178358391;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2024-01-20 22:40:02,969 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-20 22:40:02,969 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-20 22:40:02,969 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-20 22:40:02,969 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 70 
2024-01-20 22:40:02,996 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 96 
2024-01-20 22:40:02,997 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-20 22:40:02,998 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-20 22:40:04,209 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2024-01-20 22:40:04,209 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 350 bytes.
2024-01-20 22:40:04,255 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-21 16:56:57,945 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-21 16:56:57,979 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-21 16:56:57,984 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-21 16:56:58,542 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-21 16:56:58,750 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-21 16:56:58,750 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-21 16:56:58,753 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-21 16:56:58,754 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-21 16:56:59,087 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-21 16:56:59,231 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-21 16:56:59,236 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-21 16:56:59,266 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-21 16:56:59,272 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-21 16:56:59,282 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-21 16:56:59,282 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-21 16:56:59,282 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-21 16:56:59,330 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-21 16:56:59,331 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-21 16:56:59,376 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-21 16:56:59,377 INFO org.mortbay.log: jetty-6.1.26
2024-01-21 16:56:59,774 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-21 16:56:59,820 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-21 16:56:59,820 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-21 16:56:59,867 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-21 16:56:59,867 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-21 16:56:59,929 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-21 16:56:59,929 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-21 16:56:59,932 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-21 16:56:59,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 21 16:56:59
2024-01-21 16:56:59,938 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-21 16:56:59,938 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-21 16:56:59,940 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-21 16:56:59,940 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-21 16:56:59,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-21 16:56:59,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-21 16:56:59,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-21 16:56:59,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-21 16:56:59,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-21 16:56:59,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-21 16:56:59,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-21 16:56:59,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-21 16:56:59,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-21 16:56:59,960 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-21 16:56:59,960 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-21 16:56:59,960 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-21 16:56:59,962 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-21 16:56:59,964 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-21 16:57:00,330 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-21 16:57:00,330 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-21 16:57:00,330 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-21 16:57:00,330 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-21 16:57:00,351 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-21 16:57:00,351 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-21 16:57:00,351 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-21 16:57:00,351 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-21 16:57:00,363 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-21 16:57:00,363 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-21 16:57:00,363 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-21 16:57:00,363 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-21 16:57:00,365 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-21 16:57:00,365 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-21 16:57:00,365 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-21 16:57:00,370 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-21 16:57:00,371 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-21 16:57:00,373 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-21 16:57:00,374 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-21 16:57:00,374 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-21 16:57:00,377 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-21 16:57:00,377 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-21 16:57:00,377 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-21 16:57:00,377 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-21 16:57:00,454 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 143@master
2024-01-21 16:57:00,531 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-21 16:57:00,533 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-21 16:57:00,573 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-21 16:57:00,605 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-21 16:57:00,605 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-21 16:57:00,611 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-21 16:57:00,611 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-21 16:57:00,892 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-21 16:57:00,892 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 511 msecs
2024-01-21 16:57:01,361 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-21 16:57:01,377 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-21 16:57:01,387 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-21 16:57:01,474 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-21 16:57:01,496 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-21 16:57:01,497 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-21 16:57:01,497 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-21 16:57:01,499 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-21 16:57:01,500 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-21 16:57:01,500 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-21 16:57:01,510 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-21 16:57:01,521 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-21 16:57:01,521 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-21 16:57:01,521 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-21 16:57:01,521 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-21 16:57:01,521 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-21 16:57:01,521 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 21 msec
2024-01-21 16:57:01,578 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-21 16:57:01,579 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-21 16:57:01,593 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2024-01-21 16:57:01,593 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-21 16:57:01,616 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-21 16:57:05,899 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=2b792796-59e5-4be6-80e9-9b14231a277d, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-598f264a-2ba9-4c96-a8a5-cc4228cf780d;nsid=619814465;c=0) storage 2b792796-59e5-4be6-80e9-9b14231a277d
2024-01-21 16:57:05,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-21 16:57:05,900 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-01-21 16:57:05,917 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=9821ce81-11c2-4ae5-8744-9efc0dc4c617, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-598f264a-2ba9-4c96-a8a5-cc4228cf780d;nsid=619814465;c=0) storage 9821ce81-11c2-4ae5-8744-9efc0dc4c617
2024-01-21 16:57:05,917 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-21 16:57:05,921 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-21 16:57:06,021 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-21 16:57:06,021 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-65ddb5b2-cd18-4b04-8bcf-895019f97223 for DN 172.18.0.3:50010
2024-01-21 16:57:06,038 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-21 16:57:06,038 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b1c16bb3-8ba6-41ed-b7c8-31ba0b7e46e7 for DN 172.18.0.4:50010
2024-01-21 16:57:06,066 INFO BlockStateChange: BLOCK* processReport: from storage DS-b1c16bb3-8ba6-41ed-b7c8-31ba0b7e46e7 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=2b792796-59e5-4be6-80e9-9b14231a277d, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-598f264a-2ba9-4c96-a8a5-cc4228cf780d;nsid=619814465;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-21 16:57:06,071 INFO BlockStateChange: BLOCK* processReport: from storage DS-65ddb5b2-cd18-4b04-8bcf-895019f97223 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=9821ce81-11c2-4ae5-8744-9efc0dc4c617, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-598f264a-2ba9-4c96-a8a5-cc4228cf780d;nsid=619814465;c=0), blocks: 0, hasStaleStorage: false, processing time: 3 msecs
2024-01-21 16:58:10,102 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-21 16:58:10,102 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-21 16:58:10,102 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-21 16:58:10,102 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 156 
2024-01-21 16:58:10,125 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 179 
2024-01-21 16:58:10,126 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-21 16:58:10,128 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-21 16:58:11,366 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2024-01-21 16:58:11,367 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-01-21 16:58:11,401 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-21 17:29:27,976 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4165ms
GC pool 'Copy' had collection(s): count=1 time=4212ms
2024-01-22 12:21:17,813 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-22 12:21:17,852 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-22 12:21:17,855 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-22 12:21:18,479 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-22 12:21:18,726 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-22 12:21:18,727 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-22 12:21:18,728 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-22 12:21:18,729 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-22 12:21:19,006 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-22 12:21:19,158 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-22 12:21:19,166 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-22 12:21:19,193 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-22 12:21:19,197 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-22 12:21:19,211 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-22 12:21:19,211 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-22 12:21:19,211 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-22 12:21:19,265 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-22 12:21:19,266 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-22 12:21:19,312 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-22 12:21:19,313 INFO org.mortbay.log: jetty-6.1.26
2024-01-22 12:21:19,717 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-22 12:21:19,765 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-22 12:21:19,765 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-22 12:21:19,818 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-22 12:21:19,818 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-22 12:21:19,880 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-22 12:21:19,880 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-22 12:21:19,881 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-22 12:21:19,882 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 22 12:21:19
2024-01-22 12:21:19,884 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-22 12:21:19,884 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-22 12:21:19,885 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-22 12:21:19,885 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-22 12:21:19,889 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-22 12:21:19,902 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-22 12:21:19,903 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-22 12:21:19,903 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-22 12:21:19,903 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-22 12:21:19,903 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-22 12:21:19,903 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-22 12:21:19,903 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-22 12:21:19,903 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-22 12:21:19,908 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-22 12:21:19,908 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-22 12:21:19,908 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-22 12:21:19,908 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-22 12:21:19,909 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-22 12:21:20,246 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-22 12:21:20,246 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-22 12:21:20,246 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-22 12:21:20,246 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-22 12:21:20,269 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-22 12:21:20,269 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-22 12:21:20,269 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-22 12:21:20,270 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-22 12:21:20,275 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-22 12:21:20,275 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-22 12:21:20,275 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-22 12:21:20,275 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-22 12:21:20,276 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-22 12:21:20,276 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-22 12:21:20,276 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-22 12:21:20,278 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-22 12:21:20,278 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-22 12:21:20,278 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-22 12:21:20,290 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-22 12:21:20,290 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-22 12:21:20,291 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-22 12:21:20,291 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-22 12:21:20,291 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-22 12:21:20,291 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-22 12:21:20,396 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 143@master
2024-01-22 12:21:20,462 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-22 12:21:20,462 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-22 12:21:20,506 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-22 12:21:20,543 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-22 12:21:20,543 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-22 12:21:20,548 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-22 12:21:20,548 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-22 12:21:20,719 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-22 12:21:20,719 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 426 msecs
2024-01-22 12:21:21,156 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-22 12:21:21,173 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-22 12:21:21,189 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-22 12:21:21,280 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-22 12:21:21,317 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-22 12:21:21,317 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-22 12:21:21,317 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-22 12:21:21,318 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-22 12:21:21,318 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-22 12:21:21,318 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-22 12:21:21,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-22 12:21:21,345 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-22 12:21:21,345 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-22 12:21:21,345 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-22 12:21:21,345 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-22 12:21:21,345 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-22 12:21:21,345 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 25 msec
2024-01-22 12:21:21,403 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-22 12:21:21,404 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-22 12:21:21,413 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2024-01-22 12:21:21,413 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-22 12:21:21,455 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-22 12:21:26,253 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=a2178d10-9914-4745-bdee-7ed9a4697c7e, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-7b4d5891-c564-4515-8f64-e172cc0493f4;nsid=134794854;c=0) storage a2178d10-9914-4745-bdee-7ed9a4697c7e
2024-01-22 12:21:26,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-22 12:21:26,254 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-22 12:21:26,366 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=d3e7b4f2-e274-468a-8f20-ed33fdf90e56, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-7b4d5891-c564-4515-8f64-e172cc0493f4;nsid=134794854;c=0) storage d3e7b4f2-e274-468a-8f20-ed33fdf90e56
2024-01-22 12:21:26,367 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-22 12:21:26,380 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-01-22 12:21:26,453 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-22 12:21:26,453 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-062b0fb6-6c02-4e98-b8dc-5fe4d3f550a5 for DN 172.18.0.3:50010
2024-01-22 12:21:26,457 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-22 12:21:26,457 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-6541aca0-e7c6-4d28-966a-279b3dd99aa3 for DN 172.18.0.4:50010
2024-01-22 12:21:26,519 INFO BlockStateChange: BLOCK* processReport: from storage DS-062b0fb6-6c02-4e98-b8dc-5fe4d3f550a5 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=a2178d10-9914-4745-bdee-7ed9a4697c7e, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-7b4d5891-c564-4515-8f64-e172cc0493f4;nsid=134794854;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-22 12:21:26,545 INFO BlockStateChange: BLOCK* processReport: from storage DS-6541aca0-e7c6-4d28-966a-279b3dd99aa3 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=d3e7b4f2-e274-468a-8f20-ed33fdf90e56, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-7b4d5891-c564-4515-8f64-e172cc0493f4;nsid=134794854;c=0), blocks: 0, hasStaleStorage: false, processing time: 15 msecs
2024-01-22 12:30:22,789 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 6589ms
GC pool 'Copy' had collection(s): count=1 time=6799ms
2024-01-22 12:31:21,626 INFO org.mortbay.log: org.mortbay.io.nio.SelectorManager$SelectSet@1bf90a51 JVM BUG(s) - injecting delay10 times
2024-01-22 12:31:21,661 INFO org.mortbay.log: org.mortbay.io.nio.SelectorManager$SelectSet@1bf90a51 JVM BUG(s) - recreating selector 10 times, canceled keys 161 times
2024-01-22 12:51:30,836 INFO BlockStateChange: BLOCK* processReport: from storage DS-062b0fb6-6c02-4e98-b8dc-5fe4d3f550a5 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=a2178d10-9914-4745-bdee-7ed9a4697c7e, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-7b4d5891-c564-4515-8f64-e172cc0493f4;nsid=134794854;c=0), blocks: 0, hasStaleStorage: false, processing time: 55 msecs
2024-01-22 14:49:34,605 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2928ms
No GCs detected
2024-01-22 15:00:22,210 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-22 15:00:22,210 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-22 15:00:22,219 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-22 15:00:22,239 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 51 
2024-01-22 15:00:22,283 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 93 
2024-01-22 15:00:22,317 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-22 15:00:22,373 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-22 15:00:33,902 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2024-01-22 15:00:33,906 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 350 bytes.
2024-01-22 15:00:33,961 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-22 16:00:34,903 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-22 16:00:34,926 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-22 16:00:34,926 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3
2024-01-22 16:00:34,927 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 94 
2024-01-22 16:00:34,959 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 126 
2024-01-22 16:00:34,960 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000003 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000003-0000000000000000004
2024-01-22 16:00:34,980 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 5
2024-01-22 16:00:35,353 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.03s at 0.00 KB/s
2024-01-22 16:00:35,353 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000004 size 350 bytes.
2024-01-22 16:00:35,387 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2024-01-22 16:00:35,388 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2024-01-31 11:32:38,406 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-31 11:32:38,460 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-31 11:32:38,465 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-31 11:32:39,019 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-31 11:32:39,246 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-31 11:32:39,246 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-31 11:32:39,248 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-31 11:32:39,248 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-31 11:32:39,564 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-31 11:32:39,740 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-31 11:32:39,752 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-31 11:32:39,784 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-31 11:32:39,788 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-31 11:32:39,803 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-31 11:32:39,803 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-31 11:32:39,803 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-31 11:32:39,857 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-31 11:32:39,858 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-31 11:32:39,919 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-31 11:32:39,920 INFO org.mortbay.log: jetty-6.1.26
2024-01-31 11:32:40,325 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-31 11:32:40,374 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-31 11:32:40,374 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-31 11:32:40,420 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-31 11:32:40,421 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-31 11:32:40,473 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-31 11:32:40,473 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-31 11:32:40,474 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-31 11:32:40,475 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 31 11:32:40
2024-01-31 11:32:40,488 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-31 11:32:40,488 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 11:32:40,489 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-31 11:32:40,489 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-31 11:32:40,494 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-31 11:32:40,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-31 11:32:40,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-31 11:32:40,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-31 11:32:40,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-31 11:32:40,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-31 11:32:40,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-31 11:32:40,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-31 11:32:40,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-31 11:32:40,509 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-31 11:32:40,509 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-31 11:32:40,509 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-31 11:32:40,509 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-31 11:32:40,511 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-31 11:32:40,837 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-31 11:32:40,837 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 11:32:40,837 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-31 11:32:40,837 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-31 11:32:40,860 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-31 11:32:40,860 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-31 11:32:40,860 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-31 11:32:40,860 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-31 11:32:40,867 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-31 11:32:40,867 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 11:32:40,868 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-31 11:32:40,868 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-31 11:32:40,869 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-31 11:32:40,869 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-31 11:32:40,869 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-31 11:32:40,870 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-31 11:32:40,871 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-31 11:32:40,871 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-31 11:32:40,871 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-31 11:32:40,871 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-31 11:32:40,873 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-31 11:32:40,873 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 11:32:40,873 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-31 11:32:40,873 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-31 11:32:41,001 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 141@master
2024-01-31 11:32:41,065 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-31 11:32:41,065 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-31 11:32:41,095 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-31 11:32:41,128 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-31 11:32:41,128 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-31 11:32:41,134 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-31 11:32:41,135 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-31 11:32:41,340 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-31 11:32:41,340 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 466 msecs
2024-01-31 11:32:41,720 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-31 11:32:41,739 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-31 11:32:41,750 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-31 11:32:41,831 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-31 11:32:41,850 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-31 11:32:41,850 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-31 11:32:41,850 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-31 11:32:41,851 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-31 11:32:41,851 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-31 11:32:41,851 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-31 11:32:41,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 11:32:41,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-31 11:32:41,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-31 11:32:41,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-31 11:32:41,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-31 11:32:41,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-31 11:32:41,868 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 16 msec
2024-01-31 11:32:41,909 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-31 11:32:41,910 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-31 11:32:41,918 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.4:54310
2024-01-31 11:32:41,918 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-31 11:32:41,943 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-31 11:32:46,442 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=5bac4068-e7ff-4383-a039-fbdb27a7b8e8, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3e869996-b220-45f2-a5b6-cf4fc36d8ce8;nsid=634398172;c=0) storage 5bac4068-e7ff-4383-a039-fbdb27a7b8e8
2024-01-31 11:32:46,446 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 11:32:46,447 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-31 11:32:46,461 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.2:50010, datanodeUuid=861ae45f-6e8a-45f9-afc4-266bfdd979e4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3e869996-b220-45f2-a5b6-cf4fc36d8ce8;nsid=634398172;c=0) storage 861ae45f-6e8a-45f9-afc4-266bfdd979e4
2024-01-31 11:32:46,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 11:32:46,468 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.2:50010
2024-01-31 11:32:46,620 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 11:32:46,620 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-ff617199-0dc8-49c5-9add-58563e911457 for DN 172.18.0.3:50010
2024-01-31 11:32:46,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 11:32:46,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-a71c6ff2-ccdb-4298-baa6-8bbd0cafd6d5 for DN 172.18.0.2:50010
2024-01-31 11:32:46,701 INFO BlockStateChange: BLOCK* processReport: from storage DS-a71c6ff2-ccdb-4298-baa6-8bbd0cafd6d5 node DatanodeRegistration(172.18.0.2:50010, datanodeUuid=861ae45f-6e8a-45f9-afc4-266bfdd979e4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3e869996-b220-45f2-a5b6-cf4fc36d8ce8;nsid=634398172;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-31 11:32:46,710 INFO BlockStateChange: BLOCK* processReport: from storage DS-ff617199-0dc8-49c5-9add-58563e911457 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=5bac4068-e7ff-4383-a039-fbdb27a7b8e8, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3e869996-b220-45f2-a5b6-cf4fc36d8ce8;nsid=634398172;c=0), blocks: 0, hasStaleStorage: false, processing time: 4 msecs
2024-01-31 11:33:50,642 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.4
2024-01-31 11:33:50,642 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-31 11:33:50,642 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-31 11:33:50,642 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 66 
2024-01-31 11:33:50,673 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 96 
2024-01-31 11:33:50,692 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-31 11:33:50,707 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-31 11:33:52,714 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2024-01-31 11:33:52,714 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-01-31 11:33:52,759 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-31 13:01:58,553 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-31 13:01:58,598 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-31 13:01:58,601 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-31 13:01:59,173 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-31 13:01:59,401 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-31 13:01:59,401 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-31 13:01:59,403 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-31 13:01:59,404 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-31 13:01:59,725 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-31 13:01:59,870 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-31 13:01:59,879 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-31 13:01:59,913 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-31 13:01:59,917 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-31 13:01:59,931 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-31 13:01:59,931 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-31 13:01:59,931 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-31 13:01:59,977 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-31 13:01:59,978 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-31 13:02:00,048 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-31 13:02:00,049 INFO org.mortbay.log: jetty-6.1.26
2024-01-31 13:02:00,420 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-31 13:02:00,466 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-31 13:02:00,466 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-31 13:02:00,532 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-31 13:02:00,532 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-31 13:02:00,571 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-31 13:02:00,571 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-31 13:02:00,572 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-31 13:02:00,573 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 31 13:02:00
2024-01-31 13:02:00,575 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-31 13:02:00,575 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 13:02:00,576 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-31 13:02:00,576 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-31 13:02:00,580 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-31 13:02:00,585 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-31 13:02:00,585 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-31 13:02:00,585 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-31 13:02:00,585 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-31 13:02:00,585 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-31 13:02:00,585 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-31 13:02:00,585 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-31 13:02:00,585 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-31 13:02:00,591 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-31 13:02:00,591 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-31 13:02:00,591 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-31 13:02:00,591 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-31 13:02:00,593 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-31 13:02:00,924 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-31 13:02:00,924 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 13:02:00,924 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-31 13:02:00,924 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-31 13:02:00,946 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-31 13:02:00,946 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-31 13:02:00,947 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-31 13:02:00,947 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-31 13:02:00,956 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-31 13:02:00,956 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 13:02:00,956 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-31 13:02:00,956 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-31 13:02:00,957 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-31 13:02:00,957 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-31 13:02:00,957 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-31 13:02:00,972 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-31 13:02:00,973 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-31 13:02:00,974 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-31 13:02:00,975 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-31 13:02:00,975 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-31 13:02:00,976 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-31 13:02:00,976 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 13:02:00,977 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-31 13:02:00,977 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-31 13:02:01,031 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 144@master
2024-01-31 13:02:01,098 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-31 13:02:01,115 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-31 13:02:01,152 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-31 13:02:01,193 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-31 13:02:01,193 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-31 13:02:01,208 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-31 13:02:01,209 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-31 13:02:01,419 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-31 13:02:01,419 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 441 msecs
2024-01-31 13:02:01,942 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-31 13:02:01,999 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-31 13:02:02,013 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-31 13:02:02,102 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-31 13:02:02,144 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-31 13:02:02,144 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-31 13:02:02,144 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-31 13:02:02,149 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-31 13:02:02,149 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-31 13:02:02,149 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-31 13:02:02,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 13:02:02,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-31 13:02:02,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-31 13:02:02,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-31 13:02:02,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-31 13:02:02,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-31 13:02:02,188 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 36 msec
2024-01-31 13:02:02,231 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-31 13:02:02,232 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-31 13:02:02,237 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2024-01-31 13:02:02,237 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-31 13:02:02,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-31 13:02:06,807 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=8d27c7a4-cd1e-4c28-9d3d-13dffa4f52af, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-8167d057-3746-45e6-bc43-1f73010e984f;nsid=513807859;c=0) storage 8d27c7a4-cd1e-4c28-9d3d-13dffa4f52af
2024-01-31 13:02:06,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 13:02:06,808 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-31 13:02:06,826 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=7f2a8339-ddda-495c-b79b-5a495486aa09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-8167d057-3746-45e6-bc43-1f73010e984f;nsid=513807859;c=0) storage 7f2a8339-ddda-495c-b79b-5a495486aa09
2024-01-31 13:02:06,827 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 13:02:06,827 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-01-31 13:02:06,993 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 13:02:06,993 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-25d5a17b-6404-4818-ab62-643dbb38bc88 for DN 172.18.0.4:50010
2024-01-31 13:02:06,995 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 13:02:06,996 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-6fab46e7-7307-4fb4-916a-0f18b26b7456 for DN 172.18.0.3:50010
2024-01-31 13:02:07,035 INFO BlockStateChange: BLOCK* processReport: from storage DS-25d5a17b-6404-4818-ab62-643dbb38bc88 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=7f2a8339-ddda-495c-b79b-5a495486aa09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-8167d057-3746-45e6-bc43-1f73010e984f;nsid=513807859;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-31 13:02:07,039 INFO BlockStateChange: BLOCK* processReport: from storage DS-6fab46e7-7307-4fb4-916a-0f18b26b7456 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=8d27c7a4-cd1e-4c28-9d3d-13dffa4f52af, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-8167d057-3746-45e6-bc43-1f73010e984f;nsid=513807859;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2024-01-31 13:03:10,774 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-31 13:03:10,775 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-31 13:03:10,775 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-31 13:03:10,775 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 67 
2024-01-31 13:03:10,799 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 90 
2024-01-31 13:03:10,800 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-31 13:03:10,802 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-31 13:03:12,035 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2024-01-31 13:03:12,035 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-01-31 13:03:12,075 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-31 13:17:28,549 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4350ms
No GCs detected
2024-01-31 13:29:11,721 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-31 13:29:11,745 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-31 13:29:11,748 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-31 13:29:12,354 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-31 13:29:12,617 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-31 13:29:12,617 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-31 13:29:12,619 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-31 13:29:12,620 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-31 13:29:12,904 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-31 13:29:13,065 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-31 13:29:13,073 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-31 13:29:13,108 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-31 13:29:13,112 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-31 13:29:13,125 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-31 13:29:13,125 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-31 13:29:13,126 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-31 13:29:13,171 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-31 13:29:13,172 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-31 13:29:13,218 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-31 13:29:13,219 INFO org.mortbay.log: jetty-6.1.26
2024-01-31 13:29:13,599 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-31 13:29:13,647 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-31 13:29:13,647 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-31 13:29:13,699 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-31 13:29:13,699 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-31 13:29:13,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-31 13:29:13,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-31 13:29:13,750 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-31 13:29:13,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 31 13:29:13
2024-01-31 13:29:13,763 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-31 13:29:13,763 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 13:29:13,764 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-31 13:29:13,764 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-31 13:29:13,772 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-31 13:29:13,772 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-31 13:29:13,772 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-31 13:29:13,772 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-31 13:29:13,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-31 13:29:13,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-31 13:29:13,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-31 13:29:13,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-31 13:29:13,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-31 13:29:13,788 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-31 13:29:13,788 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-31 13:29:13,788 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-31 13:29:13,789 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-31 13:29:13,790 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-31 13:29:14,113 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-31 13:29:14,113 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 13:29:14,113 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-31 13:29:14,113 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-31 13:29:14,137 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-31 13:29:14,137 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-31 13:29:14,137 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-31 13:29:14,137 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-31 13:29:14,143 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-31 13:29:14,143 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 13:29:14,143 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-31 13:29:14,143 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-31 13:29:14,154 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-31 13:29:14,154 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-31 13:29:14,154 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-31 13:29:14,157 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-31 13:29:14,157 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-31 13:29:14,158 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-31 13:29:14,159 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-31 13:29:14,159 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-31 13:29:14,160 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-31 13:29:14,160 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 13:29:14,161 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-31 13:29:14,161 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-31 13:29:14,224 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 141@master
2024-01-31 13:29:14,293 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-31 13:29:14,293 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-31 13:29:14,341 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-31 13:29:14,370 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-31 13:29:14,371 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-31 13:29:14,376 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-31 13:29:14,376 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-31 13:29:14,564 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-31 13:29:14,564 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 402 msecs
2024-01-31 13:29:15,115 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-31 13:29:15,129 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-31 13:29:15,138 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-31 13:29:15,211 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-31 13:29:15,239 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-31 13:29:15,240 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-31 13:29:15,240 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-31 13:29:15,240 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-31 13:29:15,241 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-31 13:29:15,241 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-31 13:29:15,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 13:29:15,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-31 13:29:15,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-31 13:29:15,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-31 13:29:15,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-31 13:29:15,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-31 13:29:15,266 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 25 msec
2024-01-31 13:29:15,294 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-31 13:29:15,296 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-31 13:29:15,319 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2024-01-31 13:29:15,319 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-31 13:29:15,326 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-31 13:29:20,268 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=84663425-3158-4c90-a377-af0963b75596, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-058974d8-4383-4236-99ec-4ce43c431678;nsid=306995317;c=0) storage 84663425-3158-4c90-a377-af0963b75596
2024-01-31 13:29:20,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 13:29:20,273 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-31 13:29:20,283 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=8729d6b0-6fc5-4495-9e00-747eeb06f7ce, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-058974d8-4383-4236-99ec-4ce43c431678;nsid=306995317;c=0) storage 8729d6b0-6fc5-4495-9e00-747eeb06f7ce
2024-01-31 13:29:20,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 13:29:20,284 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-01-31 13:29:20,406 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 13:29:20,406 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-ac4446d4-fbdb-4d18-a22d-7b34fdb16a87 for DN 172.18.0.3:50010
2024-01-31 13:29:20,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 13:29:20,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-500a513a-3cdf-4b7b-b782-fb4b325a98a6 for DN 172.18.0.4:50010
2024-01-31 13:29:20,503 INFO BlockStateChange: BLOCK* processReport: from storage DS-500a513a-3cdf-4b7b-b782-fb4b325a98a6 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=8729d6b0-6fc5-4495-9e00-747eeb06f7ce, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-058974d8-4383-4236-99ec-4ce43c431678;nsid=306995317;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-31 13:29:20,506 INFO BlockStateChange: BLOCK* processReport: from storage DS-ac4446d4-fbdb-4d18-a22d-7b34fdb16a87 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=84663425-3158-4c90-a377-af0963b75596, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-058974d8-4383-4236-99ec-4ce43c431678;nsid=306995317;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2024-01-31 13:30:24,055 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-01-31 13:30:24,055 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-31 13:30:24,055 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-31 13:30:24,055 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 65 
2024-01-31 13:30:24,124 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 134 
2024-01-31 13:30:24,125 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-31 13:30:24,127 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-31 13:30:25,355 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2024-01-31 13:30:25,355 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-01-31 13:30:25,390 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-31 13:41:44,863 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1560ms
No GCs detected
2024-01-31 14:03:40,265 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2396ms
GC pool 'Copy' had collection(s): count=1 time=2656ms
2024-01-31 14:56:59,806 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3651ms
No GCs detected
2024-01-31 15:17:16,290 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-31 15:17:16,327 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-31 15:17:16,330 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-31 15:17:16,975 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-31 15:17:17,223 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-31 15:17:17,223 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-31 15:17:17,225 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-31 15:17:17,226 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-31 15:17:17,535 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-31 15:17:17,729 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-31 15:17:17,740 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-31 15:17:17,764 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-31 15:17:17,768 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-31 15:17:17,782 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-31 15:17:17,782 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-31 15:17:17,783 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-31 15:17:17,837 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-31 15:17:17,838 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-31 15:17:17,899 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-31 15:17:17,900 INFO org.mortbay.log: jetty-6.1.26
2024-01-31 15:17:18,321 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-31 15:17:18,373 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-31 15:17:18,373 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-31 15:17:18,425 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-31 15:17:18,425 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-31 15:17:18,488 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-31 15:17:18,488 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-31 15:17:18,493 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-31 15:17:18,495 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 31 15:17:18
2024-01-31 15:17:18,508 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-31 15:17:18,508 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 15:17:18,509 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-31 15:17:18,509 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-31 15:17:18,514 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-31 15:17:18,514 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-31 15:17:18,514 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-31 15:17:18,514 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-31 15:17:18,514 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-31 15:17:18,515 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-31 15:17:18,515 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-31 15:17:18,515 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-31 15:17:18,515 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-31 15:17:18,528 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-31 15:17:18,528 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-31 15:17:18,528 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-31 15:17:18,528 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-31 15:17:18,530 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-31 15:17:18,881 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-31 15:17:18,881 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 15:17:18,881 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-31 15:17:18,881 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-31 15:17:18,903 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-31 15:17:18,903 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-31 15:17:18,903 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-31 15:17:18,903 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-31 15:17:18,911 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-31 15:17:18,911 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 15:17:18,911 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-31 15:17:18,911 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-31 15:17:18,912 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-31 15:17:18,912 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-31 15:17:18,912 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-31 15:17:18,916 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-31 15:17:18,916 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-31 15:17:18,917 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-31 15:17:18,918 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-31 15:17:18,918 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-31 15:17:18,919 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-31 15:17:18,919 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 15:17:18,921 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-31 15:17:18,921 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-31 15:17:19,007 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 141@master
2024-01-31 15:17:19,069 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-31 15:17:19,070 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-31 15:17:19,099 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-31 15:17:19,126 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-31 15:17:19,126 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-31 15:17:19,132 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-31 15:17:19,132 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-31 15:17:19,331 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-31 15:17:19,331 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 395 msecs
2024-01-31 15:17:19,730 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-31 15:17:19,743 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-31 15:17:19,753 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-31 15:17:19,877 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-31 15:17:19,903 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-31 15:17:19,903 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-31 15:17:19,903 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-31 15:17:19,910 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-31 15:17:19,910 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-31 15:17:19,910 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-31 15:17:19,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 15:17:19,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-31 15:17:19,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-31 15:17:19,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-31 15:17:19,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-31 15:17:19,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-31 15:17:19,936 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 25 msec
2024-01-31 15:17:20,012 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-31 15:17:20,022 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-31 15:17:20,035 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.4:54310
2024-01-31 15:17:20,035 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-31 15:17:20,050 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-31 15:17:25,065 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.2:50010, datanodeUuid=1afbda39-8b75-41ca-9d19-3c86a70fd246, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cbc3682d-53ae-4915-b762-092f6d4a0db0;nsid=1371280364;c=0) storage 1afbda39-8b75-41ca-9d19-3c86a70fd246
2024-01-31 15:17:25,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 15:17:25,067 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.2:50010
2024-01-31 15:17:25,074 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=718ad682-d0fa-4b89-be7b-ec0edc48abb0, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cbc3682d-53ae-4915-b762-092f6d4a0db0;nsid=1371280364;c=0) storage 718ad682-d0fa-4b89-be7b-ec0edc48abb0
2024-01-31 15:17:25,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 15:17:25,074 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-01-31 15:17:25,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 15:17:25,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8e79e14b-86e3-4ab8-9265-3eaa58267408 for DN 172.18.0.3:50010
2024-01-31 15:17:25,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 15:17:25,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c76d2e96-bfbb-4c96-b2da-b8276a773648 for DN 172.18.0.2:50010
2024-01-31 15:17:25,361 INFO BlockStateChange: BLOCK* processReport: from storage DS-8e79e14b-86e3-4ab8-9265-3eaa58267408 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=718ad682-d0fa-4b89-be7b-ec0edc48abb0, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cbc3682d-53ae-4915-b762-092f6d4a0db0;nsid=1371280364;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-31 15:17:25,370 INFO BlockStateChange: BLOCK* processReport: from storage DS-c76d2e96-bfbb-4c96-b2da-b8276a773648 node DatanodeRegistration(172.18.0.2:50010, datanodeUuid=1afbda39-8b75-41ca-9d19-3c86a70fd246, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cbc3682d-53ae-4915-b762-092f6d4a0db0;nsid=1371280364;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2024-01-31 16:00:35,527 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.4
2024-01-31 16:00:35,684 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-31 16:00:35,712 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-31 16:00:35,862 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 67 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 73 
2024-01-31 16:00:35,960 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 67 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 149 
2024-01-31 16:00:36,329 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-31 16:00:36,996 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-31 16:00:54,173 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2024-01-31 16:00:54,173 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-01-31 16:00:54,313 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-01-31 17:00:55,513 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.4
2024-01-31 17:00:55,522 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-31 17:00:55,522 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3
2024-01-31 17:00:55,523 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 112 
2024-01-31 17:00:55,550 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 139 
2024-01-31 17:00:55,557 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000003 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000003-0000000000000000004
2024-01-31 17:00:55,578 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 5
2024-01-31 17:00:55,998 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.07s at 0.00 KB/s
2024-01-31 17:00:55,998 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000004 size 351 bytes.
2024-01-31 17:00:56,111 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2024-01-31 17:00:56,111 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2024-01-31 17:27:59,301 INFO BlockStateChange: BLOCK* processReport: from storage DS-8e79e14b-86e3-4ab8-9265-3eaa58267408 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=718ad682-d0fa-4b89-be7b-ec0edc48abb0, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-cbc3682d-53ae-4915-b762-092f6d4a0db0;nsid=1371280364;c=0), blocks: 0, hasStaleStorage: false, processing time: 15 msecs
2024-01-31 18:00:57,162 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.4
2024-01-31 18:00:57,162 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-31 18:00:57,162 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 5
2024-01-31 18:00:57,162 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 104 
2024-01-31 18:00:57,209 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 151 
2024-01-31 18:00:57,210 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000005 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000005-0000000000000000006
2024-01-31 18:00:57,211 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 7
2024-01-31 18:00:57,642 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.07s at 0.00 KB/s
2024-01-31 18:00:57,642 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000006 size 351 bytes.
2024-01-31 18:00:57,716 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4
2024-01-31 18:00:57,717 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop/dfs/name/current/fsimage_0000000000000000002, cpktTxId=0000000000000000002)
2024-01-31 18:25:19,358 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3422ms
GC pool 'Copy' had collection(s): count=1 time=3700ms
2024-01-31 22:59:03,665 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.6
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-01-31 22:59:03,720 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-01-31 22:59:03,723 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-01-31 22:59:04,268 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-01-31 22:59:04,501 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-01-31 22:59:04,501 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-01-31 22:59:04,502 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-01-31 22:59:04,503 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-01-31 22:59:04,837 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-01-31 22:59:04,989 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-01-31 22:59:04,999 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-01-31 22:59:05,033 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-01-31 22:59:05,036 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-01-31 22:59:05,050 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-01-31 22:59:05,050 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-01-31 22:59:05,050 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-01-31 22:59:05,096 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-01-31 22:59:05,097 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-01-31 22:59:05,143 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-01-31 22:59:05,143 INFO org.mortbay.log: jetty-6.1.26
2024-01-31 22:59:05,556 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-01-31 22:59:05,613 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-31 22:59:05,613 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-01-31 22:59:05,646 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-01-31 22:59:05,646 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-01-31 22:59:05,700 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-01-31 22:59:05,700 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-01-31 22:59:05,711 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-01-31 22:59:05,712 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Jan 31 22:59:05
2024-01-31 22:59:05,714 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-01-31 22:59:05,714 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 22:59:05,715 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-01-31 22:59:05,715 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-01-31 22:59:05,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-01-31 22:59:05,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-01-31 22:59:05,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-01-31 22:59:05,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-01-31 22:59:05,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-01-31 22:59:05,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-01-31 22:59:05,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-01-31 22:59:05,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-01-31 22:59:05,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-01-31 22:59:05,735 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-01-31 22:59:05,735 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-01-31 22:59:05,735 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-01-31 22:59:05,735 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-01-31 22:59:05,737 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-01-31 22:59:06,060 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-01-31 22:59:06,060 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 22:59:06,060 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-01-31 22:59:06,060 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-01-31 22:59:06,082 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-01-31 22:59:06,082 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-01-31 22:59:06,082 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-01-31 22:59:06,082 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-01-31 22:59:06,089 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-01-31 22:59:06,090 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 22:59:06,090 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-01-31 22:59:06,090 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-01-31 22:59:06,091 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-01-31 22:59:06,091 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-01-31 22:59:06,091 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-01-31 22:59:06,093 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-01-31 22:59:06,093 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-01-31 22:59:06,093 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-01-31 22:59:06,093 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-01-31 22:59:06,094 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-01-31 22:59:06,095 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-01-31 22:59:06,095 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-01-31 22:59:06,095 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-01-31 22:59:06,095 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-01-31 22:59:06,190 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 141@master
2024-01-31 22:59:06,258 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-01-31 22:59:06,259 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-01-31 22:59:06,283 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-01-31 22:59:06,309 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-01-31 22:59:06,310 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-01-31 22:59:06,315 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-01-31 22:59:06,315 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-01-31 22:59:06,521 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-01-31 22:59:06,521 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 424 msecs
2024-01-31 22:59:06,921 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-01-31 22:59:06,937 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-01-31 22:59:06,946 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-01-31 22:59:07,024 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-01-31 22:59:07,049 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-31 22:59:07,049 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-01-31 22:59:07,049 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-01-31 22:59:07,050 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-01-31 22:59:07,050 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-01-31 22:59:07,050 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-01-31 22:59:07,062 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 22:59:07,065 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-01-31 22:59:07,065 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-01-31 22:59:07,065 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-01-31 22:59:07,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-01-31 22:59:07,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-01-31 22:59:07,066 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 15 msec
2024-01-31 22:59:07,117 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-01-31 22:59:07,118 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-01-31 22:59:07,120 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.6:54310
2024-01-31 22:59:07,120 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-01-31 22:59:07,139 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-01-31 22:59:11,685 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=595d2f97-2f4e-4b97-a75e-9ef7bbba3c86, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9f407693-9c9d-42b2-929b-9cb929a35059;nsid=321320448;c=0) storage 595d2f97-2f4e-4b97-a75e-9ef7bbba3c86
2024-01-31 22:59:11,689 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 22:59:11,690 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-01-31 22:59:11,701 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.5:50010, datanodeUuid=6cd9f0df-0998-4203-a531-0a733b593d30, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9f407693-9c9d-42b2-929b-9cb929a35059;nsid=321320448;c=0) storage 6cd9f0df-0998-4203-a531-0a733b593d30
2024-01-31 22:59:11,701 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 22:59:11,702 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.5:50010
2024-01-31 22:59:11,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 22:59:11,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-bb564ef1-d8e2-46cf-9c73-f30991278154 for DN 172.18.0.4:50010
2024-01-31 22:59:11,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-01-31 22:59:11,847 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b922f016-bb6f-4a10-b9d4-9afcf346535d for DN 172.18.0.5:50010
2024-01-31 22:59:11,887 INFO BlockStateChange: BLOCK* processReport: from storage DS-bb564ef1-d8e2-46cf-9c73-f30991278154 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=595d2f97-2f4e-4b97-a75e-9ef7bbba3c86, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9f407693-9c9d-42b2-929b-9cb929a35059;nsid=321320448;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2024-01-31 22:59:11,887 INFO BlockStateChange: BLOCK* processReport: from storage DS-b922f016-bb6f-4a10-b9d4-9afcf346535d node DatanodeRegistration(172.18.0.5:50010, datanodeUuid=6cd9f0df-0998-4203-a531-0a733b593d30, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9f407693-9c9d-42b2-929b-9cb929a35059;nsid=321320448;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-01-31 23:00:15,617 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.6
2024-01-31 23:00:15,617 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-01-31 23:00:15,617 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-01-31 23:00:15,617 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 74 
2024-01-31 23:00:15,649 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 106 
2024-01-31 23:00:15,650 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-01-31 23:00:15,652 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-01-31 23:00:17,531 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2024-01-31 23:00:17,531 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-01-31 23:00:17,590 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-02-03 17:10:18,077 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.6
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-02-03 17:10:18,103 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-02-03 17:10:18,106 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-02-03 17:10:18,741 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-02-03 17:10:18,950 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-02-03 17:10:18,950 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-02-03 17:10:18,951 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-02-03 17:10:18,952 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-02-03 17:10:19,269 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-02-03 17:10:19,437 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-02-03 17:10:19,446 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-02-03 17:10:19,473 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-02-03 17:10:19,477 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-02-03 17:10:19,490 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-02-03 17:10:19,490 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-02-03 17:10:19,490 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-02-03 17:10:19,538 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-02-03 17:10:19,539 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-02-03 17:10:19,583 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-02-03 17:10:19,584 INFO org.mortbay.log: jetty-6.1.26
2024-02-03 17:10:19,989 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-02-03 17:10:20,037 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-02-03 17:10:20,037 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-02-03 17:10:20,088 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-02-03 17:10:20,088 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-02-03 17:10:20,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-02-03 17:10:20,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-02-03 17:10:20,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-02-03 17:10:20,146 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Feb 03 17:10:20
2024-02-03 17:10:20,158 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-02-03 17:10:20,158 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-02-03 17:10:20,159 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-02-03 17:10:20,160 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-02-03 17:10:20,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-02-03 17:10:20,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-02-03 17:10:20,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-02-03 17:10:20,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-02-03 17:10:20,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-02-03 17:10:20,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-02-03 17:10:20,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-02-03 17:10:20,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-02-03 17:10:20,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-02-03 17:10:20,178 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-02-03 17:10:20,178 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-02-03 17:10:20,178 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-02-03 17:10:20,178 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-02-03 17:10:20,180 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-02-03 17:10:20,535 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-02-03 17:10:20,536 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-02-03 17:10:20,536 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-02-03 17:10:20,536 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-02-03 17:10:20,558 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-02-03 17:10:20,558 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-02-03 17:10:20,558 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-02-03 17:10:20,558 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-02-03 17:10:20,570 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-02-03 17:10:20,570 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-02-03 17:10:20,571 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-02-03 17:10:20,571 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-02-03 17:10:20,572 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-02-03 17:10:20,572 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-02-03 17:10:20,572 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-02-03 17:10:20,575 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-02-03 17:10:20,575 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-02-03 17:10:20,575 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-02-03 17:10:20,576 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-02-03 17:10:20,576 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-02-03 17:10:20,588 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-02-03 17:10:20,588 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-02-03 17:10:20,589 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-02-03 17:10:20,589 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-02-03 17:10:20,669 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 147@master
2024-02-03 17:10:20,744 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-02-03 17:10:20,744 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-02-03 17:10:20,777 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-02-03 17:10:20,804 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-02-03 17:10:20,804 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-02-03 17:10:20,811 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-02-03 17:10:20,811 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-02-03 17:10:20,992 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-02-03 17:10:20,992 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 401 msecs
2024-02-03 17:10:21,426 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-02-03 17:10:21,444 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-02-03 17:10:21,453 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-02-03 17:10:21,514 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-02-03 17:10:21,531 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-02-03 17:10:21,531 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-02-03 17:10:21,531 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-02-03 17:10:21,531 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2024-02-03 17:10:21,532 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-02-03 17:10:21,532 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-02-03 17:10:21,544 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-02-03 17:10:21,547 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-02-03 17:10:21,547 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-02-03 17:10:21,547 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-02-03 17:10:21,547 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-02-03 17:10:21,547 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-02-03 17:10:21,547 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 15 msec
2024-02-03 17:10:21,578 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-02-03 17:10:21,579 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-02-03 17:10:21,581 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.6:54310
2024-02-03 17:10:21,581 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-02-03 17:10:21,600 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-02-03 17:10:26,568 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.5:50010, datanodeUuid=897fc491-f5c9-402a-bb98-b87f10150df7, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0dc58037-822b-4bcf-b296-f3e933fa8198;nsid=1537516082;c=0) storage 897fc491-f5c9-402a-bb98-b87f10150df7
2024-02-03 17:10:26,574 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-02-03 17:10:26,575 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.5:50010
2024-02-03 17:10:26,578 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=6ce68669-64d5-4a6a-ab98-0bd73d5cd642, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0dc58037-822b-4bcf-b296-f3e933fa8198;nsid=1537516082;c=0) storage 6ce68669-64d5-4a6a-ab98-0bd73d5cd642
2024-02-03 17:10:26,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-02-03 17:10:26,582 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-02-03 17:10:26,727 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-02-03 17:10:26,727 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-ef88e758-430c-4707-a2f4-e68ddb6ef1d1 for DN 172.18.0.5:50010
2024-02-03 17:10:26,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-02-03 17:10:26,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-f3d62395-fcfa-4fb6-a35c-a9b4ce8e873b for DN 172.18.0.4:50010
2024-02-03 17:10:26,771 INFO BlockStateChange: BLOCK* processReport: from storage DS-f3d62395-fcfa-4fb6-a35c-a9b4ce8e873b node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=6ce68669-64d5-4a6a-ab98-0bd73d5cd642, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0dc58037-822b-4bcf-b296-f3e933fa8198;nsid=1537516082;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-02-03 17:10:26,774 INFO BlockStateChange: BLOCK* processReport: from storage DS-ef88e758-430c-4707-a2f4-e68ddb6ef1d1 node DatanodeRegistration(172.18.0.5:50010, datanodeUuid=897fc491-f5c9-402a-bb98-b87f10150df7, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0dc58037-822b-4bcf-b296-f3e933fa8198;nsid=1537516082;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2024-02-03 17:11:30,181 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.6
2024-02-03 17:11:30,181 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-02-03 17:11:30,181 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-02-03 17:11:30,181 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 60 
2024-02-03 17:11:30,201 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 80 
2024-02-03 17:11:30,202 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-02-03 17:11:30,203 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-02-03 17:11:31,486 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2024-02-03 17:11:31,486 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-02-03 17:11:31,520 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-02-06 09:40:28,584 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/root/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/common/lib/junit-4.11.jar:/root/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/root/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/root/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/root/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/root/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/root/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/root/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/root/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/root/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/root/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/root/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/root/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/root/hadoop/share/hadoop/common/lib/activation-1.1.jar:/root/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/root/hadoop/share/hadoop/common/lib/asm-3.2.jar:/root/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/root/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/root/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/root/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/root/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/root/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/root/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/root/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/root/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/root/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/root/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/root/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/root/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/root/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/root/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/root/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/root/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/root/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/root/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/root/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/root/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/root/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/root/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/root/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/root/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/root/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/root/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/root/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/root/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/root/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/root/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/root/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/root/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/root/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2024-02-06 09:40:28,638 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2024-02-06 09:40:28,641 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2024-02-06 09:40:29,634 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2024-02-06 09:40:34,383 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2024-02-06 09:40:34,384 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2024-02-06 09:40:34,387 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:54310
2024-02-06 09:40:34,389 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:54310 to access this namenode/service.
2024-02-06 09:40:35,575 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2024-02-06 09:40:36,546 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2024-02-06 09:40:36,557 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2024-02-06 09:40:36,574 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2024-02-06 09:40:36,581 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-02-06 09:40:36,605 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-02-06 09:40:36,605 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-02-06 09:40:36,605 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-02-06 09:40:36,676 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2024-02-06 09:40:36,677 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-02-06 09:40:36,738 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2024-02-06 09:40:36,739 INFO org.mortbay.log: jetty-6.1.26
2024-02-06 09:40:37,354 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2024-02-06 09:40:37,459 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-02-06 09:40:37,459 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2024-02-06 09:40:37,605 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2024-02-06 09:40:37,605 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2024-02-06 09:40:37,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2024-02-06 09:40:37,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2024-02-06 09:40:37,787 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-02-06 09:40:37,789 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2024 Feb 06 09:40:37
2024-02-06 09:40:37,804 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2024-02-06 09:40:37,804 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-02-06 09:40:37,809 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2024-02-06 09:40:37,809 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2024-02-06 09:40:37,829 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2024-02-06 09:40:37,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2024-02-06 09:40:37,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2024-02-06 09:40:37,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2024-02-06 09:40:37,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2024-02-06 09:40:37,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2024-02-06 09:40:37,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2024-02-06 09:40:37,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2024-02-06 09:40:37,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2024-02-06 09:40:37,867 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2024-02-06 09:40:37,867 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2024-02-06 09:40:37,867 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2024-02-06 09:40:37,867 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2024-02-06 09:40:37,869 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2024-02-06 09:40:39,085 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2024-02-06 09:40:39,089 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-02-06 09:40:39,089 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2024-02-06 09:40:39,089 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2024-02-06 09:40:39,112 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2024-02-06 09:40:39,113 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2024-02-06 09:40:39,113 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2024-02-06 09:40:39,113 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2024-02-06 09:40:39,131 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2024-02-06 09:40:39,131 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-02-06 09:40:39,131 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2024-02-06 09:40:39,131 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2024-02-06 09:40:39,132 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2024-02-06 09:40:39,132 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2024-02-06 09:40:39,133 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2024-02-06 09:40:39,151 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-02-06 09:40:39,151 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2024-02-06 09:40:39,152 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-02-06 09:40:39,153 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2024-02-06 09:40:39,153 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-02-06 09:40:39,155 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2024-02-06 09:40:39,155 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2024-02-06 09:40:39,155 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2024-02-06 09:40:39,155 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2024-02-06 09:40:39,251 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop/dfs/name/in_use.lock acquired by nodename 158@master
2024-02-06 09:40:39,350 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop/dfs/name/current
2024-02-06 09:40:39,351 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2024-02-06 09:40:39,498 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2024-02-06 09:40:39,577 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2024-02-06 09:40:39,587 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop/dfs/name/current/fsimage_0000000000000000000
2024-02-06 09:40:39,606 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-02-06 09:40:39,607 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2024-02-06 09:40:40,044 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2024-02-06 09:40:40,044 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 886 msecs
2024-02-06 09:40:40,395 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:54310
2024-02-06 09:40:40,399 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2024-02-06 09:40:40,427 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 54310
2024-02-06 09:40:40,479 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2024-02-06 09:40:40,509 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-02-06 09:40:40,509 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2024-02-06 09:40:40,509 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2024-02-06 09:40:40,511 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 2 secs
2024-02-06 09:40:40,511 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2024-02-06 09:40:40,511 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2024-02-06 09:40:40,521 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-02-06 09:40:40,545 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2024-02-06 09:40:40,545 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2024-02-06 09:40:40,545 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2024-02-06 09:40:40,545 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2024-02-06 09:40:40,545 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2024-02-06 09:40:40,545 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 34 msec
2024-02-06 09:40:40,589 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.2:54310
2024-02-06 09:40:40,589 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2024-02-06 09:40:40,575 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2024-02-06 09:40:40,577 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54310: starting
2024-02-06 09:40:40,621 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-02-06 09:40:42,332 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=73e8ea4f-f7f6-47e9-8a2a-659696de7a83, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-75cdf77c-393c-4e83-ad1d-57c6c3568445;nsid=1739663540;c=0) storage 73e8ea4f-f7f6-47e9-8a2a-659696de7a83
2024-02-06 09:40:42,335 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-02-06 09:40:42,336 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2024-02-06 09:40:42,416 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.4:50010, datanodeUuid=9e12d200-caf7-4728-8cdd-7db3222beb74, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-75cdf77c-393c-4e83-ad1d-57c6c3568445;nsid=1739663540;c=0) storage 9e12d200-caf7-4728-8cdd-7db3222beb74
2024-02-06 09:40:42,416 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-02-06 09:40:42,416 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.4:50010
2024-02-06 09:40:42,492 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-02-06 09:40:42,492 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-18c0d2fb-8d84-4523-8cec-db84811b3864 for DN 172.18.0.3:50010
2024-02-06 09:40:42,519 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2024-02-06 09:40:42,519 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-fbadd809-820c-432b-8870-8fd9f92db866 for DN 172.18.0.4:50010
2024-02-06 09:40:42,570 INFO BlockStateChange: BLOCK* processReport: from storage DS-18c0d2fb-8d84-4523-8cec-db84811b3864 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=73e8ea4f-f7f6-47e9-8a2a-659696de7a83, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-75cdf77c-393c-4e83-ad1d-57c6c3568445;nsid=1739663540;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-02-06 09:40:42,577 INFO BlockStateChange: BLOCK* processReport: from storage DS-fbadd809-820c-432b-8870-8fd9f92db866 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=9e12d200-caf7-4728-8cdd-7db3222beb74, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-75cdf77c-393c-4e83-ad1d-57c6c3568445;nsid=1739663540;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2024-02-06 09:46:00,128 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3489ms
No GCs detected
2024-02-06 10:40:01,873 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3353ms
No GCs detected
2024-02-06 12:08:35,190 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 46072ms
No GCs detected
2024-02-06 12:30:14,392 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.2
2024-02-06 12:30:14,448 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2024-02-06 12:30:14,458 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2024-02-06 12:30:14,559 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 54 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 138 
2024-02-06 12:30:14,674 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 54 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 183 
2024-02-06 12:30:14,996 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002
2024-02-06 12:30:15,722 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2024-02-06 12:30:23,021 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2024-02-06 12:30:23,022 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2024-02-06 12:30:23,157 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2024-02-06 12:58:39,437 INFO BlockStateChange: BLOCK* processReport: from storage DS-fbadd809-820c-432b-8870-8fd9f92db866 node DatanodeRegistration(172.18.0.4:50010, datanodeUuid=9e12d200-caf7-4728-8cdd-7db3222beb74, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-75cdf77c-393c-4e83-ad1d-57c6c3568445;nsid=1739663540;c=0), blocks: 0, hasStaleStorage: false, processing time: 26 msecs
